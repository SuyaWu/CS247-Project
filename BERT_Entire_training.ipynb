{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_Exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtep92FewPO-"
      },
      "source": [
        "This code deletes objects that are no longer needed in order to solve memory problem and run BERT on all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb6rjYgefJaW"
      },
      "source": [
        "print(\"delete later\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v59BLuXOh4vw",
        "outputId": "2e3a10c0-99b3-4176-c02b-303ae43eab14"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpkAFhseK5c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel,BertForSequenceClassification,AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgtsPHTLPKrk",
        "outputId": "78d22728-0ee0-44e4-9378-65cc49c0274f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFcfpeuobYD"
      },
      "source": [
        "train_data=pd.read_csv(\"./data/train.csv\")\n",
        "#test_data=pd.read_csv(\"./data/test.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPVdQDJtuH-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b0260d-3a26-4aa7-ff34-0dda1fc3d32f"
      },
      "source": [
        "\n",
        "print(len(train_data))\n",
        "# train_data = train_data[:100000]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1306122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZV-xpqjiDpX"
      },
      "source": [
        "train_data['question_text']=train_data['question_text'].str.lower()\n",
        "sentences=train_data.question_text.values\n",
        "# sentences=list(map(remove_stop_words, sentences))\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_data.target.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb5hdjxnuRQL"
      },
      "source": [
        "del train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjpQAvH4f3ZT"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrnlm9OCxRbm",
        "outputId": "d376ccd5-51c6-41c9-b7cc-40f8c126f074"
      },
      "source": [
        "print(tokenized_texts[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['[CLS]', 'how', 'did', 'quebec', 'nationalists', 'see', 'their', 'province', 'as', 'a', 'nation', 'in', 'the', '1960s', '?', '[SEP]'], ['[CLS]', 'do', 'you', 'have', 'an', 'adopted', 'dog', ',', 'how', 'would', 'you', 'encourage', 'people', 'to', 'adopt', 'and', 'not', 'shop', '?', '[SEP]'], ['[CLS]', 'why', 'does', 'velocity', 'affect', 'time', '?', 'does', 'velocity', 'affect', 'space', 'geometry', '?', '[SEP]'], ['[CLS]', 'how', 'did', 'otto', 'von', 'gu', '##eric', '##ke', 'used', 'the', 'mag', '##de', '##burg', 'hemisphere', '##s', '?', '[SEP]'], ['[CLS]', 'can', 'i', 'convert', 'mont', '##ra', 'he', '##lic', '##on', 'd', 'to', 'a', 'mountain', 'bike', 'by', 'just', 'changing', 'the', 'tyres', '?', '[SEP]'], ['[CLS]', 'is', 'gaza', 'slowly', 'becoming', 'auschwitz', ',', 'da', '##cha', '##u', 'or', 'tre', '##bl', '##ink', '##a', 'for', 'palestinians', '?', '[SEP]'], ['[CLS]', 'why', 'does', 'quo', '##ra', 'automatically', 'ban', 'conservative', 'opinions', 'when', 'reported', ',', 'but', 'does', 'not', 'do', 'the', 'same', 'for', 'liberal', 'views', '?', '[SEP]'], ['[CLS]', 'is', 'it', 'crazy', 'if', 'i', 'wash', 'or', 'wipe', 'my', 'groceries', 'off', '?', 'ge', '##rm', '##s', 'are', 'everywhere', '.', '[SEP]'], ['[CLS]', 'is', 'there', 'such', 'a', 'thing', 'as', 'dressing', 'moderately', ',', 'and', 'if', 'so', ',', 'how', 'is', 'that', 'different', 'than', 'dressing', 'modest', '##ly', '?', '[SEP]'], ['[CLS]', 'is', 'it', 'just', 'me', 'or', 'have', 'you', 'ever', 'been', 'in', 'this', 'phase', 'wherein', 'you', 'became', 'ignorant', 'to', 'the', 'people', 'you', 'once', 'loved', ',', 'completely', 'disregard', '##ing', 'their', 'feelings', '/', 'lives', 'so', 'you', 'get', 'to', 'have', 'something', 'go', 'your', 'way', 'and', 'feel', 'temporarily', 'at', 'ease', '.', 'how', 'did', 'things', 'change', '?', '[SEP]']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeeFp_8bk6yR"
      },
      "source": [
        "input_ids=[]\n",
        "for i in range(len(tokenized_texts)):\n",
        "  input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XykMtSn9xdhg",
        "outputId": "8da9385f-7c01-4591-a0a6-e16513941e36"
      },
      "source": [
        "print(input_ids[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[101, 2129, 2106, 5447, 17934, 2156, 2037, 2874, 2004, 1037, 3842, 1999, 1996, 4120, 1029, 102], [101, 2079, 2017, 2031, 2019, 4233, 3899, 1010, 2129, 2052, 2017, 8627, 2111, 2000, 11092, 1998, 2025, 4497, 1029, 102], [101, 2339, 2515, 10146, 7461, 2051, 1029, 2515, 10146, 7461, 2686, 10988, 1029, 102], [101, 2129, 2106, 8064, 3854, 19739, 22420, 3489, 2109, 1996, 23848, 3207, 4645, 14130, 2015, 1029, 102], [101, 2064, 1045, 10463, 18318, 2527, 2002, 10415, 2239, 1040, 2000, 1037, 3137, 7997, 2011, 2074, 5278, 1996, 24656, 1029, 102], [101, 2003, 14474, 3254, 3352, 24363, 1010, 4830, 7507, 2226, 2030, 29461, 16558, 19839, 2050, 2005, 21524, 1029, 102], [101, 2339, 2515, 22035, 2527, 8073, 7221, 4603, 10740, 2043, 2988, 1010, 2021, 2515, 2025, 2079, 1996, 2168, 2005, 4314, 5328, 1029, 102], [101, 2003, 2009, 4689, 2065, 1045, 9378, 2030, 13387, 2026, 26298, 2125, 1029, 16216, 10867, 2015, 2024, 7249, 1012, 102], [101, 2003, 2045, 2107, 1037, 2518, 2004, 11225, 17844, 1010, 1998, 2065, 2061, 1010, 2129, 2003, 2008, 2367, 2084, 11225, 10754, 2135, 1029, 102], [101, 2003, 2009, 2074, 2033, 2030, 2031, 2017, 2412, 2042, 1999, 2023, 4403, 16726, 2017, 2150, 21591, 2000, 1996, 2111, 2017, 2320, 3866, 1010, 3294, 27770, 2075, 2037, 5346, 1013, 3268, 2061, 2017, 2131, 2000, 2031, 2242, 2175, 2115, 2126, 1998, 2514, 8184, 2012, 7496, 1012, 2129, 2106, 2477, 2689, 1029, 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOrR36wx11G"
      },
      "source": [
        "del tokenized_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsc1BHF90IZb",
        "outputId": "5b4541a5-fe40-451e-f59c-b5ec604d64bf"
      },
      "source": [
        "print(input_ids[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[101, 2129, 2106, 5447, 17934, 2156, 2037, 2874, 2004, 1037, 3842, 1999, 1996, 4120, 1029, 102], [101, 2079, 2017, 2031, 2019, 4233, 3899, 1010, 2129, 2052, 2017, 8627, 2111, 2000, 11092, 1998, 2025, 4497, 1029, 102], [101, 2339, 2515, 10146, 7461, 2051, 1029, 2515, 10146, 7461, 2686, 10988, 1029, 102], [101, 2129, 2106, 8064, 3854, 19739, 22420, 3489, 2109, 1996, 23848, 3207, 4645, 14130, 2015, 1029, 102], [101, 2064, 1045, 10463, 18318, 2527, 2002, 10415, 2239, 1040, 2000, 1037, 3137, 7997, 2011, 2074, 5278, 1996, 24656, 1029, 102], [101, 2003, 14474, 3254, 3352, 24363, 1010, 4830, 7507, 2226, 2030, 29461, 16558, 19839, 2050, 2005, 21524, 1029, 102], [101, 2339, 2515, 22035, 2527, 8073, 7221, 4603, 10740, 2043, 2988, 1010, 2021, 2515, 2025, 2079, 1996, 2168, 2005, 4314, 5328, 1029, 102], [101, 2003, 2009, 4689, 2065, 1045, 9378, 2030, 13387, 2026, 26298, 2125, 1029, 16216, 10867, 2015, 2024, 7249, 1012, 102], [101, 2003, 2045, 2107, 1037, 2518, 2004, 11225, 17844, 1010, 1998, 2065, 2061, 1010, 2129, 2003, 2008, 2367, 2084, 11225, 10754, 2135, 1029, 102], [101, 2003, 2009, 2074, 2033, 2030, 2031, 2017, 2412, 2042, 1999, 2023, 4403, 16726, 2017, 2150, 21591, 2000, 1996, 2111, 2017, 2320, 3866, 1010, 3294, 27770, 2075, 2037, 5346, 1013, 3268, 2061, 2017, 2131, 2000, 2031, 2242, 2175, 2115, 2126, 1998, 2514, 8184, 2012, 7496, 1012, 2129, 2106, 2477, 2689, 1029, 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4s2nJx4j5l6"
      },
      "source": [
        "\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yp0g-4olHfk"
      },
      "source": [
        "#Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCMZYqWslYxV"
      },
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XwiiRaZ5Xpe",
        "outputId": "037f3bce-1f91-4156-93c6-42de9b050c13"
      },
      "source": [
        "print(train_inputs[:10])\n",
        "print(validation_inputs[:10])\n",
        "print(train_labels[:10])\n",
        "print(validation_labels[:10])\n",
        "print(train_masks[:10])\n",
        "print(validation_masks[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 101 2054 2003 ...    0    0    0]\n",
            " [ 101 2054 2785 ...    0    0    0]\n",
            " [ 101 2054 2003 ...    0    0    0]\n",
            " ...\n",
            " [ 101 1045 1005 ...    0    0    0]\n",
            " [ 101 2054 2052 ...    0    0    0]\n",
            " [ 101 2054 2323 ...    0    0    0]]\n",
            "[[ 101 2339 2987 ...    0    0    0]\n",
            " [ 101 2339 2079 ...    0    0    0]\n",
            " [ 101 2054 2003 ...    0    0    0]\n",
            " ...\n",
            " [ 101 2054 2079 ...    0    0    0]\n",
            " [ 101 2054 2024 ...    0    0    0]\n",
            " [ 101 2129 2079 ...    0    0    0]]\n",
            "[0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn1e1UcUmno8"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfU0YZRpnL9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15643a18-e088-4260-f4f0-c976e5ba0795"
      },
      "source": [
        "## Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device ='cpu'\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpqdt9znz_DO"
      },
      "source": [
        "\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMgz_UUvnMz-"
      },
      "source": [
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjpPWycpbf8"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgITTtCRnRXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cd28e2-a661-4a4b-9e0e-69a6eaa9b504"
      },
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  \n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        \n",
        "        train_loss_set.append(loss.item())  \n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [1/65307], Loss: 0.5142\n",
            "Epoch [1/2], Step [51/65307], Loss: 0.2721\n",
            "Epoch [1/2], Step [101/65307], Loss: 0.3752\n",
            "Epoch [1/2], Step [151/65307], Loss: 0.0528\n",
            "Epoch [1/2], Step [201/65307], Loss: 0.1223\n",
            "Epoch [1/2], Step [251/65307], Loss: 0.2222\n",
            "Epoch [1/2], Step [301/65307], Loss: 0.0887\n",
            "Epoch [1/2], Step [351/65307], Loss: 0.0879\n",
            "Epoch [1/2], Step [401/65307], Loss: 0.3002\n",
            "Epoch [1/2], Step [451/65307], Loss: 0.0775\n",
            "Epoch [1/2], Step [501/65307], Loss: 0.3697\n",
            "Epoch [1/2], Step [551/65307], Loss: 0.0843\n",
            "Epoch [1/2], Step [601/65307], Loss: 0.3797\n",
            "Epoch [1/2], Step [651/65307], Loss: 0.2983\n",
            "Epoch [1/2], Step [701/65307], Loss: 0.0812\n",
            "Epoch [1/2], Step [751/65307], Loss: 0.1678\n",
            "Epoch [1/2], Step [801/65307], Loss: 0.0465\n",
            "Epoch [1/2], Step [851/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [901/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [951/65307], Loss: 0.3515\n",
            "Epoch [1/2], Step [1001/65307], Loss: 0.1293\n",
            "Epoch [1/2], Step [1051/65307], Loss: 0.3548\n",
            "Epoch [1/2], Step [1101/65307], Loss: 0.0490\n",
            "Epoch [1/2], Step [1151/65307], Loss: 0.1017\n",
            "Epoch [1/2], Step [1201/65307], Loss: 0.0963\n",
            "Epoch [1/2], Step [1251/65307], Loss: 0.1914\n",
            "Epoch [1/2], Step [1301/65307], Loss: 0.0580\n",
            "Epoch [1/2], Step [1351/65307], Loss: 0.0651\n",
            "Epoch [1/2], Step [1401/65307], Loss: 0.1197\n",
            "Epoch [1/2], Step [1451/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [1501/65307], Loss: 0.2315\n",
            "Epoch [1/2], Step [1551/65307], Loss: 0.0502\n",
            "Epoch [1/2], Step [1601/65307], Loss: 0.1267\n",
            "Epoch [1/2], Step [1651/65307], Loss: 0.1818\n",
            "Epoch [1/2], Step [1701/65307], Loss: 0.1784\n",
            "Epoch [1/2], Step [1751/65307], Loss: 0.4025\n",
            "Epoch [1/2], Step [1801/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [1851/65307], Loss: 0.0411\n",
            "Epoch [1/2], Step [1901/65307], Loss: 0.0071\n",
            "Epoch [1/2], Step [1951/65307], Loss: 0.0780\n",
            "Epoch [1/2], Step [2001/65307], Loss: 0.0541\n",
            "Epoch [1/2], Step [2051/65307], Loss: 0.1283\n",
            "Epoch [1/2], Step [2101/65307], Loss: 0.3529\n",
            "Epoch [1/2], Step [2151/65307], Loss: 0.2780\n",
            "Epoch [1/2], Step [2201/65307], Loss: 0.0389\n",
            "Epoch [1/2], Step [2251/65307], Loss: 0.0708\n",
            "Epoch [1/2], Step [2301/65307], Loss: 0.2834\n",
            "Epoch [1/2], Step [2351/65307], Loss: 0.1124\n",
            "Epoch [1/2], Step [2401/65307], Loss: 0.0843\n",
            "Epoch [1/2], Step [2451/65307], Loss: 0.1299\n",
            "Epoch [1/2], Step [2501/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [2551/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [2601/65307], Loss: 0.0331\n",
            "Epoch [1/2], Step [2651/65307], Loss: 0.0235\n",
            "Epoch [1/2], Step [2701/65307], Loss: 0.0374\n",
            "Epoch [1/2], Step [2751/65307], Loss: 0.1740\n",
            "Epoch [1/2], Step [2801/65307], Loss: 0.0148\n",
            "Epoch [1/2], Step [2851/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [2901/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [2951/65307], Loss: 0.2075\n",
            "Epoch [1/2], Step [3001/65307], Loss: 0.0758\n",
            "Epoch [1/2], Step [3051/65307], Loss: 0.1277\n",
            "Epoch [1/2], Step [3101/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [3151/65307], Loss: 0.0123\n",
            "Epoch [1/2], Step [3201/65307], Loss: 0.1829\n",
            "Epoch [1/2], Step [3251/65307], Loss: 0.6336\n",
            "Epoch [1/2], Step [3301/65307], Loss: 0.0380\n",
            "Epoch [1/2], Step [3351/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [3401/65307], Loss: 0.0612\n",
            "Epoch [1/2], Step [3451/65307], Loss: 0.1863\n",
            "Epoch [1/2], Step [3501/65307], Loss: 0.1718\n",
            "Epoch [1/2], Step [3551/65307], Loss: 0.1129\n",
            "Epoch [1/2], Step [3601/65307], Loss: 0.0304\n",
            "Epoch [1/2], Step [3651/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [3701/65307], Loss: 0.0584\n",
            "Epoch [1/2], Step [3751/65307], Loss: 0.0167\n",
            "Epoch [1/2], Step [3801/65307], Loss: 0.0724\n",
            "Epoch [1/2], Step [3851/65307], Loss: 0.2139\n",
            "Epoch [1/2], Step [3901/65307], Loss: 0.0954\n",
            "Epoch [1/2], Step [3951/65307], Loss: 0.0312\n",
            "Epoch [1/2], Step [4001/65307], Loss: 0.0266\n",
            "Epoch [1/2], Step [4051/65307], Loss: 0.1042\n",
            "Epoch [1/2], Step [4101/65307], Loss: 0.0183\n",
            "Epoch [1/2], Step [4151/65307], Loss: 0.0041\n",
            "Epoch [1/2], Step [4201/65307], Loss: 0.0900\n",
            "Epoch [1/2], Step [4251/65307], Loss: 0.0599\n",
            "Epoch [1/2], Step [4301/65307], Loss: 0.1064\n",
            "Epoch [1/2], Step [4351/65307], Loss: 0.0728\n",
            "Epoch [1/2], Step [4401/65307], Loss: 0.0244\n",
            "Epoch [1/2], Step [4451/65307], Loss: 0.0178\n",
            "Epoch [1/2], Step [4501/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [4551/65307], Loss: 0.0482\n",
            "Epoch [1/2], Step [4601/65307], Loss: 0.3625\n",
            "Epoch [1/2], Step [4651/65307], Loss: 0.0267\n",
            "Epoch [1/2], Step [4701/65307], Loss: 0.0516\n",
            "Epoch [1/2], Step [4751/65307], Loss: 0.1548\n",
            "Epoch [1/2], Step [4801/65307], Loss: 0.1838\n",
            "Epoch [1/2], Step [4851/65307], Loss: 0.1099\n",
            "Epoch [1/2], Step [4901/65307], Loss: 0.0752\n",
            "Epoch [1/2], Step [4951/65307], Loss: 0.1763\n",
            "Epoch [1/2], Step [5001/65307], Loss: 0.0120\n",
            "Epoch [1/2], Step [5051/65307], Loss: 0.0105\n",
            "Epoch [1/2], Step [5101/65307], Loss: 0.0855\n",
            "Epoch [1/2], Step [5151/65307], Loss: 0.2936\n",
            "Epoch [1/2], Step [5201/65307], Loss: 0.1018\n",
            "Epoch [1/2], Step [5251/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [5301/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [5351/65307], Loss: 0.0137\n",
            "Epoch [1/2], Step [5401/65307], Loss: 0.0258\n",
            "Epoch [1/2], Step [5451/65307], Loss: 0.1014\n",
            "Epoch [1/2], Step [5501/65307], Loss: 0.3433\n",
            "Epoch [1/2], Step [5551/65307], Loss: 0.0881\n",
            "Epoch [1/2], Step [5601/65307], Loss: 0.0336\n",
            "Epoch [1/2], Step [5651/65307], Loss: 0.1425\n",
            "Epoch [1/2], Step [5701/65307], Loss: 0.0050\n",
            "Epoch [1/2], Step [5751/65307], Loss: 0.4535\n",
            "Epoch [1/2], Step [5801/65307], Loss: 0.0082\n",
            "Epoch [1/2], Step [5851/65307], Loss: 0.0743\n",
            "Epoch [1/2], Step [5901/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [5951/65307], Loss: 0.0154\n",
            "Epoch [1/2], Step [6001/65307], Loss: 0.0543\n",
            "Epoch [1/2], Step [6051/65307], Loss: 0.0710\n",
            "Epoch [1/2], Step [6101/65307], Loss: 0.1268\n",
            "Epoch [1/2], Step [6151/65307], Loss: 0.1174\n",
            "Epoch [1/2], Step [6201/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [6251/65307], Loss: 0.0113\n",
            "Epoch [1/2], Step [6301/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [6351/65307], Loss: 0.0793\n",
            "Epoch [1/2], Step [6401/65307], Loss: 0.0581\n",
            "Epoch [1/2], Step [6451/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [6501/65307], Loss: 0.2214\n",
            "Epoch [1/2], Step [6551/65307], Loss: 0.0959\n",
            "Epoch [1/2], Step [6601/65307], Loss: 0.1584\n",
            "Epoch [1/2], Step [6651/65307], Loss: 0.1896\n",
            "Epoch [1/2], Step [6701/65307], Loss: 0.0692\n",
            "Epoch [1/2], Step [6751/65307], Loss: 0.4911\n",
            "Epoch [1/2], Step [6801/65307], Loss: 0.2443\n",
            "Epoch [1/2], Step [6851/65307], Loss: 0.3965\n",
            "Epoch [1/2], Step [6901/65307], Loss: 0.0142\n",
            "Epoch [1/2], Step [6951/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [7001/65307], Loss: 0.3694\n",
            "Epoch [1/2], Step [7051/65307], Loss: 0.1269\n",
            "Epoch [1/2], Step [7101/65307], Loss: 0.1088\n",
            "Epoch [1/2], Step [7151/65307], Loss: 0.1417\n",
            "Epoch [1/2], Step [7201/65307], Loss: 0.0861\n",
            "Epoch [1/2], Step [7251/65307], Loss: 0.1492\n",
            "Epoch [1/2], Step [7301/65307], Loss: 0.0136\n",
            "Epoch [1/2], Step [7351/65307], Loss: 0.3430\n",
            "Epoch [1/2], Step [7401/65307], Loss: 0.1805\n",
            "Epoch [1/2], Step [7451/65307], Loss: 0.0357\n",
            "Epoch [1/2], Step [7501/65307], Loss: 0.3232\n",
            "Epoch [1/2], Step [7551/65307], Loss: 0.0792\n",
            "Epoch [1/2], Step [7601/65307], Loss: 0.2856\n",
            "Epoch [1/2], Step [7651/65307], Loss: 0.0128\n",
            "Epoch [1/2], Step [7701/65307], Loss: 0.1001\n",
            "Epoch [1/2], Step [7751/65307], Loss: 0.2761\n",
            "Epoch [1/2], Step [7801/65307], Loss: 0.1539\n",
            "Epoch [1/2], Step [7851/65307], Loss: 0.1436\n",
            "Epoch [1/2], Step [7901/65307], Loss: 0.0567\n",
            "Epoch [1/2], Step [7951/65307], Loss: 0.2854\n",
            "Epoch [1/2], Step [8001/65307], Loss: 0.0211\n",
            "Epoch [1/2], Step [8051/65307], Loss: 0.1593\n",
            "Epoch [1/2], Step [8101/65307], Loss: 0.1335\n",
            "Epoch [1/2], Step [8151/65307], Loss: 0.1995\n",
            "Epoch [1/2], Step [8201/65307], Loss: 0.1026\n",
            "Epoch [1/2], Step [8251/65307], Loss: 0.2622\n",
            "Epoch [1/2], Step [8301/65307], Loss: 0.0177\n",
            "Epoch [1/2], Step [8351/65307], Loss: 0.0854\n",
            "Epoch [1/2], Step [8401/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [8451/65307], Loss: 0.0049\n",
            "Epoch [1/2], Step [8501/65307], Loss: 0.0562\n",
            "Epoch [1/2], Step [8551/65307], Loss: 0.1583\n",
            "Epoch [1/2], Step [8601/65307], Loss: 0.6197\n",
            "Epoch [1/2], Step [8651/65307], Loss: 0.1242\n",
            "Epoch [1/2], Step [8701/65307], Loss: 0.0674\n",
            "Epoch [1/2], Step [8751/65307], Loss: 0.0582\n",
            "Epoch [1/2], Step [8801/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [8851/65307], Loss: 0.0659\n",
            "Epoch [1/2], Step [8901/65307], Loss: 0.2496\n",
            "Epoch [1/2], Step [8951/65307], Loss: 0.0669\n",
            "Epoch [1/2], Step [9001/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [9051/65307], Loss: 0.3209\n",
            "Epoch [1/2], Step [9101/65307], Loss: 0.0536\n",
            "Epoch [1/2], Step [9151/65307], Loss: 0.0846\n",
            "Epoch [1/2], Step [9201/65307], Loss: 0.1342\n",
            "Epoch [1/2], Step [9251/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [9301/65307], Loss: 0.0273\n",
            "Epoch [1/2], Step [9351/65307], Loss: 0.2558\n",
            "Epoch [1/2], Step [9401/65307], Loss: 0.2235\n",
            "Epoch [1/2], Step [9451/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [9501/65307], Loss: 0.5653\n",
            "Epoch [1/2], Step [9551/65307], Loss: 0.1145\n",
            "Epoch [1/2], Step [9601/65307], Loss: 0.1468\n",
            "Epoch [1/2], Step [9651/65307], Loss: 0.0300\n",
            "Epoch [1/2], Step [9701/65307], Loss: 0.0901\n",
            "Epoch [1/2], Step [9751/65307], Loss: 0.0033\n",
            "Epoch [1/2], Step [9801/65307], Loss: 0.1382\n",
            "Epoch [1/2], Step [9851/65307], Loss: 0.0110\n",
            "Epoch [1/2], Step [9901/65307], Loss: 0.1859\n",
            "Epoch [1/2], Step [9951/65307], Loss: 0.0558\n",
            "Epoch [1/2], Step [10001/65307], Loss: 0.2342\n",
            "Epoch [1/2], Step [10051/65307], Loss: 0.6773\n",
            "Epoch [1/2], Step [10101/65307], Loss: 0.0308\n",
            "Epoch [1/2], Step [10151/65307], Loss: 0.0470\n",
            "Epoch [1/2], Step [10201/65307], Loss: 0.0545\n",
            "Epoch [1/2], Step [10251/65307], Loss: 0.0211\n",
            "Epoch [1/2], Step [10301/65307], Loss: 0.0781\n",
            "Epoch [1/2], Step [10351/65307], Loss: 0.0957\n",
            "Epoch [1/2], Step [10401/65307], Loss: 0.2506\n",
            "Epoch [1/2], Step [10451/65307], Loss: 0.0590\n",
            "Epoch [1/2], Step [10501/65307], Loss: 0.0850\n",
            "Epoch [1/2], Step [10551/65307], Loss: 0.2954\n",
            "Epoch [1/2], Step [10601/65307], Loss: 0.0628\n",
            "Epoch [1/2], Step [10651/65307], Loss: 0.0653\n",
            "Epoch [1/2], Step [10701/65307], Loss: 0.0961\n",
            "Epoch [1/2], Step [10751/65307], Loss: 0.0045\n",
            "Epoch [1/2], Step [10801/65307], Loss: 0.1509\n",
            "Epoch [1/2], Step [10851/65307], Loss: 0.0260\n",
            "Epoch [1/2], Step [10901/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [10951/65307], Loss: 0.0252\n",
            "Epoch [1/2], Step [11001/65307], Loss: 0.0820\n",
            "Epoch [1/2], Step [11051/65307], Loss: 0.0112\n",
            "Epoch [1/2], Step [11101/65307], Loss: 0.1440\n",
            "Epoch [1/2], Step [11151/65307], Loss: 0.1255\n",
            "Epoch [1/2], Step [11201/65307], Loss: 0.0715\n",
            "Epoch [1/2], Step [11251/65307], Loss: 0.2277\n",
            "Epoch [1/2], Step [11301/65307], Loss: 0.0711\n",
            "Epoch [1/2], Step [11351/65307], Loss: 0.1067\n",
            "Epoch [1/2], Step [11401/65307], Loss: 0.0238\n",
            "Epoch [1/2], Step [11451/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [11501/65307], Loss: 0.1405\n",
            "Epoch [1/2], Step [11551/65307], Loss: 0.0326\n",
            "Epoch [1/2], Step [11601/65307], Loss: 0.0428\n",
            "Epoch [1/2], Step [11651/65307], Loss: 0.1023\n",
            "Epoch [1/2], Step [11701/65307], Loss: 0.1033\n",
            "Epoch [1/2], Step [11751/65307], Loss: 0.1668\n",
            "Epoch [1/2], Step [11801/65307], Loss: 0.0991\n",
            "Epoch [1/2], Step [11851/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [11901/65307], Loss: 0.0054\n",
            "Epoch [1/2], Step [11951/65307], Loss: 0.0953\n",
            "Epoch [1/2], Step [12001/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [12051/65307], Loss: 0.0980\n",
            "Epoch [1/2], Step [12101/65307], Loss: 0.0802\n",
            "Epoch [1/2], Step [12151/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [12201/65307], Loss: 0.3044\n",
            "Epoch [1/2], Step [12251/65307], Loss: 0.1176\n",
            "Epoch [1/2], Step [12301/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [12351/65307], Loss: 0.0191\n",
            "Epoch [1/2], Step [12401/65307], Loss: 0.0294\n",
            "Epoch [1/2], Step [12451/65307], Loss: 0.0889\n",
            "Epoch [1/2], Step [12501/65307], Loss: 0.0045\n",
            "Epoch [1/2], Step [12551/65307], Loss: 0.0389\n",
            "Epoch [1/2], Step [12601/65307], Loss: 0.0184\n",
            "Epoch [1/2], Step [12651/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [12701/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [12751/65307], Loss: 0.2002\n",
            "Epoch [1/2], Step [12801/65307], Loss: 0.2533\n",
            "Epoch [1/2], Step [12851/65307], Loss: 0.0388\n",
            "Epoch [1/2], Step [12901/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [12951/65307], Loss: 0.1537\n",
            "Epoch [1/2], Step [13001/65307], Loss: 0.1118\n",
            "Epoch [1/2], Step [13051/65307], Loss: 0.0115\n",
            "Epoch [1/2], Step [13101/65307], Loss: 0.1560\n",
            "Epoch [1/2], Step [13151/65307], Loss: 0.0176\n",
            "Epoch [1/2], Step [13201/65307], Loss: 0.0055\n",
            "Epoch [1/2], Step [13251/65307], Loss: 0.0799\n",
            "Epoch [1/2], Step [13301/65307], Loss: 0.0638\n",
            "Epoch [1/2], Step [13351/65307], Loss: 0.1898\n",
            "Epoch [1/2], Step [13401/65307], Loss: 0.1119\n",
            "Epoch [1/2], Step [13451/65307], Loss: 0.0496\n",
            "Epoch [1/2], Step [13501/65307], Loss: 0.0721\n",
            "Epoch [1/2], Step [13551/65307], Loss: 0.3119\n",
            "Epoch [1/2], Step [13601/65307], Loss: 0.0811\n",
            "Epoch [1/2], Step [13651/65307], Loss: 0.1109\n",
            "Epoch [1/2], Step [13701/65307], Loss: 0.1438\n",
            "Epoch [1/2], Step [13751/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [13801/65307], Loss: 0.3468\n",
            "Epoch [1/2], Step [13851/65307], Loss: 0.1013\n",
            "Epoch [1/2], Step [13901/65307], Loss: 0.0945\n",
            "Epoch [1/2], Step [13951/65307], Loss: 0.2127\n",
            "Epoch [1/2], Step [14001/65307], Loss: 0.0306\n",
            "Epoch [1/2], Step [14051/65307], Loss: 0.1529\n",
            "Epoch [1/2], Step [14101/65307], Loss: 0.1455\n",
            "Epoch [1/2], Step [14151/65307], Loss: 0.5134\n",
            "Epoch [1/2], Step [14201/65307], Loss: 0.2321\n",
            "Epoch [1/2], Step [14251/65307], Loss: 0.3539\n",
            "Epoch [1/2], Step [14301/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [14351/65307], Loss: 0.0054\n",
            "Epoch [1/2], Step [14401/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [14451/65307], Loss: 0.2170\n",
            "Epoch [1/2], Step [14501/65307], Loss: 0.1114\n",
            "Epoch [1/2], Step [14551/65307], Loss: 0.0069\n",
            "Epoch [1/2], Step [14601/65307], Loss: 0.2578\n",
            "Epoch [1/2], Step [14651/65307], Loss: 0.1938\n",
            "Epoch [1/2], Step [14701/65307], Loss: 0.0460\n",
            "Epoch [1/2], Step [14751/65307], Loss: 0.1766\n",
            "Epoch [1/2], Step [14801/65307], Loss: 0.0560\n",
            "Epoch [1/2], Step [14851/65307], Loss: 0.2169\n",
            "Epoch [1/2], Step [14901/65307], Loss: 0.0683\n",
            "Epoch [1/2], Step [14951/65307], Loss: 0.0791\n",
            "Epoch [1/2], Step [15001/65307], Loss: 0.1181\n",
            "Epoch [1/2], Step [15051/65307], Loss: 0.0930\n",
            "Epoch [1/2], Step [15101/65307], Loss: 0.1195\n",
            "Epoch [1/2], Step [15151/65307], Loss: 0.0200\n",
            "Epoch [1/2], Step [15201/65307], Loss: 0.0363\n",
            "Epoch [1/2], Step [15251/65307], Loss: 0.1168\n",
            "Epoch [1/2], Step [15301/65307], Loss: 0.1055\n",
            "Epoch [1/2], Step [15351/65307], Loss: 0.0868\n",
            "Epoch [1/2], Step [15401/65307], Loss: 0.0582\n",
            "Epoch [1/2], Step [15451/65307], Loss: 0.1854\n",
            "Epoch [1/2], Step [15501/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [15551/65307], Loss: 0.2309\n",
            "Epoch [1/2], Step [15601/65307], Loss: 0.0277\n",
            "Epoch [1/2], Step [15651/65307], Loss: 0.0897\n",
            "Epoch [1/2], Step [15701/65307], Loss: 0.0914\n",
            "Epoch [1/2], Step [15751/65307], Loss: 0.3107\n",
            "Epoch [1/2], Step [15801/65307], Loss: 0.2924\n",
            "Epoch [1/2], Step [15851/65307], Loss: 0.0758\n",
            "Epoch [1/2], Step [15901/65307], Loss: 0.3135\n",
            "Epoch [1/2], Step [15951/65307], Loss: 0.2521\n",
            "Epoch [1/2], Step [16001/65307], Loss: 0.0656\n",
            "Epoch [1/2], Step [16051/65307], Loss: 0.0184\n",
            "Epoch [1/2], Step [16101/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [16151/65307], Loss: 0.1243\n",
            "Epoch [1/2], Step [16201/65307], Loss: 0.0330\n",
            "Epoch [1/2], Step [16251/65307], Loss: 0.0927\n",
            "Epoch [1/2], Step [16301/65307], Loss: 0.1039\n",
            "Epoch [1/2], Step [16351/65307], Loss: 0.0145\n",
            "Epoch [1/2], Step [16401/65307], Loss: 0.0532\n",
            "Epoch [1/2], Step [16451/65307], Loss: 0.2251\n",
            "Epoch [1/2], Step [16501/65307], Loss: 0.0357\n",
            "Epoch [1/2], Step [16551/65307], Loss: 0.2006\n",
            "Epoch [1/2], Step [16601/65307], Loss: 0.0136\n",
            "Epoch [1/2], Step [16651/65307], Loss: 0.0100\n",
            "Epoch [1/2], Step [16701/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [16751/65307], Loss: 0.0624\n",
            "Epoch [1/2], Step [16801/65307], Loss: 0.0768\n",
            "Epoch [1/2], Step [16851/65307], Loss: 0.1032\n",
            "Epoch [1/2], Step [16901/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [16951/65307], Loss: 0.0180\n",
            "Epoch [1/2], Step [17001/65307], Loss: 0.1280\n",
            "Epoch [1/2], Step [17051/65307], Loss: 0.2461\n",
            "Epoch [1/2], Step [17101/65307], Loss: 0.0282\n",
            "Epoch [1/2], Step [17151/65307], Loss: 0.0407\n",
            "Epoch [1/2], Step [17201/65307], Loss: 0.0770\n",
            "Epoch [1/2], Step [17251/65307], Loss: 0.0922\n",
            "Epoch [1/2], Step [17301/65307], Loss: 0.0342\n",
            "Epoch [1/2], Step [17351/65307], Loss: 0.0493\n",
            "Epoch [1/2], Step [17401/65307], Loss: 0.0222\n",
            "Epoch [1/2], Step [17451/65307], Loss: 0.0587\n",
            "Epoch [1/2], Step [17501/65307], Loss: 0.2307\n",
            "Epoch [1/2], Step [17551/65307], Loss: 0.0515\n",
            "Epoch [1/2], Step [17601/65307], Loss: 0.0947\n",
            "Epoch [1/2], Step [17651/65307], Loss: 0.1970\n",
            "Epoch [1/2], Step [17701/65307], Loss: 0.0305\n",
            "Epoch [1/2], Step [17751/65307], Loss: 0.1327\n",
            "Epoch [1/2], Step [17801/65307], Loss: 0.6887\n",
            "Epoch [1/2], Step [17851/65307], Loss: 0.2151\n",
            "Epoch [1/2], Step [17901/65307], Loss: 0.0479\n",
            "Epoch [1/2], Step [17951/65307], Loss: 0.1529\n",
            "Epoch [1/2], Step [18001/65307], Loss: 0.0748\n",
            "Epoch [1/2], Step [18051/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [18101/65307], Loss: 0.0404\n",
            "Epoch [1/2], Step [18151/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [18201/65307], Loss: 0.1334\n",
            "Epoch [1/2], Step [18251/65307], Loss: 0.0559\n",
            "Epoch [1/2], Step [18301/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [18351/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [18401/65307], Loss: 0.0480\n",
            "Epoch [1/2], Step [18451/65307], Loss: 0.0091\n",
            "Epoch [1/2], Step [18501/65307], Loss: 0.2612\n",
            "Epoch [1/2], Step [18551/65307], Loss: 0.0068\n",
            "Epoch [1/2], Step [18601/65307], Loss: 0.5313\n",
            "Epoch [1/2], Step [18651/65307], Loss: 0.0216\n",
            "Epoch [1/2], Step [18701/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [18751/65307], Loss: 0.0971\n",
            "Epoch [1/2], Step [18801/65307], Loss: 0.0783\n",
            "Epoch [1/2], Step [18851/65307], Loss: 0.1254\n",
            "Epoch [1/2], Step [18901/65307], Loss: 0.0585\n",
            "Epoch [1/2], Step [18951/65307], Loss: 0.1152\n",
            "Epoch [1/2], Step [19001/65307], Loss: 0.0100\n",
            "Epoch [1/2], Step [19051/65307], Loss: 0.3770\n",
            "Epoch [1/2], Step [19101/65307], Loss: 0.1511\n",
            "Epoch [1/2], Step [19151/65307], Loss: 0.3039\n",
            "Epoch [1/2], Step [19201/65307], Loss: 0.1984\n",
            "Epoch [1/2], Step [19251/65307], Loss: 0.0346\n",
            "Epoch [1/2], Step [19301/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [19351/65307], Loss: 0.0461\n",
            "Epoch [1/2], Step [19401/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [19451/65307], Loss: 0.0185\n",
            "Epoch [1/2], Step [19501/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [19551/65307], Loss: 0.0141\n",
            "Epoch [1/2], Step [19601/65307], Loss: 0.1262\n",
            "Epoch [1/2], Step [19651/65307], Loss: 0.0878\n",
            "Epoch [1/2], Step [19701/65307], Loss: 0.0213\n",
            "Epoch [1/2], Step [19751/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [19801/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [1/2], Step [19901/65307], Loss: 0.4285\n",
            "Epoch [1/2], Step [19951/65307], Loss: 0.3114\n",
            "Epoch [1/2], Step [20001/65307], Loss: 0.0316\n",
            "Epoch [1/2], Step [20051/65307], Loss: 0.0130\n",
            "Epoch [1/2], Step [20101/65307], Loss: 0.1009\n",
            "Epoch [1/2], Step [20151/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [20201/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [20251/65307], Loss: 0.2136\n",
            "Epoch [1/2], Step [20301/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [20351/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [20401/65307], Loss: 0.0408\n",
            "Epoch [1/2], Step [20451/65307], Loss: 0.0209\n",
            "Epoch [1/2], Step [20501/65307], Loss: 0.1361\n",
            "Epoch [1/2], Step [20551/65307], Loss: 0.2432\n",
            "Epoch [1/2], Step [20601/65307], Loss: 0.1074\n",
            "Epoch [1/2], Step [20651/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [20701/65307], Loss: 0.0558\n",
            "Epoch [1/2], Step [20751/65307], Loss: 0.1388\n",
            "Epoch [1/2], Step [20801/65307], Loss: 0.3362\n",
            "Epoch [1/2], Step [20851/65307], Loss: 0.1214\n",
            "Epoch [1/2], Step [20901/65307], Loss: 0.3107\n",
            "Epoch [1/2], Step [20951/65307], Loss: 0.0485\n",
            "Epoch [1/2], Step [21001/65307], Loss: 0.0293\n",
            "Epoch [1/2], Step [21051/65307], Loss: 0.0904\n",
            "Epoch [1/2], Step [21101/65307], Loss: 0.0308\n",
            "Epoch [1/2], Step [21151/65307], Loss: 0.0639\n",
            "Epoch [1/2], Step [21201/65307], Loss: 0.0544\n",
            "Epoch [1/2], Step [21251/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [21301/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [21351/65307], Loss: 0.0354\n",
            "Epoch [1/2], Step [21401/65307], Loss: 0.1113\n",
            "Epoch [1/2], Step [21451/65307], Loss: 0.1289\n",
            "Epoch [1/2], Step [21501/65307], Loss: 0.0200\n",
            "Epoch [1/2], Step [21551/65307], Loss: 0.0893\n",
            "Epoch [1/2], Step [21601/65307], Loss: 0.0233\n",
            "Epoch [1/2], Step [21651/65307], Loss: 0.1867\n",
            "Epoch [1/2], Step [21701/65307], Loss: 0.0216\n",
            "Epoch [1/2], Step [21751/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [21801/65307], Loss: 0.3973\n",
            "Epoch [1/2], Step [21851/65307], Loss: 0.1303\n",
            "Epoch [1/2], Step [21901/65307], Loss: 0.0068\n",
            "Epoch [1/2], Step [21951/65307], Loss: 0.0122\n",
            "Epoch [1/2], Step [22001/65307], Loss: 0.1552\n",
            "Epoch [1/2], Step [22051/65307], Loss: 0.0062\n",
            "Epoch [1/2], Step [22101/65307], Loss: 0.0405\n",
            "Epoch [1/2], Step [22151/65307], Loss: 0.0017\n",
            "Epoch [1/2], Step [22201/65307], Loss: 0.0509\n",
            "Epoch [1/2], Step [22251/65307], Loss: 0.0985\n",
            "Epoch [1/2], Step [22301/65307], Loss: 0.0067\n",
            "Epoch [1/2], Step [22351/65307], Loss: 0.5207\n",
            "Epoch [1/2], Step [22401/65307], Loss: 0.0469\n",
            "Epoch [1/2], Step [22451/65307], Loss: 0.0696\n",
            "Epoch [1/2], Step [22501/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [22551/65307], Loss: 0.1831\n",
            "Epoch [1/2], Step [22601/65307], Loss: 0.0902\n",
            "Epoch [1/2], Step [22651/65307], Loss: 0.1192\n",
            "Epoch [1/2], Step [22701/65307], Loss: 0.2767\n",
            "Epoch [1/2], Step [22751/65307], Loss: 0.0220\n",
            "Epoch [1/2], Step [22801/65307], Loss: 0.1211\n",
            "Epoch [1/2], Step [22851/65307], Loss: 0.0982\n",
            "Epoch [1/2], Step [22901/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [22951/65307], Loss: 0.0776\n",
            "Epoch [1/2], Step [23001/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [23051/65307], Loss: 0.2064\n",
            "Epoch [1/2], Step [23101/65307], Loss: 0.0042\n",
            "Epoch [1/2], Step [23151/65307], Loss: 0.0524\n",
            "Epoch [1/2], Step [23201/65307], Loss: 0.0945\n",
            "Epoch [1/2], Step [23251/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [23301/65307], Loss: 0.3614\n",
            "Epoch [1/2], Step [23351/65307], Loss: 0.0784\n",
            "Epoch [1/2], Step [23401/65307], Loss: 0.0126\n",
            "Epoch [1/2], Step [23451/65307], Loss: 0.1622\n",
            "Epoch [1/2], Step [23501/65307], Loss: 0.0911\n",
            "Epoch [1/2], Step [23551/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [23601/65307], Loss: 0.0010\n",
            "Epoch [1/2], Step [23651/65307], Loss: 0.2819\n",
            "Epoch [1/2], Step [23701/65307], Loss: 0.0686\n",
            "Epoch [1/2], Step [23751/65307], Loss: 0.0822\n",
            "Epoch [1/2], Step [23801/65307], Loss: 0.3872\n",
            "Epoch [1/2], Step [23851/65307], Loss: 0.0726\n",
            "Epoch [1/2], Step [23901/65307], Loss: 0.0570\n",
            "Epoch [1/2], Step [23951/65307], Loss: 0.0742\n",
            "Epoch [1/2], Step [24001/65307], Loss: 0.0250\n",
            "Epoch [1/2], Step [24051/65307], Loss: 0.4306\n",
            "Epoch [1/2], Step [24101/65307], Loss: 0.0148\n",
            "Epoch [1/2], Step [24151/65307], Loss: 0.0805\n",
            "Epoch [1/2], Step [24201/65307], Loss: 0.4834\n",
            "Epoch [1/2], Step [24251/65307], Loss: 0.0223\n",
            "Epoch [1/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [24351/65307], Loss: 0.2527\n",
            "Epoch [1/2], Step [24401/65307], Loss: 0.1315\n",
            "Epoch [1/2], Step [24451/65307], Loss: 0.1656\n",
            "Epoch [1/2], Step [24501/65307], Loss: 0.0554\n",
            "Epoch [1/2], Step [24551/65307], Loss: 0.1652\n",
            "Epoch [1/2], Step [24601/65307], Loss: 0.1548\n",
            "Epoch [1/2], Step [24651/65307], Loss: 0.0323\n",
            "Epoch [1/2], Step [24701/65307], Loss: 0.0455\n",
            "Epoch [1/2], Step [24751/65307], Loss: 0.5137\n",
            "Epoch [1/2], Step [24801/65307], Loss: 0.0624\n",
            "Epoch [1/2], Step [24851/65307], Loss: 0.0681\n",
            "Epoch [1/2], Step [24901/65307], Loss: 0.0554\n",
            "Epoch [1/2], Step [24951/65307], Loss: 0.0413\n",
            "Epoch [1/2], Step [25001/65307], Loss: 0.7531\n",
            "Epoch [1/2], Step [25051/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [25101/65307], Loss: 0.2371\n",
            "Epoch [1/2], Step [25151/65307], Loss: 0.0817\n",
            "Epoch [1/2], Step [25201/65307], Loss: 0.1731\n",
            "Epoch [1/2], Step [25251/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [25301/65307], Loss: 0.3550\n",
            "Epoch [1/2], Step [25351/65307], Loss: 0.2886\n",
            "Epoch [1/2], Step [25401/65307], Loss: 0.2273\n",
            "Epoch [1/2], Step [25451/65307], Loss: 0.1246\n",
            "Epoch [1/2], Step [25501/65307], Loss: 0.0508\n",
            "Epoch [1/2], Step [25551/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [25601/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [25651/65307], Loss: 0.1267\n",
            "Epoch [1/2], Step [25701/65307], Loss: 0.0217\n",
            "Epoch [1/2], Step [25751/65307], Loss: 0.0608\n",
            "Epoch [1/2], Step [25801/65307], Loss: 0.0366\n",
            "Epoch [1/2], Step [25851/65307], Loss: 0.0611\n",
            "Epoch [1/2], Step [25901/65307], Loss: 0.0341\n",
            "Epoch [1/2], Step [25951/65307], Loss: 0.1371\n",
            "Epoch [1/2], Step [26001/65307], Loss: 0.3328\n",
            "Epoch [1/2], Step [26051/65307], Loss: 0.4588\n",
            "Epoch [1/2], Step [26101/65307], Loss: 0.0421\n",
            "Epoch [1/2], Step [26151/65307], Loss: 0.0301\n",
            "Epoch [1/2], Step [26201/65307], Loss: 0.1631\n",
            "Epoch [1/2], Step [26251/65307], Loss: 0.0401\n",
            "Epoch [1/2], Step [26301/65307], Loss: 0.4243\n",
            "Epoch [1/2], Step [26351/65307], Loss: 0.2530\n",
            "Epoch [1/2], Step [26401/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [26451/65307], Loss: 0.2705\n",
            "Epoch [1/2], Step [26501/65307], Loss: 0.1697\n",
            "Epoch [1/2], Step [26551/65307], Loss: 0.0203\n",
            "Epoch [1/2], Step [26601/65307], Loss: 0.0531\n",
            "Epoch [1/2], Step [26651/65307], Loss: 0.0323\n",
            "Epoch [1/2], Step [26701/65307], Loss: 0.0345\n",
            "Epoch [1/2], Step [26751/65307], Loss: 0.3624\n",
            "Epoch [1/2], Step [26801/65307], Loss: 0.3698\n",
            "Epoch [1/2], Step [26851/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [26901/65307], Loss: 0.0972\n",
            "Epoch [1/2], Step [26951/65307], Loss: 0.1593\n",
            "Epoch [1/2], Step [27001/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [27051/65307], Loss: 0.0515\n",
            "Epoch [1/2], Step [27101/65307], Loss: 0.0129\n",
            "Epoch [1/2], Step [27151/65307], Loss: 0.0119\n",
            "Epoch [1/2], Step [27201/65307], Loss: 0.0522\n",
            "Epoch [1/2], Step [27251/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [27301/65307], Loss: 0.1798\n",
            "Epoch [1/2], Step [27351/65307], Loss: 0.0091\n",
            "Epoch [1/2], Step [27401/65307], Loss: 0.4129\n",
            "Epoch [1/2], Step [27451/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [27501/65307], Loss: 0.0023\n",
            "Epoch [1/2], Step [27551/65307], Loss: 0.0657\n",
            "Epoch [1/2], Step [27601/65307], Loss: 0.2478\n",
            "Epoch [1/2], Step [27651/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [27701/65307], Loss: 0.2880\n",
            "Epoch [1/2], Step [27751/65307], Loss: 0.0251\n",
            "Epoch [1/2], Step [27801/65307], Loss: 0.0502\n",
            "Epoch [1/2], Step [27851/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [27901/65307], Loss: 0.1288\n",
            "Epoch [1/2], Step [27951/65307], Loss: 0.0032\n",
            "Epoch [1/2], Step [28001/65307], Loss: 0.2214\n",
            "Epoch [1/2], Step [28051/65307], Loss: 0.0831\n",
            "Epoch [1/2], Step [28101/65307], Loss: 0.0251\n",
            "Epoch [1/2], Step [28151/65307], Loss: 0.1360\n",
            "Epoch [1/2], Step [28201/65307], Loss: 0.1204\n",
            "Epoch [1/2], Step [28251/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [28301/65307], Loss: 0.1201\n",
            "Epoch [1/2], Step [28351/65307], Loss: 0.0231\n",
            "Epoch [1/2], Step [28401/65307], Loss: 0.0798\n",
            "Epoch [1/2], Step [28451/65307], Loss: 0.0975\n",
            "Epoch [1/2], Step [28501/65307], Loss: 0.2700\n",
            "Epoch [1/2], Step [28551/65307], Loss: 0.0501\n",
            "Epoch [1/2], Step [28601/65307], Loss: 0.0353\n",
            "Epoch [1/2], Step [28651/65307], Loss: 0.0080\n",
            "Epoch [1/2], Step [28701/65307], Loss: 0.1344\n",
            "Epoch [1/2], Step [28751/65307], Loss: 0.0082\n",
            "Epoch [1/2], Step [28801/65307], Loss: 0.1507\n",
            "Epoch [1/2], Step [28851/65307], Loss: 0.0175\n",
            "Epoch [1/2], Step [28901/65307], Loss: 0.1259\n",
            "Epoch [1/2], Step [28951/65307], Loss: 0.0261\n",
            "Epoch [1/2], Step [29001/65307], Loss: 0.5826\n",
            "Epoch [1/2], Step [29051/65307], Loss: 0.0259\n",
            "Epoch [1/2], Step [29101/65307], Loss: 0.0636\n",
            "Epoch [1/2], Step [29151/65307], Loss: 0.0063\n",
            "Epoch [1/2], Step [29201/65307], Loss: 0.0430\n",
            "Epoch [1/2], Step [29251/65307], Loss: 0.0025\n",
            "Epoch [1/2], Step [29301/65307], Loss: 0.1822\n",
            "Epoch [1/2], Step [29351/65307], Loss: 0.0847\n",
            "Epoch [1/2], Step [29401/65307], Loss: 0.1699\n",
            "Epoch [1/2], Step [29451/65307], Loss: 0.2234\n",
            "Epoch [1/2], Step [29501/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [29551/65307], Loss: 0.0455\n",
            "Epoch [1/2], Step [29601/65307], Loss: 0.0587\n",
            "Epoch [1/2], Step [29651/65307], Loss: 0.0645\n",
            "Epoch [1/2], Step [29701/65307], Loss: 0.0872\n",
            "Epoch [1/2], Step [29751/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [29801/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [29851/65307], Loss: 0.0031\n",
            "Epoch [1/2], Step [29901/65307], Loss: 0.2896\n",
            "Epoch [1/2], Step [29951/65307], Loss: 0.1605\n",
            "Epoch [1/2], Step [30001/65307], Loss: 0.0398\n",
            "Epoch [1/2], Step [30051/65307], Loss: 0.0802\n",
            "Epoch [1/2], Step [30101/65307], Loss: 0.0167\n",
            "Epoch [1/2], Step [30151/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [30201/65307], Loss: 0.0732\n",
            "Epoch [1/2], Step [30251/65307], Loss: 0.0101\n",
            "Epoch [1/2], Step [30301/65307], Loss: 0.0490\n",
            "Epoch [1/2], Step [30351/65307], Loss: 0.0635\n",
            "Epoch [1/2], Step [30401/65307], Loss: 0.0986\n",
            "Epoch [1/2], Step [30451/65307], Loss: 0.0125\n",
            "Epoch [1/2], Step [30501/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [30551/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [30601/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [30651/65307], Loss: 0.0037\n",
            "Epoch [1/2], Step [30701/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [30751/65307], Loss: 0.1954\n",
            "Epoch [1/2], Step [30801/65307], Loss: 0.0846\n",
            "Epoch [1/2], Step [30851/65307], Loss: 0.0689\n",
            "Epoch [1/2], Step [30901/65307], Loss: 0.0723\n",
            "Epoch [1/2], Step [30951/65307], Loss: 0.0392\n",
            "Epoch [1/2], Step [31001/65307], Loss: 0.0128\n",
            "Epoch [1/2], Step [31051/65307], Loss: 0.0525\n",
            "Epoch [1/2], Step [31101/65307], Loss: 0.0097\n",
            "Epoch [1/2], Step [31151/65307], Loss: 0.2541\n",
            "Epoch [1/2], Step [31201/65307], Loss: 0.0330\n",
            "Epoch [1/2], Step [31251/65307], Loss: 0.0494\n",
            "Epoch [1/2], Step [31301/65307], Loss: 0.1192\n",
            "Epoch [1/2], Step [31351/65307], Loss: 0.0603\n",
            "Epoch [1/2], Step [31401/65307], Loss: 0.0228\n",
            "Epoch [1/2], Step [31451/65307], Loss: 0.1612\n",
            "Epoch [1/2], Step [31501/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [31551/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [31601/65307], Loss: 0.0827\n",
            "Epoch [1/2], Step [31651/65307], Loss: 0.0185\n",
            "Epoch [1/2], Step [31701/65307], Loss: 0.0518\n",
            "Epoch [1/2], Step [31751/65307], Loss: 0.3181\n",
            "Epoch [1/2], Step [31801/65307], Loss: 0.3229\n",
            "Epoch [1/2], Step [31851/65307], Loss: 0.0112\n",
            "Epoch [1/2], Step [31901/65307], Loss: 0.0229\n",
            "Epoch [1/2], Step [31951/65307], Loss: 0.0812\n",
            "Epoch [1/2], Step [32001/65307], Loss: 0.1767\n",
            "Epoch [1/2], Step [32051/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [32101/65307], Loss: 0.0343\n",
            "Epoch [1/2], Step [32151/65307], Loss: 0.0285\n",
            "Epoch [1/2], Step [32201/65307], Loss: 0.1554\n",
            "Epoch [1/2], Step [32251/65307], Loss: 0.1829\n",
            "Epoch [1/2], Step [32301/65307], Loss: 0.0334\n",
            "Epoch [1/2], Step [32351/65307], Loss: 0.2743\n",
            "Epoch [1/2], Step [32401/65307], Loss: 0.0271\n",
            "Epoch [1/2], Step [32451/65307], Loss: 0.0314\n",
            "Epoch [1/2], Step [32501/65307], Loss: 0.1724\n",
            "Epoch [1/2], Step [32551/65307], Loss: 0.0626\n",
            "Epoch [1/2], Step [32601/65307], Loss: 0.0753\n",
            "Epoch [1/2], Step [32651/65307], Loss: 0.2384\n",
            "Epoch [1/2], Step [32701/65307], Loss: 0.6139\n",
            "Epoch [1/2], Step [32751/65307], Loss: 0.0232\n",
            "Epoch [1/2], Step [32801/65307], Loss: 0.0066\n",
            "Epoch [1/2], Step [32851/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [32901/65307], Loss: 0.0542\n",
            "Epoch [1/2], Step [32951/65307], Loss: 0.2202\n",
            "Epoch [1/2], Step [33001/65307], Loss: 0.2028\n",
            "Epoch [1/2], Step [33051/65307], Loss: 0.1998\n",
            "Epoch [1/2], Step [33101/65307], Loss: 0.0633\n",
            "Epoch [1/2], Step [33151/65307], Loss: 0.0851\n",
            "Epoch [1/2], Step [33201/65307], Loss: 0.2239\n",
            "Epoch [1/2], Step [33251/65307], Loss: 0.0163\n",
            "Epoch [1/2], Step [33301/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [33351/65307], Loss: 0.0034\n",
            "Epoch [1/2], Step [33401/65307], Loss: 0.0829\n",
            "Epoch [1/2], Step [33451/65307], Loss: 0.0564\n",
            "Epoch [1/2], Step [33501/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [33551/65307], Loss: 0.0030\n",
            "Epoch [1/2], Step [33601/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [33651/65307], Loss: 0.0095\n",
            "Epoch [1/2], Step [33701/65307], Loss: 0.0666\n",
            "Epoch [1/2], Step [33751/65307], Loss: 0.0329\n",
            "Epoch [1/2], Step [33801/65307], Loss: 0.0791\n",
            "Epoch [1/2], Step [33851/65307], Loss: 0.0032\n",
            "Epoch [1/2], Step [33901/65307], Loss: 0.0327\n",
            "Epoch [1/2], Step [33951/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [34001/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [34051/65307], Loss: 0.1505\n",
            "Epoch [1/2], Step [34101/65307], Loss: 0.0137\n",
            "Epoch [1/2], Step [34151/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [34201/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [34251/65307], Loss: 0.0692\n",
            "Epoch [1/2], Step [34301/65307], Loss: 0.0111\n",
            "Epoch [1/2], Step [34351/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [34401/65307], Loss: 0.0933\n",
            "Epoch [1/2], Step [34451/65307], Loss: 0.1665\n",
            "Epoch [1/2], Step [34501/65307], Loss: 0.2645\n",
            "Epoch [1/2], Step [34551/65307], Loss: 0.1169\n",
            "Epoch [1/2], Step [34601/65307], Loss: 0.6550\n",
            "Epoch [1/2], Step [34651/65307], Loss: 0.0501\n",
            "Epoch [1/2], Step [34701/65307], Loss: 0.0593\n",
            "Epoch [1/2], Step [34751/65307], Loss: 0.0958\n",
            "Epoch [1/2], Step [34801/65307], Loss: 0.3792\n",
            "Epoch [1/2], Step [34851/65307], Loss: 0.2735\n",
            "Epoch [1/2], Step [34901/65307], Loss: 0.0061\n",
            "Epoch [1/2], Step [34951/65307], Loss: 0.1172\n",
            "Epoch [1/2], Step [35001/65307], Loss: 0.0319\n",
            "Epoch [1/2], Step [35051/65307], Loss: 0.2177\n",
            "Epoch [1/2], Step [35101/65307], Loss: 0.1343\n",
            "Epoch [1/2], Step [35151/65307], Loss: 0.0639\n",
            "Epoch [1/2], Step [35201/65307], Loss: 0.0173\n",
            "Epoch [1/2], Step [35251/65307], Loss: 0.0774\n",
            "Epoch [1/2], Step [35301/65307], Loss: 0.0697\n",
            "Epoch [1/2], Step [35351/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [35401/65307], Loss: 0.1834\n",
            "Epoch [1/2], Step [35451/65307], Loss: 0.0208\n",
            "Epoch [1/2], Step [35501/65307], Loss: 0.1753\n",
            "Epoch [1/2], Step [35551/65307], Loss: 0.1628\n",
            "Epoch [1/2], Step [35601/65307], Loss: 0.0611\n",
            "Epoch [1/2], Step [35651/65307], Loss: 0.0478\n",
            "Epoch [1/2], Step [35701/65307], Loss: 0.0243\n",
            "Epoch [1/2], Step [35751/65307], Loss: 0.1577\n",
            "Epoch [1/2], Step [35801/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [35851/65307], Loss: 0.0265\n",
            "Epoch [1/2], Step [35901/65307], Loss: 0.0780\n",
            "Epoch [1/2], Step [35951/65307], Loss: 0.1077\n",
            "Epoch [1/2], Step [36001/65307], Loss: 0.0911\n",
            "Epoch [1/2], Step [36051/65307], Loss: 0.0289\n",
            "Epoch [1/2], Step [36101/65307], Loss: 0.0132\n",
            "Epoch [1/2], Step [36151/65307], Loss: 0.0423\n",
            "Epoch [1/2], Step [36201/65307], Loss: 0.1263\n",
            "Epoch [1/2], Step [36251/65307], Loss: 0.3260\n",
            "Epoch [1/2], Step [36301/65307], Loss: 0.1177\n",
            "Epoch [1/2], Step [36351/65307], Loss: 0.1149\n",
            "Epoch [1/2], Step [36401/65307], Loss: 0.1001\n",
            "Epoch [1/2], Step [36451/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [36501/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [36551/65307], Loss: 0.0477\n",
            "Epoch [1/2], Step [36601/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [36651/65307], Loss: 0.6082\n",
            "Epoch [1/2], Step [36701/65307], Loss: 0.1883\n",
            "Epoch [1/2], Step [36751/65307], Loss: 0.0513\n",
            "Epoch [1/2], Step [36801/65307], Loss: 0.1157\n",
            "Epoch [1/2], Step [36851/65307], Loss: 0.0199\n",
            "Epoch [1/2], Step [36901/65307], Loss: 0.1819\n",
            "Epoch [1/2], Step [36951/65307], Loss: 0.1301\n",
            "Epoch [1/2], Step [37001/65307], Loss: 0.0658\n",
            "Epoch [1/2], Step [37051/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [37101/65307], Loss: 0.0270\n",
            "Epoch [1/2], Step [37151/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [37201/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [37251/65307], Loss: 0.2389\n",
            "Epoch [1/2], Step [37301/65307], Loss: 0.2295\n",
            "Epoch [1/2], Step [37351/65307], Loss: 0.1900\n",
            "Epoch [1/2], Step [37401/65307], Loss: 0.0021\n",
            "Epoch [1/2], Step [37451/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [37501/65307], Loss: 0.1907\n",
            "Epoch [1/2], Step [37551/65307], Loss: 0.0156\n",
            "Epoch [1/2], Step [37601/65307], Loss: 0.3683\n",
            "Epoch [1/2], Step [37651/65307], Loss: 0.0307\n",
            "Epoch [1/2], Step [37701/65307], Loss: 0.1803\n",
            "Epoch [1/2], Step [37751/65307], Loss: 0.1017\n",
            "Epoch [1/2], Step [37801/65307], Loss: 0.0009\n",
            "Epoch [1/2], Step [37851/65307], Loss: 0.3338\n",
            "Epoch [1/2], Step [37901/65307], Loss: 0.0910\n",
            "Epoch [1/2], Step [37951/65307], Loss: 0.1154\n",
            "Epoch [1/2], Step [38001/65307], Loss: 0.0651\n",
            "Epoch [1/2], Step [38051/65307], Loss: 0.0518\n",
            "Epoch [1/2], Step [38101/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [38151/65307], Loss: 0.1209\n",
            "Epoch [1/2], Step [38201/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [38251/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [38301/65307], Loss: 0.0712\n",
            "Epoch [1/2], Step [38351/65307], Loss: 0.1919\n",
            "Epoch [1/2], Step [38401/65307], Loss: 0.0199\n",
            "Epoch [1/2], Step [38451/65307], Loss: 0.0189\n",
            "Epoch [1/2], Step [38501/65307], Loss: 0.0729\n",
            "Epoch [1/2], Step [38551/65307], Loss: 0.0087\n",
            "Epoch [1/2], Step [38601/65307], Loss: 0.1494\n",
            "Epoch [1/2], Step [38651/65307], Loss: 0.0522\n",
            "Epoch [1/2], Step [38701/65307], Loss: 0.0647\n",
            "Epoch [1/2], Step [38751/65307], Loss: 0.0650\n",
            "Epoch [1/2], Step [38801/65307], Loss: 0.1517\n",
            "Epoch [1/2], Step [38851/65307], Loss: 0.0648\n",
            "Epoch [1/2], Step [38901/65307], Loss: 0.1522\n",
            "Epoch [1/2], Step [38951/65307], Loss: 0.0305\n",
            "Epoch [1/2], Step [39001/65307], Loss: 0.1081\n",
            "Epoch [1/2], Step [39051/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [39101/65307], Loss: 0.0661\n",
            "Epoch [1/2], Step [39151/65307], Loss: 0.0905\n",
            "Epoch [1/2], Step [39201/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [39251/65307], Loss: 0.0848\n",
            "Epoch [1/2], Step [39301/65307], Loss: 0.0666\n",
            "Epoch [1/2], Step [39351/65307], Loss: 0.0257\n",
            "Epoch [1/2], Step [39401/65307], Loss: 0.0204\n",
            "Epoch [1/2], Step [39451/65307], Loss: 0.0110\n",
            "Epoch [1/2], Step [39501/65307], Loss: 0.2130\n",
            "Epoch [1/2], Step [39551/65307], Loss: 0.1123\n",
            "Epoch [1/2], Step [39601/65307], Loss: 0.0273\n",
            "Epoch [1/2], Step [39651/65307], Loss: 0.2615\n",
            "Epoch [1/2], Step [39701/65307], Loss: 0.0735\n",
            "Epoch [1/2], Step [39751/65307], Loss: 0.2336\n",
            "Epoch [1/2], Step [39801/65307], Loss: 0.0821\n",
            "Epoch [1/2], Step [39851/65307], Loss: 0.0468\n",
            "Epoch [1/2], Step [39901/65307], Loss: 0.2913\n",
            "Epoch [1/2], Step [39951/65307], Loss: 0.2797\n",
            "Epoch [1/2], Step [40001/65307], Loss: 0.2276\n",
            "Epoch [1/2], Step [40051/65307], Loss: 0.0232\n",
            "Epoch [1/2], Step [40101/65307], Loss: 0.0093\n",
            "Epoch [1/2], Step [40151/65307], Loss: 0.0493\n",
            "Epoch [1/2], Step [40201/65307], Loss: 0.0534\n",
            "Epoch [1/2], Step [40251/65307], Loss: 0.0820\n",
            "Epoch [1/2], Step [40301/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [40351/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [40401/65307], Loss: 0.0174\n",
            "Epoch [1/2], Step [40451/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [40501/65307], Loss: 0.1045\n",
            "Epoch [1/2], Step [40551/65307], Loss: 0.0051\n",
            "Epoch [1/2], Step [40601/65307], Loss: 0.1339\n",
            "Epoch [1/2], Step [40651/65307], Loss: 0.2458\n",
            "Epoch [1/2], Step [40701/65307], Loss: 0.0158\n",
            "Epoch [1/2], Step [40751/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [40801/65307], Loss: 0.1074\n",
            "Epoch [1/2], Step [40851/65307], Loss: 0.0513\n",
            "Epoch [1/2], Step [40901/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [40951/65307], Loss: 0.1497\n",
            "Epoch [1/2], Step [41001/65307], Loss: 0.0140\n",
            "Epoch [1/2], Step [41051/65307], Loss: 0.0638\n",
            "Epoch [1/2], Step [41101/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [41151/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [41201/65307], Loss: 0.0939\n",
            "Epoch [1/2], Step [41251/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [41301/65307], Loss: 0.0373\n",
            "Epoch [1/2], Step [41351/65307], Loss: 0.0817\n",
            "Epoch [1/2], Step [41401/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [41451/65307], Loss: 0.1814\n",
            "Epoch [1/2], Step [41501/65307], Loss: 0.0983\n",
            "Epoch [1/2], Step [41551/65307], Loss: 0.0352\n",
            "Epoch [1/2], Step [41601/65307], Loss: 0.0841\n",
            "Epoch [1/2], Step [41651/65307], Loss: 0.1380\n",
            "Epoch [1/2], Step [41701/65307], Loss: 0.0610\n",
            "Epoch [1/2], Step [41751/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [41801/65307], Loss: 0.2055\n",
            "Epoch [1/2], Step [41851/65307], Loss: 0.0753\n",
            "Epoch [1/2], Step [41901/65307], Loss: 0.1733\n",
            "Epoch [1/2], Step [41951/65307], Loss: 0.3131\n",
            "Epoch [1/2], Step [42001/65307], Loss: 0.1087\n",
            "Epoch [1/2], Step [42051/65307], Loss: 0.2709\n",
            "Epoch [1/2], Step [42101/65307], Loss: 0.0434\n",
            "Epoch [1/2], Step [42151/65307], Loss: 0.0699\n",
            "Epoch [1/2], Step [42201/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [42251/65307], Loss: 0.0409\n",
            "Epoch [1/2], Step [42301/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [42351/65307], Loss: 0.0178\n",
            "Epoch [1/2], Step [42401/65307], Loss: 0.0384\n",
            "Epoch [1/2], Step [42451/65307], Loss: 0.0238\n",
            "Epoch [1/2], Step [42501/65307], Loss: 0.1424\n",
            "Epoch [1/2], Step [42551/65307], Loss: 0.1117\n",
            "Epoch [1/2], Step [42601/65307], Loss: 0.0183\n",
            "Epoch [1/2], Step [42651/65307], Loss: 0.0564\n",
            "Epoch [1/2], Step [42701/65307], Loss: 0.0228\n",
            "Epoch [1/2], Step [42751/65307], Loss: 0.0974\n",
            "Epoch [1/2], Step [42801/65307], Loss: 0.0335\n",
            "Epoch [1/2], Step [42851/65307], Loss: 0.0803\n",
            "Epoch [1/2], Step [42901/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [42951/65307], Loss: 0.3228\n",
            "Epoch [1/2], Step [43001/65307], Loss: 0.1016\n",
            "Epoch [1/2], Step [43051/65307], Loss: 0.0975\n",
            "Epoch [1/2], Step [43101/65307], Loss: 0.1759\n",
            "Epoch [1/2], Step [43151/65307], Loss: 0.0745\n",
            "Epoch [1/2], Step [43201/65307], Loss: 0.0303\n",
            "Epoch [1/2], Step [43251/65307], Loss: 0.0459\n",
            "Epoch [1/2], Step [43301/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [43351/65307], Loss: 0.0649\n",
            "Epoch [1/2], Step [43401/65307], Loss: 0.0700\n",
            "Epoch [1/2], Step [43451/65307], Loss: 0.2015\n",
            "Epoch [1/2], Step [43501/65307], Loss: 0.0951\n",
            "Epoch [1/2], Step [43551/65307], Loss: 0.0298\n",
            "Epoch [1/2], Step [43601/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [43651/65307], Loss: 0.0067\n",
            "Epoch [1/2], Step [43701/65307], Loss: 0.0542\n",
            "Epoch [1/2], Step [43751/65307], Loss: 0.1461\n",
            "Epoch [1/2], Step [43801/65307], Loss: 0.2042\n",
            "Epoch [1/2], Step [43851/65307], Loss: 0.1613\n",
            "Epoch [1/2], Step [43901/65307], Loss: 0.1932\n",
            "Epoch [1/2], Step [43951/65307], Loss: 0.4691\n",
            "Epoch [1/2], Step [44001/65307], Loss: 0.0104\n",
            "Epoch [1/2], Step [44051/65307], Loss: 0.1663\n",
            "Epoch [1/2], Step [44101/65307], Loss: 0.2917\n",
            "Epoch [1/2], Step [44151/65307], Loss: 0.0920\n",
            "Epoch [1/2], Step [44201/65307], Loss: 0.0392\n",
            "Epoch [1/2], Step [44251/65307], Loss: 0.1448\n",
            "Epoch [1/2], Step [44301/65307], Loss: 0.0821\n",
            "Epoch [1/2], Step [44351/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [44401/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [44451/65307], Loss: 0.3250\n",
            "Epoch [1/2], Step [44501/65307], Loss: 0.0579\n",
            "Epoch [1/2], Step [44551/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [44601/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [44651/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [44701/65307], Loss: 0.0949\n",
            "Epoch [1/2], Step [44751/65307], Loss: 0.1171\n",
            "Epoch [1/2], Step [44801/65307], Loss: 0.3239\n",
            "Epoch [1/2], Step [44851/65307], Loss: 0.0831\n",
            "Epoch [1/2], Step [44901/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [44951/65307], Loss: 0.0622\n",
            "Epoch [1/2], Step [45001/65307], Loss: 0.2390\n",
            "Epoch [1/2], Step [45051/65307], Loss: 0.3536\n",
            "Epoch [1/2], Step [45101/65307], Loss: 0.0116\n",
            "Epoch [1/2], Step [45151/65307], Loss: 0.0268\n",
            "Epoch [1/2], Step [45201/65307], Loss: 0.1000\n",
            "Epoch [1/2], Step [45251/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [45301/65307], Loss: 0.1532\n",
            "Epoch [1/2], Step [45351/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [45401/65307], Loss: 0.0506\n",
            "Epoch [1/2], Step [45451/65307], Loss: 0.0379\n",
            "Epoch [1/2], Step [45501/65307], Loss: 0.1068\n",
            "Epoch [1/2], Step [45551/65307], Loss: 0.0439\n",
            "Epoch [1/2], Step [45601/65307], Loss: 0.2724\n",
            "Epoch [1/2], Step [45651/65307], Loss: 0.1056\n",
            "Epoch [1/2], Step [45701/65307], Loss: 0.2434\n",
            "Epoch [1/2], Step [45751/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [45801/65307], Loss: 0.0377\n",
            "Epoch [1/2], Step [45851/65307], Loss: 0.0058\n",
            "Epoch [1/2], Step [45901/65307], Loss: 0.0393\n",
            "Epoch [1/2], Step [45951/65307], Loss: 0.0767\n",
            "Epoch [1/2], Step [46001/65307], Loss: 0.0348\n",
            "Epoch [1/2], Step [46051/65307], Loss: 0.4867\n",
            "Epoch [1/2], Step [46101/65307], Loss: 0.1412\n",
            "Epoch [1/2], Step [46151/65307], Loss: 0.0526\n",
            "Epoch [1/2], Step [46201/65307], Loss: 0.0623\n",
            "Epoch [1/2], Step [46251/65307], Loss: 0.4883\n",
            "Epoch [1/2], Step [46301/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [46351/65307], Loss: 0.0248\n",
            "Epoch [1/2], Step [46401/65307], Loss: 0.5721\n",
            "Epoch [1/2], Step [46451/65307], Loss: 0.0181\n",
            "Epoch [1/2], Step [46501/65307], Loss: 0.2161\n",
            "Epoch [1/2], Step [46551/65307], Loss: 0.2673\n",
            "Epoch [1/2], Step [46601/65307], Loss: 0.0783\n",
            "Epoch [1/2], Step [46651/65307], Loss: 0.0988\n",
            "Epoch [1/2], Step [46701/65307], Loss: 0.1456\n",
            "Epoch [1/2], Step [46751/65307], Loss: 0.6606\n",
            "Epoch [1/2], Step [46801/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [46851/65307], Loss: 0.0787\n",
            "Epoch [1/2], Step [46901/65307], Loss: 0.0574\n",
            "Epoch [1/2], Step [46951/65307], Loss: 0.0973\n",
            "Epoch [1/2], Step [47001/65307], Loss: 0.1360\n",
            "Epoch [1/2], Step [47051/65307], Loss: 0.2656\n",
            "Epoch [1/2], Step [47101/65307], Loss: 0.0239\n",
            "Epoch [1/2], Step [47151/65307], Loss: 0.1734\n",
            "Epoch [1/2], Step [47201/65307], Loss: 0.0043\n",
            "Epoch [1/2], Step [47251/65307], Loss: 0.0153\n",
            "Epoch [1/2], Step [47301/65307], Loss: 0.4070\n",
            "Epoch [1/2], Step [47351/65307], Loss: 0.0191\n",
            "Epoch [1/2], Step [47401/65307], Loss: 0.0750\n",
            "Epoch [1/2], Step [47451/65307], Loss: 0.1455\n",
            "Epoch [1/2], Step [47501/65307], Loss: 0.2053\n",
            "Epoch [1/2], Step [47551/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [47601/65307], Loss: 0.0705\n",
            "Epoch [1/2], Step [47651/65307], Loss: 0.1784\n",
            "Epoch [1/2], Step [47701/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [47751/65307], Loss: 0.1772\n",
            "Epoch [1/2], Step [47801/65307], Loss: 0.0025\n",
            "Epoch [1/2], Step [47851/65307], Loss: 0.0657\n",
            "Epoch [1/2], Step [47901/65307], Loss: 0.0099\n",
            "Epoch [1/2], Step [47951/65307], Loss: 0.0059\n",
            "Epoch [1/2], Step [48001/65307], Loss: 0.1327\n",
            "Epoch [1/2], Step [48051/65307], Loss: 0.2552\n",
            "Epoch [1/2], Step [48101/65307], Loss: 0.1373\n",
            "Epoch [1/2], Step [48151/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [48201/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [48251/65307], Loss: 0.0233\n",
            "Epoch [1/2], Step [48301/65307], Loss: 0.0170\n",
            "Epoch [1/2], Step [48351/65307], Loss: 0.1493\n",
            "Epoch [1/2], Step [48401/65307], Loss: 0.4541\n",
            "Epoch [1/2], Step [48451/65307], Loss: 0.0785\n",
            "Epoch [1/2], Step [48501/65307], Loss: 0.1894\n",
            "Epoch [1/2], Step [48551/65307], Loss: 0.0631\n",
            "Epoch [1/2], Step [48601/65307], Loss: 0.0500\n",
            "Epoch [1/2], Step [48651/65307], Loss: 0.1693\n",
            "Epoch [1/2], Step [48701/65307], Loss: 0.0724\n",
            "Epoch [1/2], Step [48751/65307], Loss: 0.0134\n",
            "Epoch [1/2], Step [48801/65307], Loss: 0.0351\n",
            "Epoch [1/2], Step [48851/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [48901/65307], Loss: 0.0387\n",
            "Epoch [1/2], Step [48951/65307], Loss: 0.0212\n",
            "Epoch [1/2], Step [49001/65307], Loss: 0.0495\n",
            "Epoch [1/2], Step [49051/65307], Loss: 0.1146\n",
            "Epoch [1/2], Step [49101/65307], Loss: 0.0563\n",
            "Epoch [1/2], Step [49151/65307], Loss: 0.2681\n",
            "Epoch [1/2], Step [49201/65307], Loss: 0.0363\n",
            "Epoch [1/2], Step [49251/65307], Loss: 0.1086\n",
            "Epoch [1/2], Step [49301/65307], Loss: 0.1559\n",
            "Epoch [1/2], Step [49351/65307], Loss: 0.1137\n",
            "Epoch [1/2], Step [49401/65307], Loss: 0.2568\n",
            "Epoch [1/2], Step [49451/65307], Loss: 0.1089\n",
            "Epoch [1/2], Step [49501/65307], Loss: 0.1171\n",
            "Epoch [1/2], Step [49551/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [49601/65307], Loss: 0.1411\n",
            "Epoch [1/2], Step [49651/65307], Loss: 0.0677\n",
            "Epoch [1/2], Step [49701/65307], Loss: 0.0328\n",
            "Epoch [1/2], Step [49751/65307], Loss: 0.0519\n",
            "Epoch [1/2], Step [49801/65307], Loss: 0.1337\n",
            "Epoch [1/2], Step [49851/65307], Loss: 0.0744\n",
            "Epoch [1/2], Step [49901/65307], Loss: 0.0042\n",
            "Epoch [1/2], Step [49951/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [50001/65307], Loss: 0.0734\n",
            "Epoch [1/2], Step [50051/65307], Loss: 0.0310\n",
            "Epoch [1/2], Step [50101/65307], Loss: 0.0146\n",
            "Epoch [1/2], Step [50151/65307], Loss: 0.3077\n",
            "Epoch [1/2], Step [50201/65307], Loss: 0.0585\n",
            "Epoch [1/2], Step [50251/65307], Loss: 0.1695\n",
            "Epoch [1/2], Step [50301/65307], Loss: 0.0776\n",
            "Epoch [1/2], Step [50351/65307], Loss: 0.1190\n",
            "Epoch [1/2], Step [50401/65307], Loss: 0.1299\n",
            "Epoch [1/2], Step [50451/65307], Loss: 0.0934\n",
            "Epoch [1/2], Step [50501/65307], Loss: 0.0961\n",
            "Epoch [1/2], Step [50551/65307], Loss: 0.1630\n",
            "Epoch [1/2], Step [50601/65307], Loss: 0.0815\n",
            "Epoch [1/2], Step [50651/65307], Loss: 0.4115\n",
            "Epoch [1/2], Step [50701/65307], Loss: 0.0690\n",
            "Epoch [1/2], Step [50751/65307], Loss: 0.0214\n",
            "Epoch [1/2], Step [50801/65307], Loss: 0.0218\n",
            "Epoch [1/2], Step [50851/65307], Loss: 0.0026\n",
            "Epoch [1/2], Step [50901/65307], Loss: 0.1719\n",
            "Epoch [1/2], Step [50951/65307], Loss: 0.0596\n",
            "Epoch [1/2], Step [51001/65307], Loss: 0.0934\n",
            "Epoch [1/2], Step [51051/65307], Loss: 0.0787\n",
            "Epoch [1/2], Step [51101/65307], Loss: 0.1560\n",
            "Epoch [1/2], Step [51151/65307], Loss: 0.0274\n",
            "Epoch [1/2], Step [51201/65307], Loss: 0.3629\n",
            "Epoch [1/2], Step [51251/65307], Loss: 0.0118\n",
            "Epoch [1/2], Step [51301/65307], Loss: 0.1963\n",
            "Epoch [1/2], Step [51351/65307], Loss: 0.1266\n",
            "Epoch [1/2], Step [51401/65307], Loss: 0.2185\n",
            "Epoch [1/2], Step [51451/65307], Loss: 0.0201\n",
            "Epoch [1/2], Step [51501/65307], Loss: 0.0573\n",
            "Epoch [1/2], Step [51551/65307], Loss: 0.0034\n",
            "Epoch [1/2], Step [51601/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [51651/65307], Loss: 0.0606\n",
            "Epoch [1/2], Step [51701/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [51751/65307], Loss: 0.0565\n",
            "Epoch [1/2], Step [51801/65307], Loss: 0.0255\n",
            "Epoch [1/2], Step [51851/65307], Loss: 0.2476\n",
            "Epoch [1/2], Step [51901/65307], Loss: 0.0409\n",
            "Epoch [1/2], Step [51951/65307], Loss: 0.0061\n",
            "Epoch [1/2], Step [52001/65307], Loss: 0.1112\n",
            "Epoch [1/2], Step [52051/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [52101/65307], Loss: 0.1430\n",
            "Epoch [1/2], Step [52151/65307], Loss: 0.1691\n",
            "Epoch [1/2], Step [52201/65307], Loss: 0.0679\n",
            "Epoch [1/2], Step [52251/65307], Loss: 0.4666\n",
            "Epoch [1/2], Step [52301/65307], Loss: 0.1023\n",
            "Epoch [1/2], Step [52351/65307], Loss: 0.1540\n",
            "Epoch [1/2], Step [52401/65307], Loss: 0.0695\n",
            "Epoch [1/2], Step [52451/65307], Loss: 0.2607\n",
            "Epoch [1/2], Step [52501/65307], Loss: 0.0715\n",
            "Epoch [1/2], Step [52551/65307], Loss: 0.0147\n",
            "Epoch [1/2], Step [52601/65307], Loss: 0.2109\n",
            "Epoch [1/2], Step [52651/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [52701/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [52751/65307], Loss: 0.1210\n",
            "Epoch [1/2], Step [52801/65307], Loss: 0.0300\n",
            "Epoch [1/2], Step [52851/65307], Loss: 0.0269\n",
            "Epoch [1/2], Step [52901/65307], Loss: 0.0348\n",
            "Epoch [1/2], Step [52951/65307], Loss: 0.0697\n",
            "Epoch [1/2], Step [53001/65307], Loss: 0.1125\n",
            "Epoch [1/2], Step [53051/65307], Loss: 0.0836\n",
            "Epoch [1/2], Step [53101/65307], Loss: 0.2255\n",
            "Epoch [1/2], Step [53151/65307], Loss: 0.0877\n",
            "Epoch [1/2], Step [53201/65307], Loss: 0.1019\n",
            "Epoch [1/2], Step [53251/65307], Loss: 0.0619\n",
            "Epoch [1/2], Step [53301/65307], Loss: 0.0322\n",
            "Epoch [1/2], Step [53351/65307], Loss: 0.1752\n",
            "Epoch [1/2], Step [53401/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [53451/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [53501/65307], Loss: 0.2029\n",
            "Epoch [1/2], Step [53551/65307], Loss: 0.0321\n",
            "Epoch [1/2], Step [53601/65307], Loss: 0.0479\n",
            "Epoch [1/2], Step [53651/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [53701/65307], Loss: 0.0589\n",
            "Epoch [1/2], Step [53751/65307], Loss: 0.0033\n",
            "Epoch [1/2], Step [53801/65307], Loss: 0.0365\n",
            "Epoch [1/2], Step [53851/65307], Loss: 0.1604\n",
            "Epoch [1/2], Step [53901/65307], Loss: 0.0295\n",
            "Epoch [1/2], Step [53951/65307], Loss: 0.0456\n",
            "Epoch [1/2], Step [54001/65307], Loss: 0.0208\n",
            "Epoch [1/2], Step [54051/65307], Loss: 0.1910\n",
            "Epoch [1/2], Step [54101/65307], Loss: 0.0146\n",
            "Epoch [1/2], Step [54151/65307], Loss: 0.1049\n",
            "Epoch [1/2], Step [54201/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [54251/65307], Loss: 0.0241\n",
            "Epoch [1/2], Step [54301/65307], Loss: 0.1727\n",
            "Epoch [1/2], Step [54351/65307], Loss: 0.0043\n",
            "Epoch [1/2], Step [54401/65307], Loss: 0.0451\n",
            "Epoch [1/2], Step [54451/65307], Loss: 0.1514\n",
            "Epoch [1/2], Step [54501/65307], Loss: 0.0262\n",
            "Epoch [1/2], Step [54551/65307], Loss: 0.0030\n",
            "Epoch [1/2], Step [54601/65307], Loss: 0.3719\n",
            "Epoch [1/2], Step [54651/65307], Loss: 0.1186\n",
            "Epoch [1/2], Step [54701/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [54751/65307], Loss: 0.3882\n",
            "Epoch [1/2], Step [54801/65307], Loss: 0.0326\n",
            "Epoch [1/2], Step [54851/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [54901/65307], Loss: 0.0709\n",
            "Epoch [1/2], Step [54951/65307], Loss: 0.0204\n",
            "Epoch [1/2], Step [55001/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [55051/65307], Loss: 0.2503\n",
            "Epoch [1/2], Step [55101/65307], Loss: 0.0371\n",
            "Epoch [1/2], Step [55151/65307], Loss: 0.0503\n",
            "Epoch [1/2], Step [55201/65307], Loss: 0.0016\n",
            "Epoch [1/2], Step [55251/65307], Loss: 0.2772\n",
            "Epoch [1/2], Step [55301/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [55351/65307], Loss: 0.0313\n",
            "Epoch [1/2], Step [55401/65307], Loss: 0.0373\n",
            "Epoch [1/2], Step [55451/65307], Loss: 0.1217\n",
            "Epoch [1/2], Step [55501/65307], Loss: 0.0505\n",
            "Epoch [1/2], Step [55551/65307], Loss: 0.0779\n",
            "Epoch [1/2], Step [55601/65307], Loss: 0.4122\n",
            "Epoch [1/2], Step [55651/65307], Loss: 0.0969\n",
            "Epoch [1/2], Step [55701/65307], Loss: 0.0249\n",
            "Epoch [1/2], Step [55751/65307], Loss: 0.0052\n",
            "Epoch [1/2], Step [55801/65307], Loss: 0.1545\n",
            "Epoch [1/2], Step [55851/65307], Loss: 0.1711\n",
            "Epoch [1/2], Step [55901/65307], Loss: 0.0567\n",
            "Epoch [1/2], Step [55951/65307], Loss: 0.0844\n",
            "Epoch [1/2], Step [56001/65307], Loss: 0.0295\n",
            "Epoch [1/2], Step [56051/65307], Loss: 0.0353\n",
            "Epoch [1/2], Step [56101/65307], Loss: 0.0899\n",
            "Epoch [1/2], Step [56151/65307], Loss: 0.1479\n",
            "Epoch [1/2], Step [56201/65307], Loss: 0.0297\n",
            "Epoch [1/2], Step [56251/65307], Loss: 0.2369\n",
            "Epoch [1/2], Step [56301/65307], Loss: 0.0135\n",
            "Epoch [1/2], Step [56351/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [56401/65307], Loss: 0.0026\n",
            "Epoch [1/2], Step [56451/65307], Loss: 0.0151\n",
            "Epoch [1/2], Step [56501/65307], Loss: 0.0537\n",
            "Epoch [1/2], Step [56551/65307], Loss: 0.0725\n",
            "Epoch [1/2], Step [56601/65307], Loss: 0.0445\n",
            "Epoch [1/2], Step [56651/65307], Loss: 0.1785\n",
            "Epoch [1/2], Step [56701/65307], Loss: 0.1273\n",
            "Epoch [1/2], Step [56751/65307], Loss: 0.0465\n",
            "Epoch [1/2], Step [56801/65307], Loss: 0.0040\n",
            "Epoch [1/2], Step [56851/65307], Loss: 0.0266\n",
            "Epoch [1/2], Step [56901/65307], Loss: 0.0227\n",
            "Epoch [1/2], Step [56951/65307], Loss: 0.3006\n",
            "Epoch [1/2], Step [57001/65307], Loss: 0.2209\n",
            "Epoch [1/2], Step [57051/65307], Loss: 0.1704\n",
            "Epoch [1/2], Step [57101/65307], Loss: 0.0063\n",
            "Epoch [1/2], Step [57151/65307], Loss: 0.0760\n",
            "Epoch [1/2], Step [57201/65307], Loss: 0.1544\n",
            "Epoch [1/2], Step [57251/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [57301/65307], Loss: 0.0388\n",
            "Epoch [1/2], Step [57351/65307], Loss: 0.0286\n",
            "Epoch [1/2], Step [57401/65307], Loss: 0.2511\n",
            "Epoch [1/2], Step [57451/65307], Loss: 0.1214\n",
            "Epoch [1/2], Step [57501/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [57551/65307], Loss: 0.1573\n",
            "Epoch [1/2], Step [57601/65307], Loss: 0.0440\n",
            "Epoch [1/2], Step [57651/65307], Loss: 0.2066\n",
            "Epoch [1/2], Step [57701/65307], Loss: 0.1257\n",
            "Epoch [1/2], Step [57751/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [57801/65307], Loss: 0.0402\n",
            "Epoch [1/2], Step [57851/65307], Loss: 0.0111\n",
            "Epoch [1/2], Step [57901/65307], Loss: 0.3273\n",
            "Epoch [1/2], Step [57951/65307], Loss: 0.0329\n",
            "Epoch [1/2], Step [58001/65307], Loss: 0.0930\n",
            "Epoch [1/2], Step [58051/65307], Loss: 0.0538\n",
            "Epoch [1/2], Step [58101/65307], Loss: 0.0944\n",
            "Epoch [1/2], Step [58151/65307], Loss: 0.4440\n",
            "Epoch [1/2], Step [58201/65307], Loss: 0.1406\n",
            "Epoch [1/2], Step [58251/65307], Loss: 0.0695\n",
            "Epoch [1/2], Step [58301/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [58351/65307], Loss: 0.1516\n",
            "Epoch [1/2], Step [58401/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [58451/65307], Loss: 0.1621\n",
            "Epoch [1/2], Step [58501/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [58551/65307], Loss: 0.0038\n",
            "Epoch [1/2], Step [58601/65307], Loss: 0.1884\n",
            "Epoch [1/2], Step [58651/65307], Loss: 0.2023\n",
            "Epoch [1/2], Step [58701/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [58751/65307], Loss: 0.0475\n",
            "Epoch [1/2], Step [58801/65307], Loss: 0.1632\n",
            "Epoch [1/2], Step [58851/65307], Loss: 0.0244\n",
            "Epoch [1/2], Step [58901/65307], Loss: 0.0886\n",
            "Epoch [1/2], Step [58951/65307], Loss: 0.1873\n",
            "Epoch [1/2], Step [59001/65307], Loss: 0.2068\n",
            "Epoch [1/2], Step [59051/65307], Loss: 0.0018\n",
            "Epoch [1/2], Step [59101/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [59151/65307], Loss: 0.0709\n",
            "Epoch [1/2], Step [59201/65307], Loss: 0.0393\n",
            "Epoch [1/2], Step [59251/65307], Loss: 0.2321\n",
            "Epoch [1/2], Step [59301/65307], Loss: 0.1777\n",
            "Epoch [1/2], Step [59351/65307], Loss: 0.2487\n",
            "Epoch [1/2], Step [59401/65307], Loss: 0.0498\n",
            "Epoch [1/2], Step [59451/65307], Loss: 0.0674\n",
            "Epoch [1/2], Step [59501/65307], Loss: 0.2094\n",
            "Epoch [1/2], Step [59551/65307], Loss: 0.1210\n",
            "Epoch [1/2], Step [59601/65307], Loss: 0.0771\n",
            "Epoch [1/2], Step [59651/65307], Loss: 0.0278\n",
            "Epoch [1/2], Step [59701/65307], Loss: 0.0220\n",
            "Epoch [1/2], Step [59751/65307], Loss: 0.1411\n",
            "Epoch [1/2], Step [59801/65307], Loss: 0.0024\n",
            "Epoch [1/2], Step [59851/65307], Loss: 0.0294\n",
            "Epoch [1/2], Step [59901/65307], Loss: 0.0745\n",
            "Epoch [1/2], Step [59951/65307], Loss: 0.0730\n",
            "Epoch [1/2], Step [60001/65307], Loss: 0.1217\n",
            "Epoch [1/2], Step [60051/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [60101/65307], Loss: 0.0607\n",
            "Epoch [1/2], Step [60151/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [60201/65307], Loss: 0.0399\n",
            "Epoch [1/2], Step [60251/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [60301/65307], Loss: 0.1259\n",
            "Epoch [1/2], Step [60351/65307], Loss: 0.2423\n",
            "Epoch [1/2], Step [60401/65307], Loss: 0.0371\n",
            "Epoch [1/2], Step [60451/65307], Loss: 0.3240\n",
            "Epoch [1/2], Step [60501/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [60551/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [60601/65307], Loss: 0.0243\n",
            "Epoch [1/2], Step [60651/65307], Loss: 0.8457\n",
            "Epoch [1/2], Step [60701/65307], Loss: 0.3522\n",
            "Epoch [1/2], Step [60751/65307], Loss: 0.0230\n",
            "Epoch [1/2], Step [60801/65307], Loss: 0.0544\n",
            "Epoch [1/2], Step [60851/65307], Loss: 0.1606\n",
            "Epoch [1/2], Step [60901/65307], Loss: 0.0965\n",
            "Epoch [1/2], Step [60951/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [61001/65307], Loss: 0.0623\n",
            "Epoch [1/2], Step [61051/65307], Loss: 0.0035\n",
            "Epoch [1/2], Step [61101/65307], Loss: 0.1218\n",
            "Epoch [1/2], Step [61151/65307], Loss: 0.0151\n",
            "Epoch [1/2], Step [61201/65307], Loss: 0.0059\n",
            "Epoch [1/2], Step [61251/65307], Loss: 0.0553\n",
            "Epoch [1/2], Step [61301/65307], Loss: 0.0579\n",
            "Epoch [1/2], Step [61351/65307], Loss: 0.4781\n",
            "Epoch [1/2], Step [61401/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [61451/65307], Loss: 0.0174\n",
            "Epoch [1/2], Step [61501/65307], Loss: 0.0417\n",
            "Epoch [1/2], Step [61551/65307], Loss: 0.0041\n",
            "Epoch [1/2], Step [61601/65307], Loss: 0.2606\n",
            "Epoch [1/2], Step [61651/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [61701/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [61751/65307], Loss: 0.0096\n",
            "Epoch [1/2], Step [61801/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [61851/65307], Loss: 0.1125\n",
            "Epoch [1/2], Step [61901/65307], Loss: 0.4560\n",
            "Epoch [1/2], Step [61951/65307], Loss: 0.0950\n",
            "Epoch [1/2], Step [62001/65307], Loss: 0.0021\n",
            "Epoch [1/2], Step [62051/65307], Loss: 0.0141\n",
            "Epoch [1/2], Step [62101/65307], Loss: 0.0904\n",
            "Epoch [1/2], Step [62151/65307], Loss: 0.0546\n",
            "Epoch [1/2], Step [62201/65307], Loss: 0.0309\n",
            "Epoch [1/2], Step [62251/65307], Loss: 0.0189\n",
            "Epoch [1/2], Step [62301/65307], Loss: 0.0422\n",
            "Epoch [1/2], Step [62351/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [62401/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [62451/65307], Loss: 0.0366\n",
            "Epoch [1/2], Step [62501/65307], Loss: 0.5770\n",
            "Epoch [1/2], Step [62551/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [62601/65307], Loss: 0.1332\n",
            "Epoch [1/2], Step [62651/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [62701/65307], Loss: 0.0407\n",
            "Epoch [1/2], Step [62751/65307], Loss: 0.2956\n",
            "Epoch [1/2], Step [62801/65307], Loss: 0.0171\n",
            "Epoch [1/2], Step [62851/65307], Loss: 0.1624\n",
            "Epoch [1/2], Step [62901/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [62951/65307], Loss: 0.3939\n",
            "Epoch [1/2], Step [63001/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [63051/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [63101/65307], Loss: 0.0553\n",
            "Epoch [1/2], Step [63151/65307], Loss: 0.1223\n",
            "Epoch [1/2], Step [63201/65307], Loss: 0.0155\n",
            "Epoch [1/2], Step [63251/65307], Loss: 0.2605\n",
            "Epoch [1/2], Step [63301/65307], Loss: 0.3256\n",
            "Epoch [1/2], Step [63351/65307], Loss: 0.0038\n",
            "Epoch [1/2], Step [63401/65307], Loss: 0.0275\n",
            "Epoch [1/2], Step [63451/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [63501/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [63551/65307], Loss: 0.0051\n",
            "Epoch [1/2], Step [63601/65307], Loss: 0.2283\n",
            "Epoch [1/2], Step [63651/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [63701/65307], Loss: 0.0792\n",
            "Epoch [1/2], Step [63751/65307], Loss: 0.2192\n",
            "Epoch [1/2], Step [63801/65307], Loss: 0.3167\n",
            "Epoch [1/2], Step [63851/65307], Loss: 0.2534\n",
            "Epoch [1/2], Step [63901/65307], Loss: 0.0098\n",
            "Epoch [1/2], Step [63951/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [64001/65307], Loss: 0.1512\n",
            "Epoch [1/2], Step [64051/65307], Loss: 0.0742\n",
            "Epoch [1/2], Step [64101/65307], Loss: 0.0840\n",
            "Epoch [1/2], Step [64151/65307], Loss: 0.0064\n",
            "Epoch [1/2], Step [64201/65307], Loss: 0.0290\n",
            "Epoch [1/2], Step [64251/65307], Loss: 0.1230\n",
            "Epoch [1/2], Step [64301/65307], Loss: 0.0163\n",
            "Epoch [1/2], Step [64351/65307], Loss: 0.2260\n",
            "Epoch [1/2], Step [64401/65307], Loss: 0.0417\n",
            "Epoch [1/2], Step [64451/65307], Loss: 0.2053\n",
            "Epoch [1/2], Step [64501/65307], Loss: 0.1926\n",
            "Epoch [1/2], Step [64551/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [64601/65307], Loss: 0.1384\n",
            "Epoch [1/2], Step [64651/65307], Loss: 0.0816\n",
            "Epoch [1/2], Step [64701/65307], Loss: 0.0590\n",
            "Epoch [1/2], Step [64751/65307], Loss: 0.1042\n",
            "Epoch [1/2], Step [64801/65307], Loss: 0.0258\n",
            "Epoch [1/2], Step [64851/65307], Loss: 0.0271\n",
            "Epoch [1/2], Step [64901/65307], Loss: 0.1064\n",
            "Epoch [1/2], Step [64951/65307], Loss: 0.0588\n",
            "Epoch [1/2], Step [65001/65307], Loss: 0.0092\n",
            "Epoch [1/2], Step [65051/65307], Loss: 0.0900\n",
            "Epoch [1/2], Step [65101/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [65151/65307], Loss: 0.1000\n",
            "Epoch [1/2], Step [65201/65307], Loss: 0.0143\n",
            "Epoch [1/2], Step [65251/65307], Loss: 0.1091\n",
            "Epoch [1/2], Step [65301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [1/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [51/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [101/65307], Loss: 0.1922\n",
            "Epoch [2/2], Step [151/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [201/65307], Loss: 0.0339\n",
            "Epoch [2/2], Step [251/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [301/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [351/65307], Loss: 0.3302\n",
            "Epoch [2/2], Step [401/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [451/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [501/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [551/65307], Loss: 0.3201\n",
            "Epoch [2/2], Step [601/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [651/65307], Loss: 0.0659\n",
            "Epoch [2/2], Step [701/65307], Loss: 0.0328\n",
            "Epoch [2/2], Step [751/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [801/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [851/65307], Loss: 0.1465\n",
            "Epoch [2/2], Step [901/65307], Loss: 0.0929\n",
            "Epoch [2/2], Step [951/65307], Loss: 0.0388\n",
            "Epoch [2/2], Step [1001/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [1051/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [1101/65307], Loss: 0.0331\n",
            "Epoch [2/2], Step [1151/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [1201/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [1251/65307], Loss: 0.0314\n",
            "Epoch [2/2], Step [1301/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [1351/65307], Loss: 0.0263\n",
            "Epoch [2/2], Step [1401/65307], Loss: 0.0985\n",
            "Epoch [2/2], Step [1451/65307], Loss: 0.1664\n",
            "Epoch [2/2], Step [1501/65307], Loss: 0.0572\n",
            "Epoch [2/2], Step [1551/65307], Loss: 0.3268\n",
            "Epoch [2/2], Step [1601/65307], Loss: 0.0126\n",
            "Epoch [2/2], Step [1651/65307], Loss: 0.0563\n",
            "Epoch [2/2], Step [1701/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [1751/65307], Loss: 0.1266\n",
            "Epoch [2/2], Step [1801/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [1851/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [1901/65307], Loss: 0.0193\n",
            "Epoch [2/2], Step [1951/65307], Loss: 0.1068\n",
            "Epoch [2/2], Step [2001/65307], Loss: 0.2647\n",
            "Epoch [2/2], Step [2051/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [2101/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [2151/65307], Loss: 0.0482\n",
            "Epoch [2/2], Step [2201/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [2251/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [2301/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [2351/65307], Loss: 0.4913\n",
            "Epoch [2/2], Step [2401/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [2451/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [2501/65307], Loss: 0.3364\n",
            "Epoch [2/2], Step [2551/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [2601/65307], Loss: 0.1932\n",
            "Epoch [2/2], Step [2651/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [2701/65307], Loss: 0.0503\n",
            "Epoch [2/2], Step [2751/65307], Loss: 0.0874\n",
            "Epoch [2/2], Step [2801/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [2851/65307], Loss: 0.0890\n",
            "Epoch [2/2], Step [2901/65307], Loss: 0.0736\n",
            "Epoch [2/2], Step [2951/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [3001/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [3051/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [3101/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [3151/65307], Loss: 0.2131\n",
            "Epoch [2/2], Step [3201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [3251/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [3301/65307], Loss: 0.3098\n",
            "Epoch [2/2], Step [3351/65307], Loss: 0.0633\n",
            "Epoch [2/2], Step [3401/65307], Loss: 0.1278\n",
            "Epoch [2/2], Step [3451/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [3501/65307], Loss: 0.1219\n",
            "Epoch [2/2], Step [3551/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [3601/65307], Loss: 0.0269\n",
            "Epoch [2/2], Step [3651/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [3701/65307], Loss: 0.1225\n",
            "Epoch [2/2], Step [3751/65307], Loss: 0.2270\n",
            "Epoch [2/2], Step [3801/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [3851/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [3901/65307], Loss: 0.1383\n",
            "Epoch [2/2], Step [3951/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [4001/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [4051/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [4101/65307], Loss: 0.0131\n",
            "Epoch [2/2], Step [4151/65307], Loss: 0.0217\n",
            "Epoch [2/2], Step [4201/65307], Loss: 0.1036\n",
            "Epoch [2/2], Step [4251/65307], Loss: 0.0687\n",
            "Epoch [2/2], Step [4301/65307], Loss: 0.0873\n",
            "Epoch [2/2], Step [4351/65307], Loss: 0.0868\n",
            "Epoch [2/2], Step [4401/65307], Loss: 0.1172\n",
            "Epoch [2/2], Step [4451/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [4501/65307], Loss: 0.1841\n",
            "Epoch [2/2], Step [4551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [4601/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [4651/65307], Loss: 0.0413\n",
            "Epoch [2/2], Step [4701/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [4751/65307], Loss: 0.1318\n",
            "Epoch [2/2], Step [4801/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [4851/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [4901/65307], Loss: 0.0780\n",
            "Epoch [2/2], Step [4951/65307], Loss: 0.0171\n",
            "Epoch [2/2], Step [5001/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [5051/65307], Loss: 0.2719\n",
            "Epoch [2/2], Step [5101/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [5151/65307], Loss: 0.2240\n",
            "Epoch [2/2], Step [5201/65307], Loss: 0.0299\n",
            "Epoch [2/2], Step [5251/65307], Loss: 0.0622\n",
            "Epoch [2/2], Step [5301/65307], Loss: 0.2012\n",
            "Epoch [2/2], Step [5351/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [5401/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [5451/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [5501/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [5551/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [5601/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [5651/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [5701/65307], Loss: 0.0029\n",
            "Epoch [2/2], Step [5751/65307], Loss: 0.3523\n",
            "Epoch [2/2], Step [5801/65307], Loss: 0.0131\n",
            "Epoch [2/2], Step [5851/65307], Loss: 0.3471\n",
            "Epoch [2/2], Step [5901/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [5951/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [6001/65307], Loss: 0.0669\n",
            "Epoch [2/2], Step [6051/65307], Loss: 0.1431\n",
            "Epoch [2/2], Step [6101/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [6151/65307], Loss: 0.0211\n",
            "Epoch [2/2], Step [6201/65307], Loss: 0.2423\n",
            "Epoch [2/2], Step [6251/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [6301/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [6351/65307], Loss: 0.0047\n",
            "Epoch [2/2], Step [6401/65307], Loss: 0.0335\n",
            "Epoch [2/2], Step [6451/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [6501/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [6551/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [6601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [6651/65307], Loss: 0.1236\n",
            "Epoch [2/2], Step [6701/65307], Loss: 0.0359\n",
            "Epoch [2/2], Step [6751/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [6801/65307], Loss: 0.2053\n",
            "Epoch [2/2], Step [6851/65307], Loss: 0.0045\n",
            "Epoch [2/2], Step [6901/65307], Loss: 0.0375\n",
            "Epoch [2/2], Step [6951/65307], Loss: 0.3081\n",
            "Epoch [2/2], Step [7001/65307], Loss: 0.1964\n",
            "Epoch [2/2], Step [7051/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [7101/65307], Loss: 0.2859\n",
            "Epoch [2/2], Step [7151/65307], Loss: 0.1209\n",
            "Epoch [2/2], Step [7201/65307], Loss: 0.5270\n",
            "Epoch [2/2], Step [7251/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [7301/65307], Loss: 0.2146\n",
            "Epoch [2/2], Step [7351/65307], Loss: 0.0510\n",
            "Epoch [2/2], Step [7401/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [7451/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [7501/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [7551/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [7601/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [7651/65307], Loss: 0.1990\n",
            "Epoch [2/2], Step [7701/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [7751/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [7801/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [7851/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [7901/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [7951/65307], Loss: 0.0841\n",
            "Epoch [2/2], Step [8001/65307], Loss: 0.0592\n",
            "Epoch [2/2], Step [8051/65307], Loss: 0.0874\n",
            "Epoch [2/2], Step [8101/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [8151/65307], Loss: 0.0338\n",
            "Epoch [2/2], Step [8201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [8251/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [8301/65307], Loss: 0.0527\n",
            "Epoch [2/2], Step [8351/65307], Loss: 0.3808\n",
            "Epoch [2/2], Step [8401/65307], Loss: 0.0944\n",
            "Epoch [2/2], Step [8451/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [8501/65307], Loss: 0.0703\n",
            "Epoch [2/2], Step [8551/65307], Loss: 0.2920\n",
            "Epoch [2/2], Step [8601/65307], Loss: 0.0576\n",
            "Epoch [2/2], Step [8651/65307], Loss: 0.0087\n",
            "Epoch [2/2], Step [8701/65307], Loss: 0.2364\n",
            "Epoch [2/2], Step [8751/65307], Loss: 0.1117\n",
            "Epoch [2/2], Step [8801/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [8851/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [8901/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [8951/65307], Loss: 0.0888\n",
            "Epoch [2/2], Step [9001/65307], Loss: 0.1832\n",
            "Epoch [2/2], Step [9051/65307], Loss: 0.0723\n",
            "Epoch [2/2], Step [9101/65307], Loss: 0.0648\n",
            "Epoch [2/2], Step [9151/65307], Loss: 0.1219\n",
            "Epoch [2/2], Step [9201/65307], Loss: 0.0331\n",
            "Epoch [2/2], Step [9251/65307], Loss: 0.1834\n",
            "Epoch [2/2], Step [9301/65307], Loss: 0.0507\n",
            "Epoch [2/2], Step [9351/65307], Loss: 0.0006\n",
            "Epoch [2/2], Step [9401/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [9451/65307], Loss: 0.0521\n",
            "Epoch [2/2], Step [9501/65307], Loss: 0.0850\n",
            "Epoch [2/2], Step [9551/65307], Loss: 0.1717\n",
            "Epoch [2/2], Step [9601/65307], Loss: 0.2206\n",
            "Epoch [2/2], Step [9651/65307], Loss: 0.0704\n",
            "Epoch [2/2], Step [9701/65307], Loss: 0.2082\n",
            "Epoch [2/2], Step [9751/65307], Loss: 0.0729\n",
            "Epoch [2/2], Step [9801/65307], Loss: 0.0760\n",
            "Epoch [2/2], Step [9851/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [9901/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [9951/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [10001/65307], Loss: 0.0488\n",
            "Epoch [2/2], Step [10051/65307], Loss: 0.1980\n",
            "Epoch [2/2], Step [10101/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [10151/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [10201/65307], Loss: 0.1615\n",
            "Epoch [2/2], Step [10251/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [10301/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [10351/65307], Loss: 0.1998\n",
            "Epoch [2/2], Step [10401/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [10451/65307], Loss: 0.1279\n",
            "Epoch [2/2], Step [10501/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [10551/65307], Loss: 0.1348\n",
            "Epoch [2/2], Step [10601/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [10651/65307], Loss: 0.1563\n",
            "Epoch [2/2], Step [10701/65307], Loss: 0.0681\n",
            "Epoch [2/2], Step [10751/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [10801/65307], Loss: 0.0051\n",
            "Epoch [2/2], Step [10851/65307], Loss: 0.2470\n",
            "Epoch [2/2], Step [10901/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [10951/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [11001/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [11051/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [11101/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [11151/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [11201/65307], Loss: 0.1016\n",
            "Epoch [2/2], Step [11251/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [11301/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [11351/65307], Loss: 0.0439\n",
            "Epoch [2/2], Step [11401/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [11451/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [11501/65307], Loss: 0.0521\n",
            "Epoch [2/2], Step [11551/65307], Loss: 0.0233\n",
            "Epoch [2/2], Step [11601/65307], Loss: 0.0945\n",
            "Epoch [2/2], Step [11651/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [11701/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [11751/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [11801/65307], Loss: 0.0979\n",
            "Epoch [2/2], Step [11851/65307], Loss: 0.0247\n",
            "Epoch [2/2], Step [11901/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [11951/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [12001/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [12051/65307], Loss: 0.1084\n",
            "Epoch [2/2], Step [12101/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [12151/65307], Loss: 0.1305\n",
            "Epoch [2/2], Step [12201/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [12251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [12301/65307], Loss: 0.1349\n",
            "Epoch [2/2], Step [12301/65307], Loss: 0.1349\n",
            "Epoch [2/2], Step [12351/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [12351/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [12401/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [12401/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [12451/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [12451/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [12501/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [12501/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [12551/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [12551/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [12601/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12601/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12651/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [12651/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [12701/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [12701/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [12751/65307], Loss: 0.0865\n",
            "Epoch [2/2], Step [12751/65307], Loss: 0.0865\n",
            "Epoch [2/2], Step [12801/65307], Loss: 0.0903\n",
            "Epoch [2/2], Step [12801/65307], Loss: 0.0903\n",
            "Epoch [2/2], Step [12851/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12851/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [12901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [12951/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [12951/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [13001/65307], Loss: 0.0647\n",
            "Epoch [2/2], Step [13001/65307], Loss: 0.0647\n",
            "Epoch [2/2], Step [13051/65307], Loss: 0.0422\n",
            "Epoch [2/2], Step [13051/65307], Loss: 0.0422\n",
            "Epoch [2/2], Step [13101/65307], Loss: 0.0802\n",
            "Epoch [2/2], Step [13101/65307], Loss: 0.0802\n",
            "Epoch [2/2], Step [13151/65307], Loss: 0.2784\n",
            "Epoch [2/2], Step [13151/65307], Loss: 0.2784\n",
            "Epoch [2/2], Step [13201/65307], Loss: 0.0237\n",
            "Epoch [2/2], Step [13201/65307], Loss: 0.0237\n",
            "Epoch [2/2], Step [13251/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [13251/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [13301/65307], Loss: 0.1424\n",
            "Epoch [2/2], Step [13301/65307], Loss: 0.1424\n",
            "Epoch [2/2], Step [13351/65307], Loss: 0.0205\n",
            "Epoch [2/2], Step [13351/65307], Loss: 0.0205\n",
            "Epoch [2/2], Step [13401/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [13401/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [13451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [13451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [13501/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [13501/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [13551/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [13551/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [13601/65307], Loss: 0.1550\n",
            "Epoch [2/2], Step [13601/65307], Loss: 0.1550\n",
            "Epoch [2/2], Step [13651/65307], Loss: 0.0229\n",
            "Epoch [2/2], Step [13651/65307], Loss: 0.0229\n",
            "Epoch [2/2], Step [13701/65307], Loss: 0.2159\n",
            "Epoch [2/2], Step [13701/65307], Loss: 0.2159\n",
            "Epoch [2/2], Step [13751/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [13751/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [13801/65307], Loss: 0.1727\n",
            "Epoch [2/2], Step [13801/65307], Loss: 0.1727\n",
            "Epoch [2/2], Step [13851/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [13851/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [13901/65307], Loss: 0.0096\n",
            "Epoch [2/2], Step [13901/65307], Loss: 0.0096\n",
            "Epoch [2/2], Step [13951/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [13951/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [14001/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [14001/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [14051/65307], Loss: 0.1444\n",
            "Epoch [2/2], Step [14051/65307], Loss: 0.1444\n",
            "Epoch [2/2], Step [14101/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [14101/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [14151/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [14151/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [14201/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [14201/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [14251/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [14251/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [14301/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [14301/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [14351/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [14351/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [14401/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [14401/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [14451/65307], Loss: 0.0310\n",
            "Epoch [2/2], Step [14451/65307], Loss: 0.0310\n",
            "Epoch [2/2], Step [14501/65307], Loss: 0.3865\n",
            "Epoch [2/2], Step [14501/65307], Loss: 0.3865\n",
            "Epoch [2/2], Step [14551/65307], Loss: 0.3162\n",
            "Epoch [2/2], Step [14551/65307], Loss: 0.3162\n",
            "Epoch [2/2], Step [14601/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [14601/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [14651/65307], Loss: 0.0618\n",
            "Epoch [2/2], Step [14651/65307], Loss: 0.0618\n",
            "Epoch [2/2], Step [14701/65307], Loss: 0.0653\n",
            "Epoch [2/2], Step [14701/65307], Loss: 0.0653\n",
            "Epoch [2/2], Step [14751/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [14751/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [14801/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [14801/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [14851/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [14851/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [14901/65307], Loss: 0.1403\n",
            "Epoch [2/2], Step [14901/65307], Loss: 0.1403\n",
            "Epoch [2/2], Step [14951/65307], Loss: 0.0349\n",
            "Epoch [2/2], Step [14951/65307], Loss: 0.0349\n",
            "Epoch [2/2], Step [15001/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [15001/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [15051/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [15051/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [15101/65307], Loss: 0.0337\n",
            "Epoch [2/2], Step [15101/65307], Loss: 0.0337\n",
            "Epoch [2/2], Step [15151/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [15151/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [15201/65307], Loss: 0.0810\n",
            "Epoch [2/2], Step [15201/65307], Loss: 0.0810\n",
            "Epoch [2/2], Step [15251/65307], Loss: 0.3256\n",
            "Epoch [2/2], Step [15251/65307], Loss: 0.3256\n",
            "Epoch [2/2], Step [15301/65307], Loss: 0.0500\n",
            "Epoch [2/2], Step [15301/65307], Loss: 0.0500\n",
            "Epoch [2/2], Step [15351/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [15351/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [15401/65307], Loss: 0.0943\n",
            "Epoch [2/2], Step [15401/65307], Loss: 0.0943\n",
            "Epoch [2/2], Step [15451/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [15451/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [15501/65307], Loss: 0.0181\n",
            "Epoch [2/2], Step [15501/65307], Loss: 0.0181\n",
            "Epoch [2/2], Step [15551/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [15551/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [15601/65307], Loss: 0.0536\n",
            "Epoch [2/2], Step [15601/65307], Loss: 0.0536\n",
            "Epoch [2/2], Step [15651/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [15651/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [15701/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [15701/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [15751/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [15751/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [15801/65307], Loss: 0.1338\n",
            "Epoch [2/2], Step [15801/65307], Loss: 0.1338\n",
            "Epoch [2/2], Step [15851/65307], Loss: 0.0196\n",
            "Epoch [2/2], Step [15851/65307], Loss: 0.0196\n",
            "Epoch [2/2], Step [15901/65307], Loss: 0.1026\n",
            "Epoch [2/2], Step [15901/65307], Loss: 0.1026\n",
            "Epoch [2/2], Step [15951/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [15951/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [16001/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [16001/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [16051/65307], Loss: 0.2616\n",
            "Epoch [2/2], Step [16051/65307], Loss: 0.2616\n",
            "Epoch [2/2], Step [16101/65307], Loss: 0.1381\n",
            "Epoch [2/2], Step [16101/65307], Loss: 0.1381\n",
            "Epoch [2/2], Step [16151/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [16151/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [16201/65307], Loss: 0.0052\n",
            "Epoch [2/2], Step [16201/65307], Loss: 0.0052\n",
            "Epoch [2/2], Step [16251/65307], Loss: 0.1907\n",
            "Epoch [2/2], Step [16251/65307], Loss: 0.1907\n",
            "Epoch [2/2], Step [16301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [16301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [16351/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [16351/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [16401/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [16401/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [16451/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [16451/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [16501/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [16501/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [16551/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [16551/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [16601/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [16601/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [16651/65307], Loss: 0.1902\n",
            "Epoch [2/2], Step [16651/65307], Loss: 0.1902\n",
            "Epoch [2/2], Step [16701/65307], Loss: 0.0674\n",
            "Epoch [2/2], Step [16701/65307], Loss: 0.0674\n",
            "Epoch [2/2], Step [16751/65307], Loss: 0.5359\n",
            "Epoch [2/2], Step [16751/65307], Loss: 0.5359\n",
            "Epoch [2/2], Step [16801/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [16801/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [16851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [16851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [16901/65307], Loss: 0.1447\n",
            "Epoch [2/2], Step [16901/65307], Loss: 0.1447\n",
            "Epoch [2/2], Step [16951/65307], Loss: 0.0294\n",
            "Epoch [2/2], Step [16951/65307], Loss: 0.0294\n",
            "Epoch [2/2], Step [17001/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [17001/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [17051/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [17051/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [17101/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [17101/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [17151/65307], Loss: 0.0467\n",
            "Epoch [2/2], Step [17151/65307], Loss: 0.0467\n",
            "Epoch [2/2], Step [17201/65307], Loss: 0.1475\n",
            "Epoch [2/2], Step [17201/65307], Loss: 0.1475\n",
            "Epoch [2/2], Step [17251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [17251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [17301/65307], Loss: 0.0925\n",
            "Epoch [2/2], Step [17301/65307], Loss: 0.0925\n",
            "Epoch [2/2], Step [17351/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [17351/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [17401/65307], Loss: 0.0191\n",
            "Epoch [2/2], Step [17401/65307], Loss: 0.0191\n",
            "Epoch [2/2], Step [17451/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [17451/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [17501/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [17501/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [17551/65307], Loss: 0.2900\n",
            "Epoch [2/2], Step [17551/65307], Loss: 0.2900\n",
            "Epoch [2/2], Step [17601/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [17601/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [17651/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [17651/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [17701/65307], Loss: 0.1019\n",
            "Epoch [2/2], Step [17701/65307], Loss: 0.1019\n",
            "Epoch [2/2], Step [17751/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [17751/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [17801/65307], Loss: 0.0379\n",
            "Epoch [2/2], Step [17801/65307], Loss: 0.0379\n",
            "Epoch [2/2], Step [17851/65307], Loss: 0.1320\n",
            "Epoch [2/2], Step [17851/65307], Loss: 0.1320\n",
            "Epoch [2/2], Step [17901/65307], Loss: 0.0813\n",
            "Epoch [2/2], Step [17901/65307], Loss: 0.0813\n",
            "Epoch [2/2], Step [17951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [17951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [18001/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [18001/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [18051/65307], Loss: 0.1671\n",
            "Epoch [2/2], Step [18051/65307], Loss: 0.1671\n",
            "Epoch [2/2], Step [18101/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [18101/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [18151/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [18151/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [18201/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [18201/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [18251/65307], Loss: 0.0553\n",
            "Epoch [2/2], Step [18251/65307], Loss: 0.0553\n",
            "Epoch [2/2], Step [18301/65307], Loss: 0.1136\n",
            "Epoch [2/2], Step [18301/65307], Loss: 0.1136\n",
            "Epoch [2/2], Step [18351/65307], Loss: 0.0102\n",
            "Epoch [2/2], Step [18351/65307], Loss: 0.0102\n",
            "Epoch [2/2], Step [18401/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [18401/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [18451/65307], Loss: 0.0407\n",
            "Epoch [2/2], Step [18451/65307], Loss: 0.0407\n",
            "Epoch [2/2], Step [18501/65307], Loss: 0.0253\n",
            "Epoch [2/2], Step [18501/65307], Loss: 0.0253\n",
            "Epoch [2/2], Step [18551/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [18551/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [18601/65307], Loss: 0.2588\n",
            "Epoch [2/2], Step [18601/65307], Loss: 0.2588\n",
            "Epoch [2/2], Step [18651/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [18651/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [18701/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [18701/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [18751/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [18751/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [18801/65307], Loss: 0.0462\n",
            "Epoch [2/2], Step [18801/65307], Loss: 0.0462\n",
            "Epoch [2/2], Step [18851/65307], Loss: 0.2554\n",
            "Epoch [2/2], Step [18851/65307], Loss: 0.2554\n",
            "Epoch [2/2], Step [18901/65307], Loss: 0.0737\n",
            "Epoch [2/2], Step [18901/65307], Loss: 0.0737\n",
            "Epoch [2/2], Step [18951/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [18951/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [19001/65307], Loss: 0.1421\n",
            "Epoch [2/2], Step [19001/65307], Loss: 0.1421\n",
            "Epoch [2/2], Step [19051/65307], Loss: 0.0442\n",
            "Epoch [2/2], Step [19051/65307], Loss: 0.0442\n",
            "Epoch [2/2], Step [19101/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [19101/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [19151/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [19151/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [19201/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [19201/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [19251/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [19251/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [19301/65307], Loss: 0.0261\n",
            "Epoch [2/2], Step [19301/65307], Loss: 0.0261\n",
            "Epoch [2/2], Step [19351/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [19351/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [19401/65307], Loss: 0.4466\n",
            "Epoch [2/2], Step [19401/65307], Loss: 0.4466\n",
            "Epoch [2/2], Step [19451/65307], Loss: 0.1384\n",
            "Epoch [2/2], Step [19451/65307], Loss: 0.1384\n",
            "Epoch [2/2], Step [19501/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [19501/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [19551/65307], Loss: 0.0114\n",
            "Epoch [2/2], Step [19551/65307], Loss: 0.0114\n",
            "Epoch [2/2], Step [19601/65307], Loss: 0.2517\n",
            "Epoch [2/2], Step [19601/65307], Loss: 0.2517\n",
            "Epoch [2/2], Step [19651/65307], Loss: 0.0970\n",
            "Epoch [2/2], Step [19651/65307], Loss: 0.0970\n",
            "Epoch [2/2], Step [19701/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [19701/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [19751/65307], Loss: 0.0727\n",
            "Epoch [2/2], Step [19751/65307], Loss: 0.0727\n",
            "Epoch [2/2], Step [19801/65307], Loss: 0.1502\n",
            "Epoch [2/2], Step [19801/65307], Loss: 0.1502\n",
            "Epoch [2/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [19901/65307], Loss: 0.0365\n",
            "Epoch [2/2], Step [19901/65307], Loss: 0.0365\n",
            "Epoch [2/2], Step [19951/65307], Loss: 0.1281\n",
            "Epoch [2/2], Step [19951/65307], Loss: 0.1281\n",
            "Epoch [2/2], Step [20001/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [20001/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [20051/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [20051/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [20101/65307], Loss: 0.2540\n",
            "Epoch [2/2], Step [20101/65307], Loss: 0.2540\n",
            "Epoch [2/2], Step [20151/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [20151/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [20201/65307], Loss: 0.0577\n",
            "Epoch [2/2], Step [20201/65307], Loss: 0.0577\n",
            "Epoch [2/2], Step [20251/65307], Loss: 0.0790\n",
            "Epoch [2/2], Step [20251/65307], Loss: 0.0790\n",
            "Epoch [2/2], Step [20301/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [20301/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [20351/65307], Loss: 0.0466\n",
            "Epoch [2/2], Step [20351/65307], Loss: 0.0466\n",
            "Epoch [2/2], Step [20401/65307], Loss: 0.3849\n",
            "Epoch [2/2], Step [20401/65307], Loss: 0.3849\n",
            "Epoch [2/2], Step [20451/65307], Loss: 0.0437\n",
            "Epoch [2/2], Step [20451/65307], Loss: 0.0437\n",
            "Epoch [2/2], Step [20501/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [20501/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [20551/65307], Loss: 0.0311\n",
            "Epoch [2/2], Step [20551/65307], Loss: 0.0311\n",
            "Epoch [2/2], Step [20601/65307], Loss: 0.0366\n",
            "Epoch [2/2], Step [20601/65307], Loss: 0.0366\n",
            "Epoch [2/2], Step [20651/65307], Loss: 0.0463\n",
            "Epoch [2/2], Step [20651/65307], Loss: 0.0463\n",
            "Epoch [2/2], Step [20701/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [20701/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [20751/65307], Loss: 0.0393\n",
            "Epoch [2/2], Step [20751/65307], Loss: 0.0393\n",
            "Epoch [2/2], Step [20801/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [20801/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [20851/65307], Loss: 0.0517\n",
            "Epoch [2/2], Step [20851/65307], Loss: 0.0517\n",
            "Epoch [2/2], Step [20901/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [20901/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [20951/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [20951/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [21001/65307], Loss: 0.2528\n",
            "Epoch [2/2], Step [21001/65307], Loss: 0.2528\n",
            "Epoch [2/2], Step [21051/65307], Loss: 0.1940\n",
            "Epoch [2/2], Step [21051/65307], Loss: 0.1940\n",
            "Epoch [2/2], Step [21101/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [21101/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [21151/65307], Loss: 0.1724\n",
            "Epoch [2/2], Step [21151/65307], Loss: 0.1724\n",
            "Epoch [2/2], Step [21201/65307], Loss: 0.3967\n",
            "Epoch [2/2], Step [21201/65307], Loss: 0.3967\n",
            "Epoch [2/2], Step [21251/65307], Loss: 0.1763\n",
            "Epoch [2/2], Step [21251/65307], Loss: 0.1763\n",
            "Epoch [2/2], Step [21301/65307], Loss: 0.0921\n",
            "Epoch [2/2], Step [21301/65307], Loss: 0.0921\n",
            "Epoch [2/2], Step [21351/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [21351/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [21401/65307], Loss: 0.1663\n",
            "Epoch [2/2], Step [21401/65307], Loss: 0.1663\n",
            "Epoch [2/2], Step [21451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [21451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [21501/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [21501/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [21551/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [21551/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [21601/65307], Loss: 0.0744\n",
            "Epoch [2/2], Step [21601/65307], Loss: 0.0744\n",
            "Epoch [2/2], Step [21651/65307], Loss: 0.1831\n",
            "Epoch [2/2], Step [21651/65307], Loss: 0.1831\n",
            "Epoch [2/2], Step [21701/65307], Loss: 0.1045\n",
            "Epoch [2/2], Step [21701/65307], Loss: 0.1045\n",
            "Epoch [2/2], Step [21751/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [21751/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [21801/65307], Loss: 0.1750\n",
            "Epoch [2/2], Step [21801/65307], Loss: 0.1750\n",
            "Epoch [2/2], Step [21851/65307], Loss: 0.0042\n",
            "Epoch [2/2], Step [21851/65307], Loss: 0.0042\n",
            "Epoch [2/2], Step [21901/65307], Loss: 0.0954\n",
            "Epoch [2/2], Step [21901/65307], Loss: 0.0954\n",
            "Epoch [2/2], Step [21951/65307], Loss: 0.0733\n",
            "Epoch [2/2], Step [21951/65307], Loss: 0.0733\n",
            "Epoch [2/2], Step [22001/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [22001/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [22051/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [22051/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [22101/65307], Loss: 0.0620\n",
            "Epoch [2/2], Step [22101/65307], Loss: 0.0620\n",
            "Epoch [2/2], Step [22151/65307], Loss: 0.1406\n",
            "Epoch [2/2], Step [22151/65307], Loss: 0.1406\n",
            "Epoch [2/2], Step [22201/65307], Loss: 0.1177\n",
            "Epoch [2/2], Step [22201/65307], Loss: 0.1177\n",
            "Epoch [2/2], Step [22251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [22251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [22301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [22301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [22351/65307], Loss: 0.1587\n",
            "Epoch [2/2], Step [22351/65307], Loss: 0.1587\n",
            "Epoch [2/2], Step [22401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [22401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [22451/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [22451/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [22501/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22501/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22551/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [22551/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [22601/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [22601/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [22651/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [22651/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [22701/65307], Loss: 0.0881\n",
            "Epoch [2/2], Step [22701/65307], Loss: 0.0881\n",
            "Epoch [2/2], Step [22751/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [22751/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [22801/65307], Loss: 0.1060\n",
            "Epoch [2/2], Step [22801/65307], Loss: 0.1060\n",
            "Epoch [2/2], Step [22851/65307], Loss: 0.0597\n",
            "Epoch [2/2], Step [22851/65307], Loss: 0.0597\n",
            "Epoch [2/2], Step [22901/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22901/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22951/65307], Loss: 0.1456\n",
            "Epoch [2/2], Step [22951/65307], Loss: 0.1456\n",
            "Epoch [2/2], Step [23001/65307], Loss: 0.0277\n",
            "Epoch [2/2], Step [23001/65307], Loss: 0.0277\n",
            "Epoch [2/2], Step [23051/65307], Loss: 0.0911\n",
            "Epoch [2/2], Step [23051/65307], Loss: 0.0911\n",
            "Epoch [2/2], Step [23101/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [23101/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [23151/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [23151/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [23201/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [23201/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [23251/65307], Loss: 0.1159\n",
            "Epoch [2/2], Step [23251/65307], Loss: 0.1159\n",
            "Epoch [2/2], Step [23301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [23301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [23351/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [23351/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [23401/65307], Loss: 0.0243\n",
            "Epoch [2/2], Step [23401/65307], Loss: 0.0243\n",
            "Epoch [2/2], Step [23451/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [23451/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [23501/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23501/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23551/65307], Loss: 0.3817\n",
            "Epoch [2/2], Step [23551/65307], Loss: 0.3817\n",
            "Epoch [2/2], Step [23601/65307], Loss: 0.0859\n",
            "Epoch [2/2], Step [23601/65307], Loss: 0.0859\n",
            "Epoch [2/2], Step [23651/65307], Loss: 0.0747\n",
            "Epoch [2/2], Step [23651/65307], Loss: 0.0747\n",
            "Epoch [2/2], Step [23701/65307], Loss: 0.1554\n",
            "Epoch [2/2], Step [23701/65307], Loss: 0.1554\n",
            "Epoch [2/2], Step [23751/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [23751/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [23801/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [23801/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [23851/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [23851/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [23901/65307], Loss: 0.0875\n",
            "Epoch [2/2], Step [23901/65307], Loss: 0.0875\n",
            "Epoch [2/2], Step [23951/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23951/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [24001/65307], Loss: 0.0740\n",
            "Epoch [2/2], Step [24001/65307], Loss: 0.0740\n",
            "Epoch [2/2], Step [24051/65307], Loss: 0.0255\n",
            "Epoch [2/2], Step [24051/65307], Loss: 0.0255\n",
            "Epoch [2/2], Step [24101/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [24101/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [24151/65307], Loss: 0.0892\n",
            "Epoch [2/2], Step [24151/65307], Loss: 0.0892\n",
            "Epoch [2/2], Step [24201/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [24201/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [24251/65307], Loss: 0.1537\n",
            "Epoch [2/2], Step [24251/65307], Loss: 0.1537\n",
            "Epoch [2/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [2/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [2/2], Step [24351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [24351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [24401/65307], Loss: 0.0530\n",
            "Epoch [2/2], Step [24401/65307], Loss: 0.0530\n",
            "Epoch [2/2], Step [24451/65307], Loss: 0.0258\n",
            "Epoch [2/2], Step [24451/65307], Loss: 0.0258\n",
            "Epoch [2/2], Step [24501/65307], Loss: 0.1511\n",
            "Epoch [2/2], Step [24501/65307], Loss: 0.1511\n",
            "Epoch [2/2], Step [24551/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [24551/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [24601/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [24601/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [24651/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [24651/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [24701/65307], Loss: 0.1368\n",
            "Epoch [2/2], Step [24701/65307], Loss: 0.1368\n",
            "Epoch [2/2], Step [24751/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [24751/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [24801/65307], Loss: 0.0691\n",
            "Epoch [2/2], Step [24801/65307], Loss: 0.0691\n",
            "Epoch [2/2], Step [24851/65307], Loss: 0.0159\n",
            "Epoch [2/2], Step [24851/65307], Loss: 0.0159\n",
            "Epoch [2/2], Step [24901/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [24901/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [24951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [24951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [25001/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [25001/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [25051/65307], Loss: 0.2888\n",
            "Epoch [2/2], Step [25051/65307], Loss: 0.2888\n",
            "Epoch [2/2], Step [25101/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [25101/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [25151/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [25151/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [25201/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [25201/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [25251/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [25251/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [25301/65307], Loss: 0.0403\n",
            "Epoch [2/2], Step [25301/65307], Loss: 0.0403\n",
            "Epoch [2/2], Step [25351/65307], Loss: 0.0578\n",
            "Epoch [2/2], Step [25351/65307], Loss: 0.0578\n",
            "Epoch [2/2], Step [25401/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [25401/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [25451/65307], Loss: 0.0405\n",
            "Epoch [2/2], Step [25451/65307], Loss: 0.0405\n",
            "Epoch [2/2], Step [25501/65307], Loss: 0.1298\n",
            "Epoch [2/2], Step [25501/65307], Loss: 0.1298\n",
            "Epoch [2/2], Step [25551/65307], Loss: 0.1440\n",
            "Epoch [2/2], Step [25551/65307], Loss: 0.1440\n",
            "Epoch [2/2], Step [25601/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [25601/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [25651/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [25651/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [25701/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [25701/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [25751/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [25751/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [25801/65307], Loss: 0.0472\n",
            "Epoch [2/2], Step [25801/65307], Loss: 0.0472\n",
            "Epoch [2/2], Step [25851/65307], Loss: 0.0117\n",
            "Epoch [2/2], Step [25851/65307], Loss: 0.0117\n",
            "Epoch [2/2], Step [25901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [25901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [25951/65307], Loss: 0.1624\n",
            "Epoch [2/2], Step [25951/65307], Loss: 0.1624\n",
            "Epoch [2/2], Step [26001/65307], Loss: 0.0142\n",
            "Epoch [2/2], Step [26001/65307], Loss: 0.0142\n",
            "Epoch [2/2], Step [26051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [26051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [26101/65307], Loss: 0.0495\n",
            "Epoch [2/2], Step [26101/65307], Loss: 0.0495\n",
            "Epoch [2/2], Step [26151/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [26151/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [26201/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [26201/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [26251/65307], Loss: 0.0809\n",
            "Epoch [2/2], Step [26251/65307], Loss: 0.0809\n",
            "Epoch [2/2], Step [26301/65307], Loss: 0.2137\n",
            "Epoch [2/2], Step [26301/65307], Loss: 0.2137\n",
            "Epoch [2/2], Step [26351/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [26351/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [26401/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [26401/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [26451/65307], Loss: 0.1083\n",
            "Epoch [2/2], Step [26451/65307], Loss: 0.1083\n",
            "Epoch [2/2], Step [26501/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [26501/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [26551/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [26551/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [26601/65307], Loss: 0.1928\n",
            "Epoch [2/2], Step [26601/65307], Loss: 0.1928\n",
            "Epoch [2/2], Step [26651/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [26651/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [26701/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [26701/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [26751/65307], Loss: 0.0213\n",
            "Epoch [2/2], Step [26751/65307], Loss: 0.0213\n",
            "Epoch [2/2], Step [26801/65307], Loss: 0.2139\n",
            "Epoch [2/2], Step [26801/65307], Loss: 0.2139\n",
            "Epoch [2/2], Step [26851/65307], Loss: 0.0127\n",
            "Epoch [2/2], Step [26851/65307], Loss: 0.0127\n",
            "Epoch [2/2], Step [26901/65307], Loss: 0.2513\n",
            "Epoch [2/2], Step [26901/65307], Loss: 0.2513\n",
            "Epoch [2/2], Step [26951/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [26951/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [27001/65307], Loss: 0.1542\n",
            "Epoch [2/2], Step [27001/65307], Loss: 0.1542\n",
            "Epoch [2/2], Step [27051/65307], Loss: 0.0491\n",
            "Epoch [2/2], Step [27051/65307], Loss: 0.0491\n",
            "Epoch [2/2], Step [27101/65307], Loss: 0.0777\n",
            "Epoch [2/2], Step [27101/65307], Loss: 0.0777\n",
            "Epoch [2/2], Step [27151/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [27151/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [27201/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [27201/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [27251/65307], Loss: 0.0410\n",
            "Epoch [2/2], Step [27251/65307], Loss: 0.0410\n",
            "Epoch [2/2], Step [27301/65307], Loss: 0.0715\n",
            "Epoch [2/2], Step [27301/65307], Loss: 0.0715\n",
            "Epoch [2/2], Step [27351/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [27351/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [27401/65307], Loss: 0.4865\n",
            "Epoch [2/2], Step [27401/65307], Loss: 0.4865\n",
            "Epoch [2/2], Step [27451/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [27451/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [27501/65307], Loss: 0.1147\n",
            "Epoch [2/2], Step [27501/65307], Loss: 0.1147\n",
            "Epoch [2/2], Step [27551/65307], Loss: 0.1313\n",
            "Epoch [2/2], Step [27551/65307], Loss: 0.1313\n",
            "Epoch [2/2], Step [27601/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [27601/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [27651/65307], Loss: 0.0768\n",
            "Epoch [2/2], Step [27651/65307], Loss: 0.0768\n",
            "Epoch [2/2], Step [27701/65307], Loss: 0.2505\n",
            "Epoch [2/2], Step [27701/65307], Loss: 0.2505\n",
            "Epoch [2/2], Step [27751/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [27751/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [27801/65307], Loss: 0.1976\n",
            "Epoch [2/2], Step [27801/65307], Loss: 0.1976\n",
            "Epoch [2/2], Step [27851/65307], Loss: 0.0978\n",
            "Epoch [2/2], Step [27851/65307], Loss: 0.0978\n",
            "Epoch [2/2], Step [27901/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [27901/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [27951/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [27951/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [28001/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [28001/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [28051/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [28051/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [28101/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [28101/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [28151/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [28151/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [28201/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [28201/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [28251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [28251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [28301/65307], Loss: 0.0088\n",
            "Epoch [2/2], Step [28301/65307], Loss: 0.0088\n",
            "Epoch [2/2], Step [28351/65307], Loss: 0.0918\n",
            "Epoch [2/2], Step [28351/65307], Loss: 0.0918\n",
            "Epoch [2/2], Step [28401/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [28401/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [28451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [28451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [28501/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [28501/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [28551/65307], Loss: 0.0239\n",
            "Epoch [2/2], Step [28551/65307], Loss: 0.0239\n",
            "Epoch [2/2], Step [28601/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [28601/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [28651/65307], Loss: 0.1942\n",
            "Epoch [2/2], Step [28651/65307], Loss: 0.1942\n",
            "Epoch [2/2], Step [28701/65307], Loss: 0.0178\n",
            "Epoch [2/2], Step [28701/65307], Loss: 0.0178\n",
            "Epoch [2/2], Step [28751/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [28751/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [28801/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [28801/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [28851/65307], Loss: 0.0803\n",
            "Epoch [2/2], Step [28851/65307], Loss: 0.0803\n",
            "Epoch [2/2], Step [28901/65307], Loss: 0.2707\n",
            "Epoch [2/2], Step [28901/65307], Loss: 0.2707\n",
            "Epoch [2/2], Step [28951/65307], Loss: 0.2586\n",
            "Epoch [2/2], Step [28951/65307], Loss: 0.2586\n",
            "Epoch [2/2], Step [29001/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [29001/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [29051/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [29051/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [29101/65307], Loss: 0.1504\n",
            "Epoch [2/2], Step [29101/65307], Loss: 0.1504\n",
            "Epoch [2/2], Step [29151/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [29151/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [29201/65307], Loss: 0.1651\n",
            "Epoch [2/2], Step [29201/65307], Loss: 0.1651\n",
            "Epoch [2/2], Step [29251/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [29251/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [29301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [29301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [29351/65307], Loss: 0.1769\n",
            "Epoch [2/2], Step [29351/65307], Loss: 0.1769\n",
            "Epoch [2/2], Step [29401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [29401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [29451/65307], Loss: 0.2201\n",
            "Epoch [2/2], Step [29451/65307], Loss: 0.2201\n",
            "Epoch [2/2], Step [29501/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [29501/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [29551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [29551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [29601/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [29601/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [29651/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [29651/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [29701/65307], Loss: 0.0805\n",
            "Epoch [2/2], Step [29701/65307], Loss: 0.0805\n",
            "Epoch [2/2], Step [29751/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [29751/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [29801/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [29801/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [29851/65307], Loss: 0.0435\n",
            "Epoch [2/2], Step [29851/65307], Loss: 0.0435\n",
            "Epoch [2/2], Step [29901/65307], Loss: 0.0754\n",
            "Epoch [2/2], Step [29901/65307], Loss: 0.0754\n",
            "Epoch [2/2], Step [29951/65307], Loss: 0.3547\n",
            "Epoch [2/2], Step [29951/65307], Loss: 0.3547\n",
            "Epoch [2/2], Step [30001/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [30001/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [30051/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [30051/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [30101/65307], Loss: 0.0816\n",
            "Epoch [2/2], Step [30101/65307], Loss: 0.0816\n",
            "Epoch [2/2], Step [30151/65307], Loss: 0.0486\n",
            "Epoch [2/2], Step [30151/65307], Loss: 0.0486\n",
            "Epoch [2/2], Step [30201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [30201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [30251/65307], Loss: 0.1745\n",
            "Epoch [2/2], Step [30251/65307], Loss: 0.1745\n",
            "Epoch [2/2], Step [30301/65307], Loss: 0.0845\n",
            "Epoch [2/2], Step [30301/65307], Loss: 0.0845\n",
            "Epoch [2/2], Step [30351/65307], Loss: 0.1848\n",
            "Epoch [2/2], Step [30351/65307], Loss: 0.1848\n",
            "Epoch [2/2], Step [30401/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [30401/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [30451/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [30451/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [30501/65307], Loss: 0.2430\n",
            "Epoch [2/2], Step [30501/65307], Loss: 0.2430\n",
            "Epoch [2/2], Step [30551/65307], Loss: 0.2051\n",
            "Epoch [2/2], Step [30551/65307], Loss: 0.2051\n",
            "Epoch [2/2], Step [30601/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [30601/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [30651/65307], Loss: 0.1515\n",
            "Epoch [2/2], Step [30651/65307], Loss: 0.1515\n",
            "Epoch [2/2], Step [30701/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [30701/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [30751/65307], Loss: 0.0962\n",
            "Epoch [2/2], Step [30751/65307], Loss: 0.0962\n",
            "Epoch [2/2], Step [30801/65307], Loss: 0.0469\n",
            "Epoch [2/2], Step [30801/65307], Loss: 0.0469\n",
            "Epoch [2/2], Step [30851/65307], Loss: 0.0801\n",
            "Epoch [2/2], Step [30851/65307], Loss: 0.0801\n",
            "Epoch [2/2], Step [30901/65307], Loss: 0.1987\n",
            "Epoch [2/2], Step [30901/65307], Loss: 0.1987\n",
            "Epoch [2/2], Step [30951/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [30951/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [31001/65307], Loss: 0.0119\n",
            "Epoch [2/2], Step [31001/65307], Loss: 0.0119\n",
            "Epoch [2/2], Step [31051/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [31051/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [31101/65307], Loss: 0.0168\n",
            "Epoch [2/2], Step [31101/65307], Loss: 0.0168\n",
            "Epoch [2/2], Step [31151/65307], Loss: 0.0194\n",
            "Epoch [2/2], Step [31151/65307], Loss: 0.0194\n",
            "Epoch [2/2], Step [31201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [31201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [31251/65307], Loss: 0.0129\n",
            "Epoch [2/2], Step [31251/65307], Loss: 0.0129\n",
            "Epoch [2/2], Step [31301/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [31301/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [31351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [31351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [31401/65307], Loss: 0.1642\n",
            "Epoch [2/2], Step [31401/65307], Loss: 0.1642\n",
            "Epoch [2/2], Step [31451/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [31451/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [31501/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [31501/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [31551/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [31551/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [31601/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [31601/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [31651/65307], Loss: 0.1777\n",
            "Epoch [2/2], Step [31651/65307], Loss: 0.1777\n",
            "Epoch [2/2], Step [31701/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [31701/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [31751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [31751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [31801/65307], Loss: 0.4305\n",
            "Epoch [2/2], Step [31801/65307], Loss: 0.4305\n",
            "Epoch [2/2], Step [31851/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [31851/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [31901/65307], Loss: 0.0574\n",
            "Epoch [2/2], Step [31901/65307], Loss: 0.0574\n",
            "Epoch [2/2], Step [31951/65307], Loss: 0.0134\n",
            "Epoch [2/2], Step [31951/65307], Loss: 0.0134\n",
            "Epoch [2/2], Step [32001/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [32001/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [32051/65307], Loss: 0.0949\n",
            "Epoch [2/2], Step [32051/65307], Loss: 0.0949\n",
            "Epoch [2/2], Step [32101/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [32101/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [32151/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [32151/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [32201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [32201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [32251/65307], Loss: 0.0408\n",
            "Epoch [2/2], Step [32251/65307], Loss: 0.0408\n",
            "Epoch [2/2], Step [32301/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [32301/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [32351/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [32351/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [32401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [32401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [32451/65307], Loss: 0.0214\n",
            "Epoch [2/2], Step [32451/65307], Loss: 0.0214\n",
            "Epoch [2/2], Step [32501/65307], Loss: 0.0444\n",
            "Epoch [2/2], Step [32501/65307], Loss: 0.0444\n",
            "Epoch [2/2], Step [32551/65307], Loss: 0.0019\n",
            "Epoch [2/2], Step [32551/65307], Loss: 0.0019\n",
            "Epoch [2/2], Step [32601/65307], Loss: 0.0861\n",
            "Epoch [2/2], Step [32601/65307], Loss: 0.0861\n",
            "Epoch [2/2], Step [32651/65307], Loss: 0.2033\n",
            "Epoch [2/2], Step [32651/65307], Loss: 0.2033\n",
            "Epoch [2/2], Step [32701/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [32701/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [32751/65307], Loss: 0.0822\n",
            "Epoch [2/2], Step [32751/65307], Loss: 0.0822\n",
            "Epoch [2/2], Step [32801/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [32801/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [32851/65307], Loss: 0.0409\n",
            "Epoch [2/2], Step [32851/65307], Loss: 0.0409\n",
            "Epoch [2/2], Step [32901/65307], Loss: 0.0794\n",
            "Epoch [2/2], Step [32901/65307], Loss: 0.0794\n",
            "Epoch [2/2], Step [32951/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [32951/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [33001/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [33001/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [33051/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [33051/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [33101/65307], Loss: 0.1031\n",
            "Epoch [2/2], Step [33101/65307], Loss: 0.1031\n",
            "Epoch [2/2], Step [33151/65307], Loss: 0.1065\n",
            "Epoch [2/2], Step [33151/65307], Loss: 0.1065\n",
            "Epoch [2/2], Step [33201/65307], Loss: 0.0128\n",
            "Epoch [2/2], Step [33201/65307], Loss: 0.0128\n",
            "Epoch [2/2], Step [33251/65307], Loss: 0.1675\n",
            "Epoch [2/2], Step [33251/65307], Loss: 0.1675\n",
            "Epoch [2/2], Step [33301/65307], Loss: 0.0797\n",
            "Epoch [2/2], Step [33301/65307], Loss: 0.0797\n",
            "Epoch [2/2], Step [33351/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [33351/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [33401/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [33401/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [33451/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [33451/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [33501/65307], Loss: 0.0776\n",
            "Epoch [2/2], Step [33501/65307], Loss: 0.0776\n",
            "Epoch [2/2], Step [33551/65307], Loss: 0.1588\n",
            "Epoch [2/2], Step [33551/65307], Loss: 0.1588\n",
            "Epoch [2/2], Step [33601/65307], Loss: 0.2607\n",
            "Epoch [2/2], Step [33601/65307], Loss: 0.2607\n",
            "Epoch [2/2], Step [33651/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [33651/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [33701/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [33701/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [33751/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [33751/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [33801/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [33801/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [33851/65307], Loss: 0.0041\n",
            "Epoch [2/2], Step [33851/65307], Loss: 0.0041\n",
            "Epoch [2/2], Step [33901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [33901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [33951/65307], Loss: 0.2519\n",
            "Epoch [2/2], Step [33951/65307], Loss: 0.2519\n",
            "Epoch [2/2], Step [34001/65307], Loss: 0.1113\n",
            "Epoch [2/2], Step [34001/65307], Loss: 0.1113\n",
            "Epoch [2/2], Step [34051/65307], Loss: 0.3291\n",
            "Epoch [2/2], Step [34051/65307], Loss: 0.3291\n",
            "Epoch [2/2], Step [34101/65307], Loss: 0.4168\n",
            "Epoch [2/2], Step [34101/65307], Loss: 0.4168\n",
            "Epoch [2/2], Step [34151/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [34151/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [34201/65307], Loss: 0.1014\n",
            "Epoch [2/2], Step [34201/65307], Loss: 0.1014\n",
            "Epoch [2/2], Step [34251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [34251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [34301/65307], Loss: 0.0319\n",
            "Epoch [2/2], Step [34301/65307], Loss: 0.0319\n",
            "Epoch [2/2], Step [34351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [34351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [34401/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [34401/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [34451/65307], Loss: 0.1971\n",
            "Epoch [2/2], Step [34451/65307], Loss: 0.1971\n",
            "Epoch [2/2], Step [34501/65307], Loss: 0.0417\n",
            "Epoch [2/2], Step [34501/65307], Loss: 0.0417\n",
            "Epoch [2/2], Step [34551/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [34551/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [34601/65307], Loss: 0.0389\n",
            "Epoch [2/2], Step [34601/65307], Loss: 0.0389\n",
            "Epoch [2/2], Step [34651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [34651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [34701/65307], Loss: 0.3792\n",
            "Epoch [2/2], Step [34701/65307], Loss: 0.3792\n",
            "Epoch [2/2], Step [34751/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [34751/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [34801/65307], Loss: 0.1626\n",
            "Epoch [2/2], Step [34801/65307], Loss: 0.1626\n",
            "Epoch [2/2], Step [34851/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [34851/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [34901/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [34901/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [34951/65307], Loss: 0.0506\n",
            "Epoch [2/2], Step [34951/65307], Loss: 0.0506\n",
            "Epoch [2/2], Step [35001/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [35001/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [35051/65307], Loss: 0.0281\n",
            "Epoch [2/2], Step [35051/65307], Loss: 0.0281\n",
            "Epoch [2/2], Step [35101/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [35101/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [35151/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [35151/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [35201/65307], Loss: 0.0474\n",
            "Epoch [2/2], Step [35201/65307], Loss: 0.0474\n",
            "Epoch [2/2], Step [35251/65307], Loss: 0.0496\n",
            "Epoch [2/2], Step [35251/65307], Loss: 0.0496\n",
            "Epoch [2/2], Step [35301/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [35301/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [35351/65307], Loss: 0.1877\n",
            "Epoch [2/2], Step [35351/65307], Loss: 0.1877\n",
            "Epoch [2/2], Step [35401/65307], Loss: 0.1051\n",
            "Epoch [2/2], Step [35401/65307], Loss: 0.1051\n",
            "Epoch [2/2], Step [35451/65307], Loss: 0.1689\n",
            "Epoch [2/2], Step [35451/65307], Loss: 0.1689\n",
            "Epoch [2/2], Step [35501/65307], Loss: 0.0199\n",
            "Epoch [2/2], Step [35501/65307], Loss: 0.0199\n",
            "Epoch [2/2], Step [35551/65307], Loss: 0.0024\n",
            "Epoch [2/2], Step [35551/65307], Loss: 0.0024\n",
            "Epoch [2/2], Step [35601/65307], Loss: 0.2444\n",
            "Epoch [2/2], Step [35601/65307], Loss: 0.2444\n",
            "Epoch [2/2], Step [35651/65307], Loss: 0.4366\n",
            "Epoch [2/2], Step [35651/65307], Loss: 0.4366\n",
            "Epoch [2/2], Step [35701/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [35701/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [35751/65307], Loss: 0.1611\n",
            "Epoch [2/2], Step [35751/65307], Loss: 0.1611\n",
            "Epoch [2/2], Step [35801/65307], Loss: 0.0484\n",
            "Epoch [2/2], Step [35801/65307], Loss: 0.0484\n",
            "Epoch [2/2], Step [35851/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [35851/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [35901/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [35901/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [35951/65307], Loss: 0.1653\n",
            "Epoch [2/2], Step [35951/65307], Loss: 0.1653\n",
            "Epoch [2/2], Step [36001/65307], Loss: 0.1105\n",
            "Epoch [2/2], Step [36001/65307], Loss: 0.1105\n",
            "Epoch [2/2], Step [36051/65307], Loss: 0.3247\n",
            "Epoch [2/2], Step [36051/65307], Loss: 0.3247\n",
            "Epoch [2/2], Step [36101/65307], Loss: 0.3296\n",
            "Epoch [2/2], Step [36101/65307], Loss: 0.3296\n",
            "Epoch [2/2], Step [36151/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [36151/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [36201/65307], Loss: 0.0257\n",
            "Epoch [2/2], Step [36201/65307], Loss: 0.0257\n",
            "Epoch [2/2], Step [36251/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [36251/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [36301/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [36301/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [36351/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [36351/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [36401/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [36401/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [36451/65307], Loss: 0.0696\n",
            "Epoch [2/2], Step [36451/65307], Loss: 0.0696\n",
            "Epoch [2/2], Step [36501/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [36501/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [36551/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [36551/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [36601/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [36601/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [36651/65307], Loss: 0.0218\n",
            "Epoch [2/2], Step [36651/65307], Loss: 0.0218\n",
            "Epoch [2/2], Step [36701/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [36701/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [36751/65307], Loss: 0.4773\n",
            "Epoch [2/2], Step [36751/65307], Loss: 0.4773\n",
            "Epoch [2/2], Step [36801/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [36801/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [36851/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [36851/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [36901/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [36901/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [36951/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [36951/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [37001/65307], Loss: 0.0458\n",
            "Epoch [2/2], Step [37001/65307], Loss: 0.0458\n",
            "Epoch [2/2], Step [37051/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [37051/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [37101/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [37101/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [37151/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [37151/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [37201/65307], Loss: 0.0418\n",
            "Epoch [2/2], Step [37201/65307], Loss: 0.0418\n",
            "Epoch [2/2], Step [37251/65307], Loss: 0.2036\n",
            "Epoch [2/2], Step [37251/65307], Loss: 0.2036\n",
            "Epoch [2/2], Step [37301/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [37301/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [37351/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [37351/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [37401/65307], Loss: 0.0718\n",
            "Epoch [2/2], Step [37401/65307], Loss: 0.0718\n",
            "Epoch [2/2], Step [37451/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [37451/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [37501/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [37501/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [37551/65307], Loss: 0.1496\n",
            "Epoch [2/2], Step [37551/65307], Loss: 0.1496\n",
            "Epoch [2/2], Step [37601/65307], Loss: 0.5549\n",
            "Epoch [2/2], Step [37601/65307], Loss: 0.5549\n",
            "Epoch [2/2], Step [37651/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [37651/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [37701/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [37701/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [37751/65307], Loss: 0.1775\n",
            "Epoch [2/2], Step [37751/65307], Loss: 0.1775\n",
            "Epoch [2/2], Step [37801/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [37801/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [37851/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [37851/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [37901/65307], Loss: 0.4917\n",
            "Epoch [2/2], Step [37901/65307], Loss: 0.4917\n",
            "Epoch [2/2], Step [37951/65307], Loss: 0.2535\n",
            "Epoch [2/2], Step [37951/65307], Loss: 0.2535\n",
            "Epoch [2/2], Step [38001/65307], Loss: 0.0516\n",
            "Epoch [2/2], Step [38001/65307], Loss: 0.0516\n",
            "Epoch [2/2], Step [38051/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [38051/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [38101/65307], Loss: 0.1226\n",
            "Epoch [2/2], Step [38101/65307], Loss: 0.1226\n",
            "Epoch [2/2], Step [38151/65307], Loss: 0.0327\n",
            "Epoch [2/2], Step [38151/65307], Loss: 0.0327\n",
            "Epoch [2/2], Step [38201/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [38201/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [38251/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [38251/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [38301/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [38301/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [38351/65307], Loss: 0.1235\n",
            "Epoch [2/2], Step [38351/65307], Loss: 0.1235\n",
            "Epoch [2/2], Step [38401/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [38401/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [38451/65307], Loss: 0.0376\n",
            "Epoch [2/2], Step [38451/65307], Loss: 0.0376\n",
            "Epoch [2/2], Step [38501/65307], Loss: 0.0610\n",
            "Epoch [2/2], Step [38501/65307], Loss: 0.0610\n",
            "Epoch [2/2], Step [38551/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38551/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38601/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [38601/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [38651/65307], Loss: 0.0361\n",
            "Epoch [2/2], Step [38651/65307], Loss: 0.0361\n",
            "Epoch [2/2], Step [38701/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [38701/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [38751/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [38751/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [38801/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38801/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38851/65307], Loss: 0.0711\n",
            "Epoch [2/2], Step [38851/65307], Loss: 0.0711\n",
            "Epoch [2/2], Step [38901/65307], Loss: 0.0123\n",
            "Epoch [2/2], Step [38901/65307], Loss: 0.0123\n",
            "Epoch [2/2], Step [38951/65307], Loss: 0.2177\n",
            "Epoch [2/2], Step [38951/65307], Loss: 0.2177\n",
            "Epoch [2/2], Step [39001/65307], Loss: 0.0368\n",
            "Epoch [2/2], Step [39001/65307], Loss: 0.0368\n",
            "Epoch [2/2], Step [39051/65307], Loss: 0.1826\n",
            "Epoch [2/2], Step [39051/65307], Loss: 0.1826\n",
            "Epoch [2/2], Step [39101/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [39101/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [39151/65307], Loss: 0.1132\n",
            "Epoch [2/2], Step [39151/65307], Loss: 0.1132\n",
            "Epoch [2/2], Step [39201/65307], Loss: 0.3011\n",
            "Epoch [2/2], Step [39201/65307], Loss: 0.3011\n",
            "Epoch [2/2], Step [39251/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [39251/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [39301/65307], Loss: 0.0554\n",
            "Epoch [2/2], Step [39301/65307], Loss: 0.0554\n",
            "Epoch [2/2], Step [39351/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [39351/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [39401/65307], Loss: 0.1627\n",
            "Epoch [2/2], Step [39401/65307], Loss: 0.1627\n",
            "Epoch [2/2], Step [39451/65307], Loss: 0.0184\n",
            "Epoch [2/2], Step [39451/65307], Loss: 0.0184\n",
            "Epoch [2/2], Step [39501/65307], Loss: 0.0148\n",
            "Epoch [2/2], Step [39501/65307], Loss: 0.0148\n",
            "Epoch [2/2], Step [39551/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [39551/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [39601/65307], Loss: 0.0775\n",
            "Epoch [2/2], Step [39601/65307], Loss: 0.0775\n",
            "Epoch [2/2], Step [39651/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [39651/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [39701/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [39701/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [39751/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [39751/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [39801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [39801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [39851/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [39851/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [39901/65307], Loss: 0.0313\n",
            "Epoch [2/2], Step [39901/65307], Loss: 0.0313\n",
            "Epoch [2/2], Step [39951/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [39951/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [40001/65307], Loss: 0.0800\n",
            "Epoch [2/2], Step [40001/65307], Loss: 0.0800\n",
            "Epoch [2/2], Step [40051/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [40051/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [40101/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [40101/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [40151/65307], Loss: 0.0538\n",
            "Epoch [2/2], Step [40151/65307], Loss: 0.0538\n",
            "Epoch [2/2], Step [40201/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [40201/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [40251/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [40251/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [40301/65307], Loss: 0.0369\n",
            "Epoch [2/2], Step [40301/65307], Loss: 0.0369\n",
            "Epoch [2/2], Step [40351/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [40351/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [40401/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [40401/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [40451/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [40451/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [40501/65307], Loss: 0.0787\n",
            "Epoch [2/2], Step [40501/65307], Loss: 0.0787\n",
            "Epoch [2/2], Step [40551/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [40551/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [40601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [40601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [40651/65307], Loss: 0.1458\n",
            "Epoch [2/2], Step [40651/65307], Loss: 0.1458\n",
            "Epoch [2/2], Step [40701/65307], Loss: 0.1862\n",
            "Epoch [2/2], Step [40701/65307], Loss: 0.1862\n",
            "Epoch [2/2], Step [40751/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [40751/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [40801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [40801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [40851/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [40851/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [40901/65307], Loss: 0.0172\n",
            "Epoch [2/2], Step [40901/65307], Loss: 0.0172\n",
            "Epoch [2/2], Step [40951/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [40951/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [41001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [41001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [41051/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [41051/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [41101/65307], Loss: 0.0552\n",
            "Epoch [2/2], Step [41101/65307], Loss: 0.0552\n",
            "Epoch [2/2], Step [41151/65307], Loss: 0.3584\n",
            "Epoch [2/2], Step [41151/65307], Loss: 0.3584\n",
            "Epoch [2/2], Step [41201/65307], Loss: 0.0870\n",
            "Epoch [2/2], Step [41201/65307], Loss: 0.0870\n",
            "Epoch [2/2], Step [41251/65307], Loss: 0.0766\n",
            "Epoch [2/2], Step [41251/65307], Loss: 0.0766\n",
            "Epoch [2/2], Step [41301/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [41301/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [41351/65307], Loss: 0.0811\n",
            "Epoch [2/2], Step [41351/65307], Loss: 0.0811\n",
            "Epoch [2/2], Step [41401/65307], Loss: 0.0254\n",
            "Epoch [2/2], Step [41401/65307], Loss: 0.0254\n",
            "Epoch [2/2], Step [41451/65307], Loss: 0.2074\n",
            "Epoch [2/2], Step [41451/65307], Loss: 0.2074\n",
            "Epoch [2/2], Step [41501/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [41501/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [41551/65307], Loss: 0.3000\n",
            "Epoch [2/2], Step [41551/65307], Loss: 0.3000\n",
            "Epoch [2/2], Step [41601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [41601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [41651/65307], Loss: 0.0158\n",
            "Epoch [2/2], Step [41651/65307], Loss: 0.0158\n",
            "Epoch [2/2], Step [41701/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [41701/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [41751/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [41751/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [41801/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [41801/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [41851/65307], Loss: 0.1453\n",
            "Epoch [2/2], Step [41851/65307], Loss: 0.1453\n",
            "Epoch [2/2], Step [41901/65307], Loss: 0.0511\n",
            "Epoch [2/2], Step [41901/65307], Loss: 0.0511\n",
            "Epoch [2/2], Step [41951/65307], Loss: 0.1081\n",
            "Epoch [2/2], Step [41951/65307], Loss: 0.1081\n",
            "Epoch [2/2], Step [42001/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [42001/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [42051/65307], Loss: 0.0390\n",
            "Epoch [2/2], Step [42051/65307], Loss: 0.0390\n",
            "Epoch [2/2], Step [42101/65307], Loss: 0.1650\n",
            "Epoch [2/2], Step [42101/65307], Loss: 0.1650\n",
            "Epoch [2/2], Step [42151/65307], Loss: 0.1683\n",
            "Epoch [2/2], Step [42151/65307], Loss: 0.1683\n",
            "Epoch [2/2], Step [42201/65307], Loss: 0.3551\n",
            "Epoch [2/2], Step [42201/65307], Loss: 0.3551\n",
            "Epoch [2/2], Step [42251/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [42251/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [42301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [42301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [42351/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [42351/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [42401/65307], Loss: 0.0936\n",
            "Epoch [2/2], Step [42401/65307], Loss: 0.0936\n",
            "Epoch [2/2], Step [42451/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [42451/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [42501/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [42501/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [42551/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [42551/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [42601/65307], Loss: 0.1201\n",
            "Epoch [2/2], Step [42601/65307], Loss: 0.1201\n",
            "Epoch [2/2], Step [42651/65307], Loss: 0.2641\n",
            "Epoch [2/2], Step [42651/65307], Loss: 0.2641\n",
            "Epoch [2/2], Step [42701/65307], Loss: 0.0685\n",
            "Epoch [2/2], Step [42701/65307], Loss: 0.0685\n",
            "Epoch [2/2], Step [42751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [42751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [42801/65307], Loss: 0.0176\n",
            "Epoch [2/2], Step [42801/65307], Loss: 0.0176\n",
            "Epoch [2/2], Step [42851/65307], Loss: 0.2481\n",
            "Epoch [2/2], Step [42851/65307], Loss: 0.2481\n",
            "Epoch [2/2], Step [42901/65307], Loss: 0.1785\n",
            "Epoch [2/2], Step [42901/65307], Loss: 0.1785\n",
            "Epoch [2/2], Step [42951/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [42951/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [43001/65307], Loss: 0.0078\n",
            "Epoch [2/2], Step [43001/65307], Loss: 0.0078\n",
            "Epoch [2/2], Step [43051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [43051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [43101/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43101/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43151/65307], Loss: 0.2728\n",
            "Epoch [2/2], Step [43151/65307], Loss: 0.2728\n",
            "Epoch [2/2], Step [43201/65307], Loss: 0.0930\n",
            "Epoch [2/2], Step [43201/65307], Loss: 0.0930\n",
            "Epoch [2/2], Step [43251/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43251/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43301/65307], Loss: 0.4954\n",
            "Epoch [2/2], Step [43301/65307], Loss: 0.4954\n",
            "Epoch [2/2], Step [43351/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [43351/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [43401/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [43401/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [43451/65307], Loss: 0.0710\n",
            "Epoch [2/2], Step [43451/65307], Loss: 0.0710\n",
            "Epoch [2/2], Step [43501/65307], Loss: 0.1013\n",
            "Epoch [2/2], Step [43501/65307], Loss: 0.1013\n",
            "Epoch [2/2], Step [43551/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [43551/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [43601/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [43601/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [43651/65307], Loss: 0.0112\n",
            "Epoch [2/2], Step [43651/65307], Loss: 0.0112\n",
            "Epoch [2/2], Step [43701/65307], Loss: 0.2337\n",
            "Epoch [2/2], Step [43701/65307], Loss: 0.2337\n",
            "Epoch [2/2], Step [43751/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [43751/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [43801/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [43801/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [43851/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [43851/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [43901/65307], Loss: 0.2230\n",
            "Epoch [2/2], Step [43901/65307], Loss: 0.2230\n",
            "Epoch [2/2], Step [43951/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [43951/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [44001/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [44001/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [44051/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [44051/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [44101/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [44101/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [44151/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [44151/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [44201/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [44201/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [44251/65307], Loss: 0.2709\n",
            "Epoch [2/2], Step [44251/65307], Loss: 0.2709\n",
            "Epoch [2/2], Step [44301/65307], Loss: 0.4022\n",
            "Epoch [2/2], Step [44301/65307], Loss: 0.4022\n",
            "Epoch [2/2], Step [44351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [44351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [44401/65307], Loss: 0.1215\n",
            "Epoch [2/2], Step [44401/65307], Loss: 0.1215\n",
            "Epoch [2/2], Step [44451/65307], Loss: 0.1006\n",
            "Epoch [2/2], Step [44451/65307], Loss: 0.1006\n",
            "Epoch [2/2], Step [44501/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [44501/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [44551/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [44551/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [44601/65307], Loss: 0.0100\n",
            "Epoch [2/2], Step [44601/65307], Loss: 0.0100\n",
            "Epoch [2/2], Step [44651/65307], Loss: 0.2645\n",
            "Epoch [2/2], Step [44651/65307], Loss: 0.2645\n",
            "Epoch [2/2], Step [44701/65307], Loss: 0.5550\n",
            "Epoch [2/2], Step [44701/65307], Loss: 0.5550\n",
            "Epoch [2/2], Step [44751/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [44751/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [44801/65307], Loss: 0.0522\n",
            "Epoch [2/2], Step [44801/65307], Loss: 0.0522\n",
            "Epoch [2/2], Step [44851/65307], Loss: 0.2464\n",
            "Epoch [2/2], Step [44851/65307], Loss: 0.2464\n",
            "Epoch [2/2], Step [44901/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [44901/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [44951/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [44951/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [45001/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [45001/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [45051/65307], Loss: 0.0765\n",
            "Epoch [2/2], Step [45051/65307], Loss: 0.0765\n",
            "Epoch [2/2], Step [45101/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [45101/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [45151/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [45151/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [45201/65307], Loss: 0.4553\n",
            "Epoch [2/2], Step [45201/65307], Loss: 0.4553\n",
            "Epoch [2/2], Step [45251/65307], Loss: 0.2395\n",
            "Epoch [2/2], Step [45251/65307], Loss: 0.2395\n",
            "Epoch [2/2], Step [45301/65307], Loss: 0.1239\n",
            "Epoch [2/2], Step [45301/65307], Loss: 0.1239\n",
            "Epoch [2/2], Step [45351/65307], Loss: 0.1291\n",
            "Epoch [2/2], Step [45351/65307], Loss: 0.1291\n",
            "Epoch [2/2], Step [45401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [45401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [45451/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [45451/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [45501/65307], Loss: 0.0774\n",
            "Epoch [2/2], Step [45501/65307], Loss: 0.0774\n",
            "Epoch [2/2], Step [45551/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [45551/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [45601/65307], Loss: 0.1254\n",
            "Epoch [2/2], Step [45601/65307], Loss: 0.1254\n",
            "Epoch [2/2], Step [45651/65307], Loss: 0.0910\n",
            "Epoch [2/2], Step [45651/65307], Loss: 0.0910\n",
            "Epoch [2/2], Step [45701/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [45701/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [45751/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [45751/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [45801/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [45801/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [45851/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [45851/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [45901/65307], Loss: 0.2027\n",
            "Epoch [2/2], Step [45901/65307], Loss: 0.2027\n",
            "Epoch [2/2], Step [45951/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [45951/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [46001/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [46001/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [46051/65307], Loss: 0.0116\n",
            "Epoch [2/2], Step [46051/65307], Loss: 0.0116\n",
            "Epoch [2/2], Step [46101/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [46101/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [46151/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [46151/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [46201/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [46201/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [46251/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [46251/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [46301/65307], Loss: 0.0867\n",
            "Epoch [2/2], Step [46301/65307], Loss: 0.0867\n",
            "Epoch [2/2], Step [46351/65307], Loss: 0.0640\n",
            "Epoch [2/2], Step [46351/65307], Loss: 0.0640\n",
            "Epoch [2/2], Step [46401/65307], Loss: 0.0884\n",
            "Epoch [2/2], Step [46401/65307], Loss: 0.0884\n",
            "Epoch [2/2], Step [46451/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [46451/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [46501/65307], Loss: 0.2494\n",
            "Epoch [2/2], Step [46501/65307], Loss: 0.2494\n",
            "Epoch [2/2], Step [46551/65307], Loss: 0.0568\n",
            "Epoch [2/2], Step [46551/65307], Loss: 0.0568\n",
            "Epoch [2/2], Step [46601/65307], Loss: 0.0998\n",
            "Epoch [2/2], Step [46601/65307], Loss: 0.0998\n",
            "Epoch [2/2], Step [46651/65307], Loss: 0.4177\n",
            "Epoch [2/2], Step [46651/65307], Loss: 0.4177\n",
            "Epoch [2/2], Step [46701/65307], Loss: 0.0615\n",
            "Epoch [2/2], Step [46701/65307], Loss: 0.0615\n",
            "Epoch [2/2], Step [46751/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [46751/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [46801/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [46801/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [46851/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [46851/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [46901/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [46901/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [46951/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [46951/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [47001/65307], Loss: 0.1865\n",
            "Epoch [2/2], Step [47001/65307], Loss: 0.1865\n",
            "Epoch [2/2], Step [47051/65307], Loss: 0.0759\n",
            "Epoch [2/2], Step [47051/65307], Loss: 0.0759\n",
            "Epoch [2/2], Step [47101/65307], Loss: 0.1173\n",
            "Epoch [2/2], Step [47101/65307], Loss: 0.1173\n",
            "Epoch [2/2], Step [47151/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [47151/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [47201/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [47201/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [47251/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [47251/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [47301/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [47301/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [47351/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [47351/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [47401/65307], Loss: 0.0208\n",
            "Epoch [2/2], Step [47401/65307], Loss: 0.0208\n",
            "Epoch [2/2], Step [47451/65307], Loss: 0.0926\n",
            "Epoch [2/2], Step [47451/65307], Loss: 0.0926\n",
            "Epoch [2/2], Step [47501/65307], Loss: 0.0636\n",
            "Epoch [2/2], Step [47501/65307], Loss: 0.0636\n",
            "Epoch [2/2], Step [47551/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [47551/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [47601/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [47601/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [47651/65307], Loss: 0.1190\n",
            "Epoch [2/2], Step [47651/65307], Loss: 0.1190\n",
            "Epoch [2/2], Step [47701/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [47701/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [47751/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [47751/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [47801/65307], Loss: 0.5097\n",
            "Epoch [2/2], Step [47801/65307], Loss: 0.5097\n",
            "Epoch [2/2], Step [47851/65307], Loss: 0.0334\n",
            "Epoch [2/2], Step [47851/65307], Loss: 0.0334\n",
            "Epoch [2/2], Step [47901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [47901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [47951/65307], Loss: 0.3728\n",
            "Epoch [2/2], Step [47951/65307], Loss: 0.3728\n",
            "Epoch [2/2], Step [48001/65307], Loss: 0.0483\n",
            "Epoch [2/2], Step [48001/65307], Loss: 0.0483\n",
            "Epoch [2/2], Step [48051/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [48051/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [48101/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [48101/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [48151/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48151/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48201/65307], Loss: 0.1059\n",
            "Epoch [2/2], Step [48201/65307], Loss: 0.1059\n",
            "Epoch [2/2], Step [48251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48301/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [48301/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [48351/65307], Loss: 0.1137\n",
            "Epoch [2/2], Step [48351/65307], Loss: 0.1137\n",
            "Epoch [2/2], Step [48401/65307], Loss: 0.0719\n",
            "Epoch [2/2], Step [48401/65307], Loss: 0.0719\n",
            "Epoch [2/2], Step [48451/65307], Loss: 0.0478\n",
            "Epoch [2/2], Step [48451/65307], Loss: 0.0478\n",
            "Epoch [2/2], Step [48501/65307], Loss: 0.0099\n",
            "Epoch [2/2], Step [48501/65307], Loss: 0.0099\n",
            "Epoch [2/2], Step [48551/65307], Loss: 0.2995\n",
            "Epoch [2/2], Step [48551/65307], Loss: 0.2995\n",
            "Epoch [2/2], Step [48601/65307], Loss: 0.0734\n",
            "Epoch [2/2], Step [48601/65307], Loss: 0.0734\n",
            "Epoch [2/2], Step [48651/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [48651/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [48701/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [48701/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [48751/65307], Loss: 0.0141\n",
            "Epoch [2/2], Step [48751/65307], Loss: 0.0141\n",
            "Epoch [2/2], Step [48801/65307], Loss: 0.1248\n",
            "Epoch [2/2], Step [48801/65307], Loss: 0.1248\n",
            "Epoch [2/2], Step [48851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [48851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [48901/65307], Loss: 0.0091\n",
            "Epoch [2/2], Step [48901/65307], Loss: 0.0091\n",
            "Epoch [2/2], Step [48951/65307], Loss: 0.1681\n",
            "Epoch [2/2], Step [48951/65307], Loss: 0.1681\n",
            "Epoch [2/2], Step [49001/65307], Loss: 0.0350\n",
            "Epoch [2/2], Step [49001/65307], Loss: 0.0350\n",
            "Epoch [2/2], Step [49051/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [49051/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [49101/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [49101/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [49151/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [49151/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [49201/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [49201/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [49251/65307], Loss: 0.0707\n",
            "Epoch [2/2], Step [49251/65307], Loss: 0.0707\n",
            "Epoch [2/2], Step [49301/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [49301/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [49351/65307], Loss: 0.0460\n",
            "Epoch [2/2], Step [49351/65307], Loss: 0.0460\n",
            "Epoch [2/2], Step [49401/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [49401/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [49451/65307], Loss: 0.2863\n",
            "Epoch [2/2], Step [49451/65307], Loss: 0.2863\n",
            "Epoch [2/2], Step [49501/65307], Loss: 0.1341\n",
            "Epoch [2/2], Step [49501/65307], Loss: 0.1341\n",
            "Epoch [2/2], Step [49551/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [49551/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [49601/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [49601/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [49651/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [49651/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [49701/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [49701/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [49751/65307], Loss: 0.0877\n",
            "Epoch [2/2], Step [49751/65307], Loss: 0.0877\n",
            "Epoch [2/2], Step [49801/65307], Loss: 0.1120\n",
            "Epoch [2/2], Step [49801/65307], Loss: 0.1120\n",
            "Epoch [2/2], Step [49851/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [49851/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [49901/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [49901/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [49951/65307], Loss: 0.0453\n",
            "Epoch [2/2], Step [49951/65307], Loss: 0.0453\n",
            "Epoch [2/2], Step [50001/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [50001/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [50051/65307], Loss: 0.1012\n",
            "Epoch [2/2], Step [50051/65307], Loss: 0.1012\n",
            "Epoch [2/2], Step [50101/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [50101/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [50151/65307], Loss: 0.0282\n",
            "Epoch [2/2], Step [50151/65307], Loss: 0.0282\n",
            "Epoch [2/2], Step [50201/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [50201/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [50251/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [50251/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [50301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [50301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [50351/65307], Loss: 0.0913\n",
            "Epoch [2/2], Step [50351/65307], Loss: 0.0913\n",
            "Epoch [2/2], Step [50401/65307], Loss: 0.0586\n",
            "Epoch [2/2], Step [50401/65307], Loss: 0.0586\n",
            "Epoch [2/2], Step [50451/65307], Loss: 0.0767\n",
            "Epoch [2/2], Step [50451/65307], Loss: 0.0767\n",
            "Epoch [2/2], Step [50501/65307], Loss: 0.1093\n",
            "Epoch [2/2], Step [50501/65307], Loss: 0.1093\n",
            "Epoch [2/2], Step [50551/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [50551/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [50601/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [50601/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [50651/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [50651/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [50701/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [50701/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [50751/65307], Loss: 0.0385\n",
            "Epoch [2/2], Step [50751/65307], Loss: 0.0385\n",
            "Epoch [2/2], Step [50801/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [50801/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [50851/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [50851/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [50901/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [50901/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [50951/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [50951/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [51001/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [51001/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [51051/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [51051/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [51101/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [51101/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [51151/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [51151/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [51201/65307], Loss: 0.0306\n",
            "Epoch [2/2], Step [51201/65307], Loss: 0.0306\n",
            "Epoch [2/2], Step [51251/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [51251/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [51301/65307], Loss: 0.0499\n",
            "Epoch [2/2], Step [51301/65307], Loss: 0.0499\n",
            "Epoch [2/2], Step [51351/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [51351/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [51401/65307], Loss: 0.0652\n",
            "Epoch [2/2], Step [51401/65307], Loss: 0.0652\n",
            "Epoch [2/2], Step [51451/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [51451/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [51501/65307], Loss: 0.0452\n",
            "Epoch [2/2], Step [51501/65307], Loss: 0.0452\n",
            "Epoch [2/2], Step [51551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [51551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [51601/65307], Loss: 0.0537\n",
            "Epoch [2/2], Step [51601/65307], Loss: 0.0537\n",
            "Epoch [2/2], Step [51651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [51651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [51701/65307], Loss: 0.0749\n",
            "Epoch [2/2], Step [51701/65307], Loss: 0.0749\n",
            "Epoch [2/2], Step [51751/65307], Loss: 0.0009\n",
            "Epoch [2/2], Step [51751/65307], Loss: 0.0009\n",
            "Epoch [2/2], Step [51801/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [51801/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [51851/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [51851/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [51901/65307], Loss: 0.2776\n",
            "Epoch [2/2], Step [51901/65307], Loss: 0.2776\n",
            "Epoch [2/2], Step [51951/65307], Loss: 0.1714\n",
            "Epoch [2/2], Step [51951/65307], Loss: 0.1714\n",
            "Epoch [2/2], Step [52001/65307], Loss: 0.1529\n",
            "Epoch [2/2], Step [52001/65307], Loss: 0.1529\n",
            "Epoch [2/2], Step [52051/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [52051/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [52101/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [52101/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [52151/65307], Loss: 0.0069\n",
            "Epoch [2/2], Step [52151/65307], Loss: 0.0069\n",
            "Epoch [2/2], Step [52201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [52201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [52251/65307], Loss: 0.1896\n",
            "Epoch [2/2], Step [52251/65307], Loss: 0.1896\n",
            "Epoch [2/2], Step [52301/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [52301/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [52351/65307], Loss: 0.0155\n",
            "Epoch [2/2], Step [52351/65307], Loss: 0.0155\n",
            "Epoch [2/2], Step [52401/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [52401/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [52451/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [52451/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [52501/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [52501/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [52551/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [52551/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [52601/65307], Loss: 0.0745\n",
            "Epoch [2/2], Step [52601/65307], Loss: 0.0745\n",
            "Epoch [2/2], Step [52651/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [52651/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [52701/65307], Loss: 0.1756\n",
            "Epoch [2/2], Step [52701/65307], Loss: 0.1756\n",
            "Epoch [2/2], Step [52751/65307], Loss: 0.0479\n",
            "Epoch [2/2], Step [52751/65307], Loss: 0.0479\n",
            "Epoch [2/2], Step [52801/65307], Loss: 0.0541\n",
            "Epoch [2/2], Step [52801/65307], Loss: 0.0541\n",
            "Epoch [2/2], Step [52851/65307], Loss: 0.1304\n",
            "Epoch [2/2], Step [52851/65307], Loss: 0.1304\n",
            "Epoch [2/2], Step [52901/65307], Loss: 0.4685\n",
            "Epoch [2/2], Step [52901/65307], Loss: 0.4685\n",
            "Epoch [2/2], Step [52951/65307], Loss: 0.1106\n",
            "Epoch [2/2], Step [52951/65307], Loss: 0.1106\n",
            "Epoch [2/2], Step [53001/65307], Loss: 0.1432\n",
            "Epoch [2/2], Step [53001/65307], Loss: 0.1432\n",
            "Epoch [2/2], Step [53051/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [53051/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [53101/65307], Loss: 0.2140\n",
            "Epoch [2/2], Step [53101/65307], Loss: 0.2140\n",
            "Epoch [2/2], Step [53151/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [53151/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [53201/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [53201/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [53251/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [53251/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [53301/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [53301/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [53351/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [53351/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [53401/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [53401/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [53451/65307], Loss: 0.0124\n",
            "Epoch [2/2], Step [53451/65307], Loss: 0.0124\n",
            "Epoch [2/2], Step [53501/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [53501/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [53551/65307], Loss: 0.1112\n",
            "Epoch [2/2], Step [53551/65307], Loss: 0.1112\n",
            "Epoch [2/2], Step [53601/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [53601/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [53651/65307], Loss: 0.0748\n",
            "Epoch [2/2], Step [53651/65307], Loss: 0.0748\n",
            "Epoch [2/2], Step [53701/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [53701/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [53751/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [53751/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [53801/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [53801/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [53851/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [53851/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [53901/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [53901/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [53951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [53951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [54001/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [54001/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [54051/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [54051/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [54101/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54101/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54151/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [54151/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [54201/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [54201/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [54251/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [54251/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [54301/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [54301/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [54351/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [54351/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [54401/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [54401/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [54451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [54451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [54501/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [54501/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [54551/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [54551/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [54601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [54601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [54651/65307], Loss: 0.1306\n",
            "Epoch [2/2], Step [54651/65307], Loss: 0.1306\n",
            "Epoch [2/2], Step [54701/65307], Loss: 0.0971\n",
            "Epoch [2/2], Step [54701/65307], Loss: 0.0971\n",
            "Epoch [2/2], Step [54751/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54751/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54801/65307], Loss: 0.1350\n",
            "Epoch [2/2], Step [54801/65307], Loss: 0.1350\n",
            "Epoch [2/2], Step [54851/65307], Loss: 0.0125\n",
            "Epoch [2/2], Step [54851/65307], Loss: 0.0125\n",
            "Epoch [2/2], Step [54901/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [54901/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [54951/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [54951/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [55001/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [55001/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [55051/65307], Loss: 0.2673\n",
            "Epoch [2/2], Step [55051/65307], Loss: 0.2673\n",
            "Epoch [2/2], Step [55101/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [55101/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [55151/65307], Loss: 0.0893\n",
            "Epoch [2/2], Step [55151/65307], Loss: 0.0893\n",
            "Epoch [2/2], Step [55201/65307], Loss: 0.1347\n",
            "Epoch [2/2], Step [55201/65307], Loss: 0.1347\n",
            "Epoch [2/2], Step [55251/65307], Loss: 0.0070\n",
            "Epoch [2/2], Step [55251/65307], Loss: 0.0070\n",
            "Epoch [2/2], Step [55301/65307], Loss: 0.0933\n",
            "Epoch [2/2], Step [55301/65307], Loss: 0.0933\n",
            "Epoch [2/2], Step [55351/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [55351/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [55401/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [55401/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [55451/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [55451/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [55501/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [55501/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [55551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [55551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [55601/65307], Loss: 0.1037\n",
            "Epoch [2/2], Step [55601/65307], Loss: 0.1037\n",
            "Epoch [2/2], Step [55651/65307], Loss: 0.1508\n",
            "Epoch [2/2], Step [55651/65307], Loss: 0.1508\n",
            "Epoch [2/2], Step [55701/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [55701/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [55751/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [55751/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [55801/65307], Loss: 0.0621\n",
            "Epoch [2/2], Step [55801/65307], Loss: 0.0621\n",
            "Epoch [2/2], Step [55851/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [55851/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [55901/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [55901/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [55951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [55951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [56001/65307], Loss: 0.3520\n",
            "Epoch [2/2], Step [56001/65307], Loss: 0.3520\n",
            "Epoch [2/2], Step [56051/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [56051/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [56101/65307], Loss: 0.1770\n",
            "Epoch [2/2], Step [56101/65307], Loss: 0.1770\n",
            "Epoch [2/2], Step [56151/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [56151/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [56201/65307], Loss: 0.1687\n",
            "Epoch [2/2], Step [56201/65307], Loss: 0.1687\n",
            "Epoch [2/2], Step [56251/65307], Loss: 0.2714\n",
            "Epoch [2/2], Step [56251/65307], Loss: 0.2714\n",
            "Epoch [2/2], Step [56301/65307], Loss: 0.0814\n",
            "Epoch [2/2], Step [56301/65307], Loss: 0.0814\n",
            "Epoch [2/2], Step [56351/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [56351/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [56401/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [56401/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [56451/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [56451/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [56501/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [56501/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [56551/65307], Loss: 0.1685\n",
            "Epoch [2/2], Step [56551/65307], Loss: 0.1685\n",
            "Epoch [2/2], Step [56601/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [56601/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [56651/65307], Loss: 0.0751\n",
            "Epoch [2/2], Step [56651/65307], Loss: 0.0751\n",
            "Epoch [2/2], Step [56701/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [56701/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [56751/65307], Loss: 0.0761\n",
            "Epoch [2/2], Step [56751/65307], Loss: 0.0761\n",
            "Epoch [2/2], Step [56801/65307], Loss: 0.0699\n",
            "Epoch [2/2], Step [56801/65307], Loss: 0.0699\n",
            "Epoch [2/2], Step [56851/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [56851/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [56901/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [56901/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [56951/65307], Loss: 0.1647\n",
            "Epoch [2/2], Step [56951/65307], Loss: 0.1647\n",
            "Epoch [2/2], Step [57001/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [57001/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [57051/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [57051/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [57101/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [57101/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [57151/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [57151/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [57201/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [57201/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [57251/65307], Loss: 0.1115\n",
            "Epoch [2/2], Step [57251/65307], Loss: 0.1115\n",
            "Epoch [2/2], Step [57301/65307], Loss: 0.1591\n",
            "Epoch [2/2], Step [57301/65307], Loss: 0.1591\n",
            "Epoch [2/2], Step [57351/65307], Loss: 0.0666\n",
            "Epoch [2/2], Step [57351/65307], Loss: 0.0666\n",
            "Epoch [2/2], Step [57401/65307], Loss: 0.2468\n",
            "Epoch [2/2], Step [57401/65307], Loss: 0.2468\n",
            "Epoch [2/2], Step [57451/65307], Loss: 0.1850\n",
            "Epoch [2/2], Step [57451/65307], Loss: 0.1850\n",
            "Epoch [2/2], Step [57501/65307], Loss: 0.0909\n",
            "Epoch [2/2], Step [57501/65307], Loss: 0.0909\n",
            "Epoch [2/2], Step [57551/65307], Loss: 0.0240\n",
            "Epoch [2/2], Step [57551/65307], Loss: 0.0240\n",
            "Epoch [2/2], Step [57601/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [57601/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [57651/65307], Loss: 0.0060\n",
            "Epoch [2/2], Step [57651/65307], Loss: 0.0060\n",
            "Epoch [2/2], Step [57701/65307], Loss: 0.3056\n",
            "Epoch [2/2], Step [57701/65307], Loss: 0.3056\n",
            "Epoch [2/2], Step [57751/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [57751/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [57801/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [57801/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [57851/65307], Loss: 0.0216\n",
            "Epoch [2/2], Step [57851/65307], Loss: 0.0216\n",
            "Epoch [2/2], Step [57901/65307], Loss: 0.0695\n",
            "Epoch [2/2], Step [57901/65307], Loss: 0.0695\n",
            "Epoch [2/2], Step [57951/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [57951/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [58001/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [58001/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [58051/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [58051/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [58101/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [58101/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [58151/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [58151/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [58201/65307], Loss: 0.2215\n",
            "Epoch [2/2], Step [58201/65307], Loss: 0.2215\n",
            "Epoch [2/2], Step [58251/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [58251/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [58301/65307], Loss: 0.2284\n",
            "Epoch [2/2], Step [58301/65307], Loss: 0.2284\n",
            "Epoch [2/2], Step [58351/65307], Loss: 0.0209\n",
            "Epoch [2/2], Step [58351/65307], Loss: 0.0209\n",
            "Epoch [2/2], Step [58401/65307], Loss: 0.0706\n",
            "Epoch [2/2], Step [58401/65307], Loss: 0.0706\n",
            "Epoch [2/2], Step [58451/65307], Loss: 0.1290\n",
            "Epoch [2/2], Step [58451/65307], Loss: 0.1290\n",
            "Epoch [2/2], Step [58501/65307], Loss: 0.3585\n",
            "Epoch [2/2], Step [58501/65307], Loss: 0.3585\n",
            "Epoch [2/2], Step [58551/65307], Loss: 0.0628\n",
            "Epoch [2/2], Step [58551/65307], Loss: 0.0628\n",
            "Epoch [2/2], Step [58601/65307], Loss: 0.1561\n",
            "Epoch [2/2], Step [58601/65307], Loss: 0.1561\n",
            "Epoch [2/2], Step [58651/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [58651/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [58701/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [58701/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [58751/65307], Loss: 0.1134\n",
            "Epoch [2/2], Step [58751/65307], Loss: 0.1134\n",
            "Epoch [2/2], Step [58801/65307], Loss: 0.1210\n",
            "Epoch [2/2], Step [58801/65307], Loss: 0.1210\n",
            "Epoch [2/2], Step [58851/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [58851/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [58901/65307], Loss: 0.1753\n",
            "Epoch [2/2], Step [58901/65307], Loss: 0.1753\n",
            "Epoch [2/2], Step [58951/65307], Loss: 0.0400\n",
            "Epoch [2/2], Step [58951/65307], Loss: 0.0400\n",
            "Epoch [2/2], Step [59001/65307], Loss: 0.0225\n",
            "Epoch [2/2], Step [59001/65307], Loss: 0.0225\n",
            "Epoch [2/2], Step [59051/65307], Loss: 0.1408\n",
            "Epoch [2/2], Step [59051/65307], Loss: 0.1408\n",
            "Epoch [2/2], Step [59101/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [59101/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [59151/65307], Loss: 0.0504\n",
            "Epoch [2/2], Step [59151/65307], Loss: 0.0504\n",
            "Epoch [2/2], Step [59201/65307], Loss: 0.0987\n",
            "Epoch [2/2], Step [59201/65307], Loss: 0.0987\n",
            "Epoch [2/2], Step [59251/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [59251/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [59301/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [59301/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [59351/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [59351/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [59401/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [59401/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [59451/65307], Loss: 0.0731\n",
            "Epoch [2/2], Step [59451/65307], Loss: 0.0731\n",
            "Epoch [2/2], Step [59501/65307], Loss: 0.0122\n",
            "Epoch [2/2], Step [59501/65307], Loss: 0.0122\n",
            "Epoch [2/2], Step [59551/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [59551/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [59601/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [59601/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [59651/65307], Loss: 0.0465\n",
            "Epoch [2/2], Step [59651/65307], Loss: 0.0465\n",
            "Epoch [2/2], Step [59701/65307], Loss: 0.0965\n",
            "Epoch [2/2], Step [59701/65307], Loss: 0.0965\n",
            "Epoch [2/2], Step [59751/65307], Loss: 0.0115\n",
            "Epoch [2/2], Step [59751/65307], Loss: 0.0115\n",
            "Epoch [2/2], Step [59801/65307], Loss: 0.0265\n",
            "Epoch [2/2], Step [59801/65307], Loss: 0.0265\n",
            "Epoch [2/2], Step [59851/65307], Loss: 0.1367\n",
            "Epoch [2/2], Step [59851/65307], Loss: 0.1367\n",
            "Epoch [2/2], Step [59901/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [59901/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [59951/65307], Loss: 0.1861\n",
            "Epoch [2/2], Step [59951/65307], Loss: 0.1861\n",
            "Epoch [2/2], Step [60001/65307], Loss: 0.0351\n",
            "Epoch [2/2], Step [60001/65307], Loss: 0.0351\n",
            "Epoch [2/2], Step [60051/65307], Loss: 0.1797\n",
            "Epoch [2/2], Step [60051/65307], Loss: 0.1797\n",
            "Epoch [2/2], Step [60101/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [60101/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [60151/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [60151/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [60201/65307], Loss: 0.1154\n",
            "Epoch [2/2], Step [60201/65307], Loss: 0.1154\n",
            "Epoch [2/2], Step [60251/65307], Loss: 0.1418\n",
            "Epoch [2/2], Step [60251/65307], Loss: 0.1418\n",
            "Epoch [2/2], Step [60301/65307], Loss: 0.1571\n",
            "Epoch [2/2], Step [60301/65307], Loss: 0.1571\n",
            "Epoch [2/2], Step [60351/65307], Loss: 0.0179\n",
            "Epoch [2/2], Step [60351/65307], Loss: 0.0179\n",
            "Epoch [2/2], Step [60401/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [60401/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [60451/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [60451/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [60501/65307], Loss: 0.0988\n",
            "Epoch [2/2], Step [60501/65307], Loss: 0.0988\n",
            "Epoch [2/2], Step [60551/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [60551/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [60601/65307], Loss: 0.1192\n",
            "Epoch [2/2], Step [60601/65307], Loss: 0.1192\n",
            "Epoch [2/2], Step [60651/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [60651/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [60701/65307], Loss: 0.5129\n",
            "Epoch [2/2], Step [60701/65307], Loss: 0.5129\n",
            "Epoch [2/2], Step [60751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [60751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [60801/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [60801/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [60851/65307], Loss: 0.0623\n",
            "Epoch [2/2], Step [60851/65307], Loss: 0.0623\n",
            "Epoch [2/2], Step [60901/65307], Loss: 0.0076\n",
            "Epoch [2/2], Step [60901/65307], Loss: 0.0076\n",
            "Epoch [2/2], Step [60951/65307], Loss: 0.0382\n",
            "Epoch [2/2], Step [60951/65307], Loss: 0.0382\n",
            "Epoch [2/2], Step [61001/65307], Loss: 0.1080\n",
            "Epoch [2/2], Step [61001/65307], Loss: 0.1080\n",
            "Epoch [2/2], Step [61051/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [61051/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [61101/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [61101/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [61151/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [61151/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [61201/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [61201/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [61251/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [61251/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [61301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61351/65307], Loss: 0.2706\n",
            "Epoch [2/2], Step [61351/65307], Loss: 0.2706\n",
            "Epoch [2/2], Step [61401/65307], Loss: 0.0959\n",
            "Epoch [2/2], Step [61401/65307], Loss: 0.0959\n",
            "Epoch [2/2], Step [61451/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [61451/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [61501/65307], Loss: 0.1230\n",
            "Epoch [2/2], Step [61501/65307], Loss: 0.1230\n",
            "Epoch [2/2], Step [61551/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [61551/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [61601/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61601/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61651/65307], Loss: 0.1393\n",
            "Epoch [2/2], Step [61651/65307], Loss: 0.1393\n",
            "Epoch [2/2], Step [61701/65307], Loss: 0.0468\n",
            "Epoch [2/2], Step [61701/65307], Loss: 0.0468\n",
            "Epoch [2/2], Step [61751/65307], Loss: 0.1195\n",
            "Epoch [2/2], Step [61751/65307], Loss: 0.1195\n",
            "Epoch [2/2], Step [61801/65307], Loss: 0.0273\n",
            "Epoch [2/2], Step [61801/65307], Loss: 0.0273\n",
            "Epoch [2/2], Step [61851/65307], Loss: 0.1386\n",
            "Epoch [2/2], Step [61851/65307], Loss: 0.1386\n",
            "Epoch [2/2], Step [61901/65307], Loss: 0.1321\n",
            "Epoch [2/2], Step [61901/65307], Loss: 0.1321\n",
            "Epoch [2/2], Step [61951/65307], Loss: 0.1479\n",
            "Epoch [2/2], Step [61951/65307], Loss: 0.1479\n",
            "Epoch [2/2], Step [62001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [62001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [62051/65307], Loss: 0.3153\n",
            "Epoch [2/2], Step [62051/65307], Loss: 0.3153\n",
            "Epoch [2/2], Step [62101/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [62101/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [62151/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [62151/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [62201/65307], Loss: 0.1534\n",
            "Epoch [2/2], Step [62201/65307], Loss: 0.1534\n",
            "Epoch [2/2], Step [62251/65307], Loss: 0.0307\n",
            "Epoch [2/2], Step [62251/65307], Loss: 0.0307\n",
            "Epoch [2/2], Step [62301/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [62301/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [62351/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62351/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62401/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [62401/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [62451/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [62451/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [62501/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62501/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62551/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [62551/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [62601/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [62601/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [62651/65307], Loss: 0.1157\n",
            "Epoch [2/2], Step [62651/65307], Loss: 0.1157\n",
            "Epoch [2/2], Step [62701/65307], Loss: 0.0600\n",
            "Epoch [2/2], Step [62701/65307], Loss: 0.0600\n",
            "Epoch [2/2], Step [62751/65307], Loss: 0.1700\n",
            "Epoch [2/2], Step [62751/65307], Loss: 0.1700\n",
            "Epoch [2/2], Step [62801/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [62801/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [62851/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [62851/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [62901/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [62901/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [62951/65307], Loss: 0.1566\n",
            "Epoch [2/2], Step [62951/65307], Loss: 0.1566\n",
            "Epoch [2/2], Step [63001/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [63001/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [63051/65307], Loss: 0.0799\n",
            "Epoch [2/2], Step [63051/65307], Loss: 0.0799\n",
            "Epoch [2/2], Step [63101/65307], Loss: 0.0519\n",
            "Epoch [2/2], Step [63101/65307], Loss: 0.0519\n",
            "Epoch [2/2], Step [63151/65307], Loss: 0.0290\n",
            "Epoch [2/2], Step [63151/65307], Loss: 0.0290\n",
            "Epoch [2/2], Step [63201/65307], Loss: 0.0512\n",
            "Epoch [2/2], Step [63201/65307], Loss: 0.0512\n",
            "Epoch [2/2], Step [63251/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63251/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63301/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [63301/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [63351/65307], Loss: 0.1181\n",
            "Epoch [2/2], Step [63351/65307], Loss: 0.1181\n",
            "Epoch [2/2], Step [63401/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [63401/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [63451/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [63451/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [63501/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [63501/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [63551/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [63551/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [63601/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [63601/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [63651/65307], Loss: 0.0293\n",
            "Epoch [2/2], Step [63651/65307], Loss: 0.0293\n",
            "Epoch [2/2], Step [63701/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [63701/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [63751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63801/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [63801/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [63851/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [63851/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [63901/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [63901/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [63951/65307], Loss: 0.2522\n",
            "Epoch [2/2], Step [63951/65307], Loss: 0.2522\n",
            "Epoch [2/2], Step [64001/65307], Loss: 0.1856\n",
            "Epoch [2/2], Step [64001/65307], Loss: 0.1856\n",
            "Epoch [2/2], Step [64051/65307], Loss: 0.0246\n",
            "Epoch [2/2], Step [64051/65307], Loss: 0.0246\n",
            "Epoch [2/2], Step [64101/65307], Loss: 0.1073\n",
            "Epoch [2/2], Step [64101/65307], Loss: 0.1073\n",
            "Epoch [2/2], Step [64151/65307], Loss: 0.1460\n",
            "Epoch [2/2], Step [64151/65307], Loss: 0.1460\n",
            "Epoch [2/2], Step [64201/65307], Loss: 0.0204\n",
            "Epoch [2/2], Step [64201/65307], Loss: 0.0204\n",
            "Epoch [2/2], Step [64251/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [64251/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [64301/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [64301/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [64351/65307], Loss: 0.0210\n",
            "Epoch [2/2], Step [64351/65307], Loss: 0.0210\n",
            "Epoch [2/2], Step [64401/65307], Loss: 0.1063\n",
            "Epoch [2/2], Step [64401/65307], Loss: 0.1063\n",
            "Epoch [2/2], Step [64451/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [64451/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [64501/65307], Loss: 0.1228\n",
            "Epoch [2/2], Step [64501/65307], Loss: 0.1228\n",
            "Epoch [2/2], Step [64551/65307], Loss: 0.0887\n",
            "Epoch [2/2], Step [64551/65307], Loss: 0.0887\n",
            "Epoch [2/2], Step [64601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [64601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [64651/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [64651/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [64701/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [64701/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [64751/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [64751/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [64801/65307], Loss: 0.1936\n",
            "Epoch [2/2], Step [64801/65307], Loss: 0.1936\n",
            "Epoch [2/2], Step [64851/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [64851/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [64901/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [64901/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [64951/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [64951/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [65001/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [65001/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [65051/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [65051/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [65101/65307], Loss: 0.0920\n",
            "Epoch [2/2], Step [65101/65307], Loss: 0.0920\n",
            "Epoch [2/2], Step [65151/65307], Loss: 0.0680\n",
            "Epoch [2/2], Step [65151/65307], Loss: 0.0680\n",
            "Epoch [2/2], Step [65201/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [65201/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [65251/65307], Loss: 0.0244\n",
            "Epoch [2/2], Step [65251/65307], Loss: 0.0244\n",
            "Epoch [2/2], Step [65301/65307], Loss: 0.1451\n",
            "Epoch [2/2], Step [65301/65307], Loss: 0.1451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpgWKiwkHPaK"
      },
      "source": [
        "The above output has some duplicates because there is a socketIO error from my browser during runtime, but there is nothing wrong with the model that runs on the kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-yUQn5R5yO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3259d8c8-8e57-43e5-8dab-81832a4a303e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss_set)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('every 50 iterations')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-o-9Is3AhLt"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7naz9vzBYI0"
      },
      "source": [
        "del train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8SPUxkPBsOv"
      },
      "source": [
        "del train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyH9oexYByyj"
      },
      "source": [
        "del train_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpYTSySzCN3T"
      },
      "source": [
        "del train_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDu7IkYDDDB"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztqfa6mE_f9G"
      },
      "source": [
        "total_labels=[]\n",
        "total_predictions=[]\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      # print (outputs)\n",
        "      print(i)\n",
        "      prediction = torch.argmax(outputs[0],dim=1)\n",
        "      total_labels.append(b_labels)\n",
        "      total_predictions.append(prediction)\n",
        "      total += b_labels.size(0)\n",
        "      correct+=(prediction==b_labels).sum().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCsZaKtb_q6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec975325-0bc6-44b7-98e3-3f05f2e36a3a"
      },
      "source": [
        "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on val data is: 96.17188247679204 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HprWRhuXeD2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8639c8ab-5d4f-4eb6-cb29-e1971d960f3b"
      },
      "source": [
        "len(total_labels)==len(total_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB0kEgq2gv1F"
      },
      "source": [
        "total_labels=[x.tolist() for x in total_labels]\n",
        "total_predictions=[x.tolist() for x in total_predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlywfbeWg3mv"
      },
      "source": [
        "total_labels=list(np.concatenate(total_labels).flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFtFvaOOhJir"
      },
      "source": [
        "total_predictions=list(np.concatenate(total_predictions).flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQm81XJyDH-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248de4b2-16c8-438c-ba74-50b2a54153fe"
      },
      "source": [
        "f1_score(total_labels, total_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6933455995093529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvqFpaBMu39"
      },
      "source": [
        "Final bert score using entire training data"
      ]
    }
  ]
}
