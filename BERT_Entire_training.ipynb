{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_Exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtep92FewPO-"
      },
      "source": [
        "This code deletes objects that are no longer needed in order to solve memory problem and run BERT on all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb6rjYgefJaW"
      },
      "source": [
        "print(\"delete later\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v59BLuXOh4vw",
        "outputId": "2e3a10c0-99b3-4176-c02b-303ae43eab14"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpkAFhseK5c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel,BertForSequenceClassification,AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgtsPHTLPKrk",
        "outputId": "78d22728-0ee0-44e4-9378-65cc49c0274f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFcfpeuobYD"
      },
      "source": [
        "train_data=pd.read_csv(\"/content/drive/Shareddrives/CS 247 project/data/train.csv\")\n",
        "#test_data=pd.read_csv(\"/content/drive/Shareddrives/CS 247 project/data/test.csv\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPVdQDJtuH-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b0260d-3a26-4aa7-ff34-0dda1fc3d32f"
      },
      "source": [
        "\n",
        "print(len(train_data))\n",
        "# train_data = train_data[:100000]\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1306122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZV-xpqjiDpX"
      },
      "source": [
        "train_data['question_text']=train_data['question_text'].str.lower()\n",
        "sentences=train_data.question_text.values\n",
        "# sentences=list(map(remove_stop_words, sentences))\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_data.target.values\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb5hdjxnuRQL"
      },
      "source": [
        "del train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjpQAvH4f3ZT"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrnlm9OCxRbm",
        "outputId": "d376ccd5-51c6-41c9-b7cc-40f8c126f074"
      },
      "source": [
        "print(tokenized_texts[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['[CLS]', 'how', 'did', 'quebec', 'nationalists', 'see', 'their', 'province', 'as', 'a', 'nation', 'in', 'the', '1960s', '?', '[SEP]'], ['[CLS]', 'do', 'you', 'have', 'an', 'adopted', 'dog', ',', 'how', 'would', 'you', 'encourage', 'people', 'to', 'adopt', 'and', 'not', 'shop', '?', '[SEP]'], ['[CLS]', 'why', 'does', 'velocity', 'affect', 'time', '?', 'does', 'velocity', 'affect', 'space', 'geometry', '?', '[SEP]'], ['[CLS]', 'how', 'did', 'otto', 'von', 'gu', '##eric', '##ke', 'used', 'the', 'mag', '##de', '##burg', 'hemisphere', '##s', '?', '[SEP]'], ['[CLS]', 'can', 'i', 'convert', 'mont', '##ra', 'he', '##lic', '##on', 'd', 'to', 'a', 'mountain', 'bike', 'by', 'just', 'changing', 'the', 'tyres', '?', '[SEP]'], ['[CLS]', 'is', 'gaza', 'slowly', 'becoming', 'auschwitz', ',', 'da', '##cha', '##u', 'or', 'tre', '##bl', '##ink', '##a', 'for', 'palestinians', '?', '[SEP]'], ['[CLS]', 'why', 'does', 'quo', '##ra', 'automatically', 'ban', 'conservative', 'opinions', 'when', 'reported', ',', 'but', 'does', 'not', 'do', 'the', 'same', 'for', 'liberal', 'views', '?', '[SEP]'], ['[CLS]', 'is', 'it', 'crazy', 'if', 'i', 'wash', 'or', 'wipe', 'my', 'groceries', 'off', '?', 'ge', '##rm', '##s', 'are', 'everywhere', '.', '[SEP]'], ['[CLS]', 'is', 'there', 'such', 'a', 'thing', 'as', 'dressing', 'moderately', ',', 'and', 'if', 'so', ',', 'how', 'is', 'that', 'different', 'than', 'dressing', 'modest', '##ly', '?', '[SEP]'], ['[CLS]', 'is', 'it', 'just', 'me', 'or', 'have', 'you', 'ever', 'been', 'in', 'this', 'phase', 'wherein', 'you', 'became', 'ignorant', 'to', 'the', 'people', 'you', 'once', 'loved', ',', 'completely', 'disregard', '##ing', 'their', 'feelings', '/', 'lives', 'so', 'you', 'get', 'to', 'have', 'something', 'go', 'your', 'way', 'and', 'feel', 'temporarily', 'at', 'ease', '.', 'how', 'did', 'things', 'change', '?', '[SEP]']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeeFp_8bk6yR"
      },
      "source": [
        "input_ids=[]\n",
        "for i in range(len(tokenized_texts)):\n",
        "  input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XykMtSn9xdhg",
        "outputId": "8da9385f-7c01-4591-a0a6-e16513941e36"
      },
      "source": [
        "print(input_ids[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[101, 2129, 2106, 5447, 17934, 2156, 2037, 2874, 2004, 1037, 3842, 1999, 1996, 4120, 1029, 102], [101, 2079, 2017, 2031, 2019, 4233, 3899, 1010, 2129, 2052, 2017, 8627, 2111, 2000, 11092, 1998, 2025, 4497, 1029, 102], [101, 2339, 2515, 10146, 7461, 2051, 1029, 2515, 10146, 7461, 2686, 10988, 1029, 102], [101, 2129, 2106, 8064, 3854, 19739, 22420, 3489, 2109, 1996, 23848, 3207, 4645, 14130, 2015, 1029, 102], [101, 2064, 1045, 10463, 18318, 2527, 2002, 10415, 2239, 1040, 2000, 1037, 3137, 7997, 2011, 2074, 5278, 1996, 24656, 1029, 102], [101, 2003, 14474, 3254, 3352, 24363, 1010, 4830, 7507, 2226, 2030, 29461, 16558, 19839, 2050, 2005, 21524, 1029, 102], [101, 2339, 2515, 22035, 2527, 8073, 7221, 4603, 10740, 2043, 2988, 1010, 2021, 2515, 2025, 2079, 1996, 2168, 2005, 4314, 5328, 1029, 102], [101, 2003, 2009, 4689, 2065, 1045, 9378, 2030, 13387, 2026, 26298, 2125, 1029, 16216, 10867, 2015, 2024, 7249, 1012, 102], [101, 2003, 2045, 2107, 1037, 2518, 2004, 11225, 17844, 1010, 1998, 2065, 2061, 1010, 2129, 2003, 2008, 2367, 2084, 11225, 10754, 2135, 1029, 102], [101, 2003, 2009, 2074, 2033, 2030, 2031, 2017, 2412, 2042, 1999, 2023, 4403, 16726, 2017, 2150, 21591, 2000, 1996, 2111, 2017, 2320, 3866, 1010, 3294, 27770, 2075, 2037, 5346, 1013, 3268, 2061, 2017, 2131, 2000, 2031, 2242, 2175, 2115, 2126, 1998, 2514, 8184, 2012, 7496, 1012, 2129, 2106, 2477, 2689, 1029, 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOrR36wx11G"
      },
      "source": [
        "del tokenized_texts"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsc1BHF90IZb",
        "outputId": "5b4541a5-fe40-451e-f59c-b5ec604d64bf"
      },
      "source": [
        "print(input_ids[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[101, 2129, 2106, 5447, 17934, 2156, 2037, 2874, 2004, 1037, 3842, 1999, 1996, 4120, 1029, 102], [101, 2079, 2017, 2031, 2019, 4233, 3899, 1010, 2129, 2052, 2017, 8627, 2111, 2000, 11092, 1998, 2025, 4497, 1029, 102], [101, 2339, 2515, 10146, 7461, 2051, 1029, 2515, 10146, 7461, 2686, 10988, 1029, 102], [101, 2129, 2106, 8064, 3854, 19739, 22420, 3489, 2109, 1996, 23848, 3207, 4645, 14130, 2015, 1029, 102], [101, 2064, 1045, 10463, 18318, 2527, 2002, 10415, 2239, 1040, 2000, 1037, 3137, 7997, 2011, 2074, 5278, 1996, 24656, 1029, 102], [101, 2003, 14474, 3254, 3352, 24363, 1010, 4830, 7507, 2226, 2030, 29461, 16558, 19839, 2050, 2005, 21524, 1029, 102], [101, 2339, 2515, 22035, 2527, 8073, 7221, 4603, 10740, 2043, 2988, 1010, 2021, 2515, 2025, 2079, 1996, 2168, 2005, 4314, 5328, 1029, 102], [101, 2003, 2009, 4689, 2065, 1045, 9378, 2030, 13387, 2026, 26298, 2125, 1029, 16216, 10867, 2015, 2024, 7249, 1012, 102], [101, 2003, 2045, 2107, 1037, 2518, 2004, 11225, 17844, 1010, 1998, 2065, 2061, 1010, 2129, 2003, 2008, 2367, 2084, 11225, 10754, 2135, 1029, 102], [101, 2003, 2009, 2074, 2033, 2030, 2031, 2017, 2412, 2042, 1999, 2023, 4403, 16726, 2017, 2150, 21591, 2000, 1996, 2111, 2017, 2320, 3866, 1010, 3294, 27770, 2075, 2037, 5346, 1013, 3268, 2061, 2017, 2131, 2000, 2031, 2242, 2175, 2115, 2126, 1998, 2514, 8184, 2012, 7496, 1012, 2129, 2106, 2477, 2689, 1029, 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4s2nJx4j5l6"
      },
      "source": [
        "\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yp0g-4olHfk"
      },
      "source": [
        "#Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCMZYqWslYxV"
      },
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XwiiRaZ5Xpe",
        "outputId": "037f3bce-1f91-4156-93c6-42de9b050c13"
      },
      "source": [
        "print(train_inputs[:10])\n",
        "print(validation_inputs[:10])\n",
        "print(train_labels[:10])\n",
        "print(validation_labels[:10])\n",
        "print(train_masks[:10])\n",
        "print(validation_masks[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 101 2054 2003 ...    0    0    0]\n",
            " [ 101 2054 2785 ...    0    0    0]\n",
            " [ 101 2054 2003 ...    0    0    0]\n",
            " ...\n",
            " [ 101 1045 1005 ...    0    0    0]\n",
            " [ 101 2054 2052 ...    0    0    0]\n",
            " [ 101 2054 2323 ...    0    0    0]]\n",
            "[[ 101 2339 2987 ...    0    0    0]\n",
            " [ 101 2339 2079 ...    0    0    0]\n",
            " [ 101 2054 2003 ...    0    0    0]\n",
            " ...\n",
            " [ 101 2054 2079 ...    0    0    0]\n",
            " [ 101 2054 2024 ...    0    0    0]\n",
            " [ 101 2129 2079 ...    0    0    0]]\n",
            "[0 0 0 0 0 0 0 0 0 1]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn1e1UcUmno8"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfU0YZRpnL9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15643a18-e088-4260-f4f0-c976e5ba0795"
      },
      "source": [
        "## Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device ='cpu'\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpqdt9znz_DO"
      },
      "source": [
        "\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMgz_UUvnMz-"
      },
      "source": [
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False) \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjpPWycpbf8"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgITTtCRnRXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cd28e2-a661-4a4b-9e0e-69a6eaa9b504"
      },
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  \n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        \n",
        "        train_loss_set.append(loss.item())  \n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "        "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [1/65307], Loss: 0.5142\n",
            "Epoch [1/2], Step [51/65307], Loss: 0.2721\n",
            "Epoch [1/2], Step [101/65307], Loss: 0.3752\n",
            "Epoch [1/2], Step [151/65307], Loss: 0.0528\n",
            "Epoch [1/2], Step [201/65307], Loss: 0.1223\n",
            "Epoch [1/2], Step [251/65307], Loss: 0.2222\n",
            "Epoch [1/2], Step [301/65307], Loss: 0.0887\n",
            "Epoch [1/2], Step [351/65307], Loss: 0.0879\n",
            "Epoch [1/2], Step [401/65307], Loss: 0.3002\n",
            "Epoch [1/2], Step [451/65307], Loss: 0.0775\n",
            "Epoch [1/2], Step [501/65307], Loss: 0.3697\n",
            "Epoch [1/2], Step [551/65307], Loss: 0.0843\n",
            "Epoch [1/2], Step [601/65307], Loss: 0.3797\n",
            "Epoch [1/2], Step [651/65307], Loss: 0.2983\n",
            "Epoch [1/2], Step [701/65307], Loss: 0.0812\n",
            "Epoch [1/2], Step [751/65307], Loss: 0.1678\n",
            "Epoch [1/2], Step [801/65307], Loss: 0.0465\n",
            "Epoch [1/2], Step [851/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [901/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [951/65307], Loss: 0.3515\n",
            "Epoch [1/2], Step [1001/65307], Loss: 0.1293\n",
            "Epoch [1/2], Step [1051/65307], Loss: 0.3548\n",
            "Epoch [1/2], Step [1101/65307], Loss: 0.0490\n",
            "Epoch [1/2], Step [1151/65307], Loss: 0.1017\n",
            "Epoch [1/2], Step [1201/65307], Loss: 0.0963\n",
            "Epoch [1/2], Step [1251/65307], Loss: 0.1914\n",
            "Epoch [1/2], Step [1301/65307], Loss: 0.0580\n",
            "Epoch [1/2], Step [1351/65307], Loss: 0.0651\n",
            "Epoch [1/2], Step [1401/65307], Loss: 0.1197\n",
            "Epoch [1/2], Step [1451/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [1501/65307], Loss: 0.2315\n",
            "Epoch [1/2], Step [1551/65307], Loss: 0.0502\n",
            "Epoch [1/2], Step [1601/65307], Loss: 0.1267\n",
            "Epoch [1/2], Step [1651/65307], Loss: 0.1818\n",
            "Epoch [1/2], Step [1701/65307], Loss: 0.1784\n",
            "Epoch [1/2], Step [1751/65307], Loss: 0.4025\n",
            "Epoch [1/2], Step [1801/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [1851/65307], Loss: 0.0411\n",
            "Epoch [1/2], Step [1901/65307], Loss: 0.0071\n",
            "Epoch [1/2], Step [1951/65307], Loss: 0.0780\n",
            "Epoch [1/2], Step [2001/65307], Loss: 0.0541\n",
            "Epoch [1/2], Step [2051/65307], Loss: 0.1283\n",
            "Epoch [1/2], Step [2101/65307], Loss: 0.3529\n",
            "Epoch [1/2], Step [2151/65307], Loss: 0.2780\n",
            "Epoch [1/2], Step [2201/65307], Loss: 0.0389\n",
            "Epoch [1/2], Step [2251/65307], Loss: 0.0708\n",
            "Epoch [1/2], Step [2301/65307], Loss: 0.2834\n",
            "Epoch [1/2], Step [2351/65307], Loss: 0.1124\n",
            "Epoch [1/2], Step [2401/65307], Loss: 0.0843\n",
            "Epoch [1/2], Step [2451/65307], Loss: 0.1299\n",
            "Epoch [1/2], Step [2501/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [2551/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [2601/65307], Loss: 0.0331\n",
            "Epoch [1/2], Step [2651/65307], Loss: 0.0235\n",
            "Epoch [1/2], Step [2701/65307], Loss: 0.0374\n",
            "Epoch [1/2], Step [2751/65307], Loss: 0.1740\n",
            "Epoch [1/2], Step [2801/65307], Loss: 0.0148\n",
            "Epoch [1/2], Step [2851/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [2901/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [2951/65307], Loss: 0.2075\n",
            "Epoch [1/2], Step [3001/65307], Loss: 0.0758\n",
            "Epoch [1/2], Step [3051/65307], Loss: 0.1277\n",
            "Epoch [1/2], Step [3101/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [3151/65307], Loss: 0.0123\n",
            "Epoch [1/2], Step [3201/65307], Loss: 0.1829\n",
            "Epoch [1/2], Step [3251/65307], Loss: 0.6336\n",
            "Epoch [1/2], Step [3301/65307], Loss: 0.0380\n",
            "Epoch [1/2], Step [3351/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [3401/65307], Loss: 0.0612\n",
            "Epoch [1/2], Step [3451/65307], Loss: 0.1863\n",
            "Epoch [1/2], Step [3501/65307], Loss: 0.1718\n",
            "Epoch [1/2], Step [3551/65307], Loss: 0.1129\n",
            "Epoch [1/2], Step [3601/65307], Loss: 0.0304\n",
            "Epoch [1/2], Step [3651/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [3701/65307], Loss: 0.0584\n",
            "Epoch [1/2], Step [3751/65307], Loss: 0.0167\n",
            "Epoch [1/2], Step [3801/65307], Loss: 0.0724\n",
            "Epoch [1/2], Step [3851/65307], Loss: 0.2139\n",
            "Epoch [1/2], Step [3901/65307], Loss: 0.0954\n",
            "Epoch [1/2], Step [3951/65307], Loss: 0.0312\n",
            "Epoch [1/2], Step [4001/65307], Loss: 0.0266\n",
            "Epoch [1/2], Step [4051/65307], Loss: 0.1042\n",
            "Epoch [1/2], Step [4101/65307], Loss: 0.0183\n",
            "Epoch [1/2], Step [4151/65307], Loss: 0.0041\n",
            "Epoch [1/2], Step [4201/65307], Loss: 0.0900\n",
            "Epoch [1/2], Step [4251/65307], Loss: 0.0599\n",
            "Epoch [1/2], Step [4301/65307], Loss: 0.1064\n",
            "Epoch [1/2], Step [4351/65307], Loss: 0.0728\n",
            "Epoch [1/2], Step [4401/65307], Loss: 0.0244\n",
            "Epoch [1/2], Step [4451/65307], Loss: 0.0178\n",
            "Epoch [1/2], Step [4501/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [4551/65307], Loss: 0.0482\n",
            "Epoch [1/2], Step [4601/65307], Loss: 0.3625\n",
            "Epoch [1/2], Step [4651/65307], Loss: 0.0267\n",
            "Epoch [1/2], Step [4701/65307], Loss: 0.0516\n",
            "Epoch [1/2], Step [4751/65307], Loss: 0.1548\n",
            "Epoch [1/2], Step [4801/65307], Loss: 0.1838\n",
            "Epoch [1/2], Step [4851/65307], Loss: 0.1099\n",
            "Epoch [1/2], Step [4901/65307], Loss: 0.0752\n",
            "Epoch [1/2], Step [4951/65307], Loss: 0.1763\n",
            "Epoch [1/2], Step [5001/65307], Loss: 0.0120\n",
            "Epoch [1/2], Step [5051/65307], Loss: 0.0105\n",
            "Epoch [1/2], Step [5101/65307], Loss: 0.0855\n",
            "Epoch [1/2], Step [5151/65307], Loss: 0.2936\n",
            "Epoch [1/2], Step [5201/65307], Loss: 0.1018\n",
            "Epoch [1/2], Step [5251/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [5301/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [5351/65307], Loss: 0.0137\n",
            "Epoch [1/2], Step [5401/65307], Loss: 0.0258\n",
            "Epoch [1/2], Step [5451/65307], Loss: 0.1014\n",
            "Epoch [1/2], Step [5501/65307], Loss: 0.3433\n",
            "Epoch [1/2], Step [5551/65307], Loss: 0.0881\n",
            "Epoch [1/2], Step [5601/65307], Loss: 0.0336\n",
            "Epoch [1/2], Step [5651/65307], Loss: 0.1425\n",
            "Epoch [1/2], Step [5701/65307], Loss: 0.0050\n",
            "Epoch [1/2], Step [5751/65307], Loss: 0.4535\n",
            "Epoch [1/2], Step [5801/65307], Loss: 0.0082\n",
            "Epoch [1/2], Step [5851/65307], Loss: 0.0743\n",
            "Epoch [1/2], Step [5901/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [5951/65307], Loss: 0.0154\n",
            "Epoch [1/2], Step [6001/65307], Loss: 0.0543\n",
            "Epoch [1/2], Step [6051/65307], Loss: 0.0710\n",
            "Epoch [1/2], Step [6101/65307], Loss: 0.1268\n",
            "Epoch [1/2], Step [6151/65307], Loss: 0.1174\n",
            "Epoch [1/2], Step [6201/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [6251/65307], Loss: 0.0113\n",
            "Epoch [1/2], Step [6301/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [6351/65307], Loss: 0.0793\n",
            "Epoch [1/2], Step [6401/65307], Loss: 0.0581\n",
            "Epoch [1/2], Step [6451/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [6501/65307], Loss: 0.2214\n",
            "Epoch [1/2], Step [6551/65307], Loss: 0.0959\n",
            "Epoch [1/2], Step [6601/65307], Loss: 0.1584\n",
            "Epoch [1/2], Step [6651/65307], Loss: 0.1896\n",
            "Epoch [1/2], Step [6701/65307], Loss: 0.0692\n",
            "Epoch [1/2], Step [6751/65307], Loss: 0.4911\n",
            "Epoch [1/2], Step [6801/65307], Loss: 0.2443\n",
            "Epoch [1/2], Step [6851/65307], Loss: 0.3965\n",
            "Epoch [1/2], Step [6901/65307], Loss: 0.0142\n",
            "Epoch [1/2], Step [6951/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [7001/65307], Loss: 0.3694\n",
            "Epoch [1/2], Step [7051/65307], Loss: 0.1269\n",
            "Epoch [1/2], Step [7101/65307], Loss: 0.1088\n",
            "Epoch [1/2], Step [7151/65307], Loss: 0.1417\n",
            "Epoch [1/2], Step [7201/65307], Loss: 0.0861\n",
            "Epoch [1/2], Step [7251/65307], Loss: 0.1492\n",
            "Epoch [1/2], Step [7301/65307], Loss: 0.0136\n",
            "Epoch [1/2], Step [7351/65307], Loss: 0.3430\n",
            "Epoch [1/2], Step [7401/65307], Loss: 0.1805\n",
            "Epoch [1/2], Step [7451/65307], Loss: 0.0357\n",
            "Epoch [1/2], Step [7501/65307], Loss: 0.3232\n",
            "Epoch [1/2], Step [7551/65307], Loss: 0.0792\n",
            "Epoch [1/2], Step [7601/65307], Loss: 0.2856\n",
            "Epoch [1/2], Step [7651/65307], Loss: 0.0128\n",
            "Epoch [1/2], Step [7701/65307], Loss: 0.1001\n",
            "Epoch [1/2], Step [7751/65307], Loss: 0.2761\n",
            "Epoch [1/2], Step [7801/65307], Loss: 0.1539\n",
            "Epoch [1/2], Step [7851/65307], Loss: 0.1436\n",
            "Epoch [1/2], Step [7901/65307], Loss: 0.0567\n",
            "Epoch [1/2], Step [7951/65307], Loss: 0.2854\n",
            "Epoch [1/2], Step [8001/65307], Loss: 0.0211\n",
            "Epoch [1/2], Step [8051/65307], Loss: 0.1593\n",
            "Epoch [1/2], Step [8101/65307], Loss: 0.1335\n",
            "Epoch [1/2], Step [8151/65307], Loss: 0.1995\n",
            "Epoch [1/2], Step [8201/65307], Loss: 0.1026\n",
            "Epoch [1/2], Step [8251/65307], Loss: 0.2622\n",
            "Epoch [1/2], Step [8301/65307], Loss: 0.0177\n",
            "Epoch [1/2], Step [8351/65307], Loss: 0.0854\n",
            "Epoch [1/2], Step [8401/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [8451/65307], Loss: 0.0049\n",
            "Epoch [1/2], Step [8501/65307], Loss: 0.0562\n",
            "Epoch [1/2], Step [8551/65307], Loss: 0.1583\n",
            "Epoch [1/2], Step [8601/65307], Loss: 0.6197\n",
            "Epoch [1/2], Step [8651/65307], Loss: 0.1242\n",
            "Epoch [1/2], Step [8701/65307], Loss: 0.0674\n",
            "Epoch [1/2], Step [8751/65307], Loss: 0.0582\n",
            "Epoch [1/2], Step [8801/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [8851/65307], Loss: 0.0659\n",
            "Epoch [1/2], Step [8901/65307], Loss: 0.2496\n",
            "Epoch [1/2], Step [8951/65307], Loss: 0.0669\n",
            "Epoch [1/2], Step [9001/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [9051/65307], Loss: 0.3209\n",
            "Epoch [1/2], Step [9101/65307], Loss: 0.0536\n",
            "Epoch [1/2], Step [9151/65307], Loss: 0.0846\n",
            "Epoch [1/2], Step [9201/65307], Loss: 0.1342\n",
            "Epoch [1/2], Step [9251/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [9301/65307], Loss: 0.0273\n",
            "Epoch [1/2], Step [9351/65307], Loss: 0.2558\n",
            "Epoch [1/2], Step [9401/65307], Loss: 0.2235\n",
            "Epoch [1/2], Step [9451/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [9501/65307], Loss: 0.5653\n",
            "Epoch [1/2], Step [9551/65307], Loss: 0.1145\n",
            "Epoch [1/2], Step [9601/65307], Loss: 0.1468\n",
            "Epoch [1/2], Step [9651/65307], Loss: 0.0300\n",
            "Epoch [1/2], Step [9701/65307], Loss: 0.0901\n",
            "Epoch [1/2], Step [9751/65307], Loss: 0.0033\n",
            "Epoch [1/2], Step [9801/65307], Loss: 0.1382\n",
            "Epoch [1/2], Step [9851/65307], Loss: 0.0110\n",
            "Epoch [1/2], Step [9901/65307], Loss: 0.1859\n",
            "Epoch [1/2], Step [9951/65307], Loss: 0.0558\n",
            "Epoch [1/2], Step [10001/65307], Loss: 0.2342\n",
            "Epoch [1/2], Step [10051/65307], Loss: 0.6773\n",
            "Epoch [1/2], Step [10101/65307], Loss: 0.0308\n",
            "Epoch [1/2], Step [10151/65307], Loss: 0.0470\n",
            "Epoch [1/2], Step [10201/65307], Loss: 0.0545\n",
            "Epoch [1/2], Step [10251/65307], Loss: 0.0211\n",
            "Epoch [1/2], Step [10301/65307], Loss: 0.0781\n",
            "Epoch [1/2], Step [10351/65307], Loss: 0.0957\n",
            "Epoch [1/2], Step [10401/65307], Loss: 0.2506\n",
            "Epoch [1/2], Step [10451/65307], Loss: 0.0590\n",
            "Epoch [1/2], Step [10501/65307], Loss: 0.0850\n",
            "Epoch [1/2], Step [10551/65307], Loss: 0.2954\n",
            "Epoch [1/2], Step [10601/65307], Loss: 0.0628\n",
            "Epoch [1/2], Step [10651/65307], Loss: 0.0653\n",
            "Epoch [1/2], Step [10701/65307], Loss: 0.0961\n",
            "Epoch [1/2], Step [10751/65307], Loss: 0.0045\n",
            "Epoch [1/2], Step [10801/65307], Loss: 0.1509\n",
            "Epoch [1/2], Step [10851/65307], Loss: 0.0260\n",
            "Epoch [1/2], Step [10901/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [10951/65307], Loss: 0.0252\n",
            "Epoch [1/2], Step [11001/65307], Loss: 0.0820\n",
            "Epoch [1/2], Step [11051/65307], Loss: 0.0112\n",
            "Epoch [1/2], Step [11101/65307], Loss: 0.1440\n",
            "Epoch [1/2], Step [11151/65307], Loss: 0.1255\n",
            "Epoch [1/2], Step [11201/65307], Loss: 0.0715\n",
            "Epoch [1/2], Step [11251/65307], Loss: 0.2277\n",
            "Epoch [1/2], Step [11301/65307], Loss: 0.0711\n",
            "Epoch [1/2], Step [11351/65307], Loss: 0.1067\n",
            "Epoch [1/2], Step [11401/65307], Loss: 0.0238\n",
            "Epoch [1/2], Step [11451/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [11501/65307], Loss: 0.1405\n",
            "Epoch [1/2], Step [11551/65307], Loss: 0.0326\n",
            "Epoch [1/2], Step [11601/65307], Loss: 0.0428\n",
            "Epoch [1/2], Step [11651/65307], Loss: 0.1023\n",
            "Epoch [1/2], Step [11701/65307], Loss: 0.1033\n",
            "Epoch [1/2], Step [11751/65307], Loss: 0.1668\n",
            "Epoch [1/2], Step [11801/65307], Loss: 0.0991\n",
            "Epoch [1/2], Step [11851/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [11901/65307], Loss: 0.0054\n",
            "Epoch [1/2], Step [11951/65307], Loss: 0.0953\n",
            "Epoch [1/2], Step [12001/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [12051/65307], Loss: 0.0980\n",
            "Epoch [1/2], Step [12101/65307], Loss: 0.0802\n",
            "Epoch [1/2], Step [12151/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [12201/65307], Loss: 0.3044\n",
            "Epoch [1/2], Step [12251/65307], Loss: 0.1176\n",
            "Epoch [1/2], Step [12301/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [12351/65307], Loss: 0.0191\n",
            "Epoch [1/2], Step [12401/65307], Loss: 0.0294\n",
            "Epoch [1/2], Step [12451/65307], Loss: 0.0889\n",
            "Epoch [1/2], Step [12501/65307], Loss: 0.0045\n",
            "Epoch [1/2], Step [12551/65307], Loss: 0.0389\n",
            "Epoch [1/2], Step [12601/65307], Loss: 0.0184\n",
            "Epoch [1/2], Step [12651/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [12701/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [12751/65307], Loss: 0.2002\n",
            "Epoch [1/2], Step [12801/65307], Loss: 0.2533\n",
            "Epoch [1/2], Step [12851/65307], Loss: 0.0388\n",
            "Epoch [1/2], Step [12901/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [12951/65307], Loss: 0.1537\n",
            "Epoch [1/2], Step [13001/65307], Loss: 0.1118\n",
            "Epoch [1/2], Step [13051/65307], Loss: 0.0115\n",
            "Epoch [1/2], Step [13101/65307], Loss: 0.1560\n",
            "Epoch [1/2], Step [13151/65307], Loss: 0.0176\n",
            "Epoch [1/2], Step [13201/65307], Loss: 0.0055\n",
            "Epoch [1/2], Step [13251/65307], Loss: 0.0799\n",
            "Epoch [1/2], Step [13301/65307], Loss: 0.0638\n",
            "Epoch [1/2], Step [13351/65307], Loss: 0.1898\n",
            "Epoch [1/2], Step [13401/65307], Loss: 0.1119\n",
            "Epoch [1/2], Step [13451/65307], Loss: 0.0496\n",
            "Epoch [1/2], Step [13501/65307], Loss: 0.0721\n",
            "Epoch [1/2], Step [13551/65307], Loss: 0.3119\n",
            "Epoch [1/2], Step [13601/65307], Loss: 0.0811\n",
            "Epoch [1/2], Step [13651/65307], Loss: 0.1109\n",
            "Epoch [1/2], Step [13701/65307], Loss: 0.1438\n",
            "Epoch [1/2], Step [13751/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [13801/65307], Loss: 0.3468\n",
            "Epoch [1/2], Step [13851/65307], Loss: 0.1013\n",
            "Epoch [1/2], Step [13901/65307], Loss: 0.0945\n",
            "Epoch [1/2], Step [13951/65307], Loss: 0.2127\n",
            "Epoch [1/2], Step [14001/65307], Loss: 0.0306\n",
            "Epoch [1/2], Step [14051/65307], Loss: 0.1529\n",
            "Epoch [1/2], Step [14101/65307], Loss: 0.1455\n",
            "Epoch [1/2], Step [14151/65307], Loss: 0.5134\n",
            "Epoch [1/2], Step [14201/65307], Loss: 0.2321\n",
            "Epoch [1/2], Step [14251/65307], Loss: 0.3539\n",
            "Epoch [1/2], Step [14301/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [14351/65307], Loss: 0.0054\n",
            "Epoch [1/2], Step [14401/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [14451/65307], Loss: 0.2170\n",
            "Epoch [1/2], Step [14501/65307], Loss: 0.1114\n",
            "Epoch [1/2], Step [14551/65307], Loss: 0.0069\n",
            "Epoch [1/2], Step [14601/65307], Loss: 0.2578\n",
            "Epoch [1/2], Step [14651/65307], Loss: 0.1938\n",
            "Epoch [1/2], Step [14701/65307], Loss: 0.0460\n",
            "Epoch [1/2], Step [14751/65307], Loss: 0.1766\n",
            "Epoch [1/2], Step [14801/65307], Loss: 0.0560\n",
            "Epoch [1/2], Step [14851/65307], Loss: 0.2169\n",
            "Epoch [1/2], Step [14901/65307], Loss: 0.0683\n",
            "Epoch [1/2], Step [14951/65307], Loss: 0.0791\n",
            "Epoch [1/2], Step [15001/65307], Loss: 0.1181\n",
            "Epoch [1/2], Step [15051/65307], Loss: 0.0930\n",
            "Epoch [1/2], Step [15101/65307], Loss: 0.1195\n",
            "Epoch [1/2], Step [15151/65307], Loss: 0.0200\n",
            "Epoch [1/2], Step [15201/65307], Loss: 0.0363\n",
            "Epoch [1/2], Step [15251/65307], Loss: 0.1168\n",
            "Epoch [1/2], Step [15301/65307], Loss: 0.1055\n",
            "Epoch [1/2], Step [15351/65307], Loss: 0.0868\n",
            "Epoch [1/2], Step [15401/65307], Loss: 0.0582\n",
            "Epoch [1/2], Step [15451/65307], Loss: 0.1854\n",
            "Epoch [1/2], Step [15501/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [15551/65307], Loss: 0.2309\n",
            "Epoch [1/2], Step [15601/65307], Loss: 0.0277\n",
            "Epoch [1/2], Step [15651/65307], Loss: 0.0897\n",
            "Epoch [1/2], Step [15701/65307], Loss: 0.0914\n",
            "Epoch [1/2], Step [15751/65307], Loss: 0.3107\n",
            "Epoch [1/2], Step [15801/65307], Loss: 0.2924\n",
            "Epoch [1/2], Step [15851/65307], Loss: 0.0758\n",
            "Epoch [1/2], Step [15901/65307], Loss: 0.3135\n",
            "Epoch [1/2], Step [15951/65307], Loss: 0.2521\n",
            "Epoch [1/2], Step [16001/65307], Loss: 0.0656\n",
            "Epoch [1/2], Step [16051/65307], Loss: 0.0184\n",
            "Epoch [1/2], Step [16101/65307], Loss: 0.0121\n",
            "Epoch [1/2], Step [16151/65307], Loss: 0.1243\n",
            "Epoch [1/2], Step [16201/65307], Loss: 0.0330\n",
            "Epoch [1/2], Step [16251/65307], Loss: 0.0927\n",
            "Epoch [1/2], Step [16301/65307], Loss: 0.1039\n",
            "Epoch [1/2], Step [16351/65307], Loss: 0.0145\n",
            "Epoch [1/2], Step [16401/65307], Loss: 0.0532\n",
            "Epoch [1/2], Step [16451/65307], Loss: 0.2251\n",
            "Epoch [1/2], Step [16501/65307], Loss: 0.0357\n",
            "Epoch [1/2], Step [16551/65307], Loss: 0.2006\n",
            "Epoch [1/2], Step [16601/65307], Loss: 0.0136\n",
            "Epoch [1/2], Step [16651/65307], Loss: 0.0100\n",
            "Epoch [1/2], Step [16701/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [16751/65307], Loss: 0.0624\n",
            "Epoch [1/2], Step [16801/65307], Loss: 0.0768\n",
            "Epoch [1/2], Step [16851/65307], Loss: 0.1032\n",
            "Epoch [1/2], Step [16901/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [16951/65307], Loss: 0.0180\n",
            "Epoch [1/2], Step [17001/65307], Loss: 0.1280\n",
            "Epoch [1/2], Step [17051/65307], Loss: 0.2461\n",
            "Epoch [1/2], Step [17101/65307], Loss: 0.0282\n",
            "Epoch [1/2], Step [17151/65307], Loss: 0.0407\n",
            "Epoch [1/2], Step [17201/65307], Loss: 0.0770\n",
            "Epoch [1/2], Step [17251/65307], Loss: 0.0922\n",
            "Epoch [1/2], Step [17301/65307], Loss: 0.0342\n",
            "Epoch [1/2], Step [17351/65307], Loss: 0.0493\n",
            "Epoch [1/2], Step [17401/65307], Loss: 0.0222\n",
            "Epoch [1/2], Step [17451/65307], Loss: 0.0587\n",
            "Epoch [1/2], Step [17501/65307], Loss: 0.2307\n",
            "Epoch [1/2], Step [17551/65307], Loss: 0.0515\n",
            "Epoch [1/2], Step [17601/65307], Loss: 0.0947\n",
            "Epoch [1/2], Step [17651/65307], Loss: 0.1970\n",
            "Epoch [1/2], Step [17701/65307], Loss: 0.0305\n",
            "Epoch [1/2], Step [17751/65307], Loss: 0.1327\n",
            "Epoch [1/2], Step [17801/65307], Loss: 0.6887\n",
            "Epoch [1/2], Step [17851/65307], Loss: 0.2151\n",
            "Epoch [1/2], Step [17901/65307], Loss: 0.0479\n",
            "Epoch [1/2], Step [17951/65307], Loss: 0.1529\n",
            "Epoch [1/2], Step [18001/65307], Loss: 0.0748\n",
            "Epoch [1/2], Step [18051/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [18101/65307], Loss: 0.0404\n",
            "Epoch [1/2], Step [18151/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [18201/65307], Loss: 0.1334\n",
            "Epoch [1/2], Step [18251/65307], Loss: 0.0559\n",
            "Epoch [1/2], Step [18301/65307], Loss: 0.0463\n",
            "Epoch [1/2], Step [18351/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [18401/65307], Loss: 0.0480\n",
            "Epoch [1/2], Step [18451/65307], Loss: 0.0091\n",
            "Epoch [1/2], Step [18501/65307], Loss: 0.2612\n",
            "Epoch [1/2], Step [18551/65307], Loss: 0.0068\n",
            "Epoch [1/2], Step [18601/65307], Loss: 0.5313\n",
            "Epoch [1/2], Step [18651/65307], Loss: 0.0216\n",
            "Epoch [1/2], Step [18701/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [18751/65307], Loss: 0.0971\n",
            "Epoch [1/2], Step [18801/65307], Loss: 0.0783\n",
            "Epoch [1/2], Step [18851/65307], Loss: 0.1254\n",
            "Epoch [1/2], Step [18901/65307], Loss: 0.0585\n",
            "Epoch [1/2], Step [18951/65307], Loss: 0.1152\n",
            "Epoch [1/2], Step [19001/65307], Loss: 0.0100\n",
            "Epoch [1/2], Step [19051/65307], Loss: 0.3770\n",
            "Epoch [1/2], Step [19101/65307], Loss: 0.1511\n",
            "Epoch [1/2], Step [19151/65307], Loss: 0.3039\n",
            "Epoch [1/2], Step [19201/65307], Loss: 0.1984\n",
            "Epoch [1/2], Step [19251/65307], Loss: 0.0346\n",
            "Epoch [1/2], Step [19301/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [19351/65307], Loss: 0.0461\n",
            "Epoch [1/2], Step [19401/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [19451/65307], Loss: 0.0185\n",
            "Epoch [1/2], Step [19501/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [19551/65307], Loss: 0.0141\n",
            "Epoch [1/2], Step [19601/65307], Loss: 0.1262\n",
            "Epoch [1/2], Step [19651/65307], Loss: 0.0878\n",
            "Epoch [1/2], Step [19701/65307], Loss: 0.0213\n",
            "Epoch [1/2], Step [19751/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [19801/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [1/2], Step [19901/65307], Loss: 0.4285\n",
            "Epoch [1/2], Step [19951/65307], Loss: 0.3114\n",
            "Epoch [1/2], Step [20001/65307], Loss: 0.0316\n",
            "Epoch [1/2], Step [20051/65307], Loss: 0.0130\n",
            "Epoch [1/2], Step [20101/65307], Loss: 0.1009\n",
            "Epoch [1/2], Step [20151/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [20201/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [20251/65307], Loss: 0.2136\n",
            "Epoch [1/2], Step [20301/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [20351/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [20401/65307], Loss: 0.0408\n",
            "Epoch [1/2], Step [20451/65307], Loss: 0.0209\n",
            "Epoch [1/2], Step [20501/65307], Loss: 0.1361\n",
            "Epoch [1/2], Step [20551/65307], Loss: 0.2432\n",
            "Epoch [1/2], Step [20601/65307], Loss: 0.1074\n",
            "Epoch [1/2], Step [20651/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [20701/65307], Loss: 0.0558\n",
            "Epoch [1/2], Step [20751/65307], Loss: 0.1388\n",
            "Epoch [1/2], Step [20801/65307], Loss: 0.3362\n",
            "Epoch [1/2], Step [20851/65307], Loss: 0.1214\n",
            "Epoch [1/2], Step [20901/65307], Loss: 0.3107\n",
            "Epoch [1/2], Step [20951/65307], Loss: 0.0485\n",
            "Epoch [1/2], Step [21001/65307], Loss: 0.0293\n",
            "Epoch [1/2], Step [21051/65307], Loss: 0.0904\n",
            "Epoch [1/2], Step [21101/65307], Loss: 0.0308\n",
            "Epoch [1/2], Step [21151/65307], Loss: 0.0639\n",
            "Epoch [1/2], Step [21201/65307], Loss: 0.0544\n",
            "Epoch [1/2], Step [21251/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [21301/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [21351/65307], Loss: 0.0354\n",
            "Epoch [1/2], Step [21401/65307], Loss: 0.1113\n",
            "Epoch [1/2], Step [21451/65307], Loss: 0.1289\n",
            "Epoch [1/2], Step [21501/65307], Loss: 0.0200\n",
            "Epoch [1/2], Step [21551/65307], Loss: 0.0893\n",
            "Epoch [1/2], Step [21601/65307], Loss: 0.0233\n",
            "Epoch [1/2], Step [21651/65307], Loss: 0.1867\n",
            "Epoch [1/2], Step [21701/65307], Loss: 0.0216\n",
            "Epoch [1/2], Step [21751/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [21801/65307], Loss: 0.3973\n",
            "Epoch [1/2], Step [21851/65307], Loss: 0.1303\n",
            "Epoch [1/2], Step [21901/65307], Loss: 0.0068\n",
            "Epoch [1/2], Step [21951/65307], Loss: 0.0122\n",
            "Epoch [1/2], Step [22001/65307], Loss: 0.1552\n",
            "Epoch [1/2], Step [22051/65307], Loss: 0.0062\n",
            "Epoch [1/2], Step [22101/65307], Loss: 0.0405\n",
            "Epoch [1/2], Step [22151/65307], Loss: 0.0017\n",
            "Epoch [1/2], Step [22201/65307], Loss: 0.0509\n",
            "Epoch [1/2], Step [22251/65307], Loss: 0.0985\n",
            "Epoch [1/2], Step [22301/65307], Loss: 0.0067\n",
            "Epoch [1/2], Step [22351/65307], Loss: 0.5207\n",
            "Epoch [1/2], Step [22401/65307], Loss: 0.0469\n",
            "Epoch [1/2], Step [22451/65307], Loss: 0.0696\n",
            "Epoch [1/2], Step [22501/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [22551/65307], Loss: 0.1831\n",
            "Epoch [1/2], Step [22601/65307], Loss: 0.0902\n",
            "Epoch [1/2], Step [22651/65307], Loss: 0.1192\n",
            "Epoch [1/2], Step [22701/65307], Loss: 0.2767\n",
            "Epoch [1/2], Step [22751/65307], Loss: 0.0220\n",
            "Epoch [1/2], Step [22801/65307], Loss: 0.1211\n",
            "Epoch [1/2], Step [22851/65307], Loss: 0.0982\n",
            "Epoch [1/2], Step [22901/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [22951/65307], Loss: 0.0776\n",
            "Epoch [1/2], Step [23001/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [23051/65307], Loss: 0.2064\n",
            "Epoch [1/2], Step [23101/65307], Loss: 0.0042\n",
            "Epoch [1/2], Step [23151/65307], Loss: 0.0524\n",
            "Epoch [1/2], Step [23201/65307], Loss: 0.0945\n",
            "Epoch [1/2], Step [23251/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [23301/65307], Loss: 0.3614\n",
            "Epoch [1/2], Step [23351/65307], Loss: 0.0784\n",
            "Epoch [1/2], Step [23401/65307], Loss: 0.0126\n",
            "Epoch [1/2], Step [23451/65307], Loss: 0.1622\n",
            "Epoch [1/2], Step [23501/65307], Loss: 0.0911\n",
            "Epoch [1/2], Step [23551/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [23601/65307], Loss: 0.0010\n",
            "Epoch [1/2], Step [23651/65307], Loss: 0.2819\n",
            "Epoch [1/2], Step [23701/65307], Loss: 0.0686\n",
            "Epoch [1/2], Step [23751/65307], Loss: 0.0822\n",
            "Epoch [1/2], Step [23801/65307], Loss: 0.3872\n",
            "Epoch [1/2], Step [23851/65307], Loss: 0.0726\n",
            "Epoch [1/2], Step [23901/65307], Loss: 0.0570\n",
            "Epoch [1/2], Step [23951/65307], Loss: 0.0742\n",
            "Epoch [1/2], Step [24001/65307], Loss: 0.0250\n",
            "Epoch [1/2], Step [24051/65307], Loss: 0.4306\n",
            "Epoch [1/2], Step [24101/65307], Loss: 0.0148\n",
            "Epoch [1/2], Step [24151/65307], Loss: 0.0805\n",
            "Epoch [1/2], Step [24201/65307], Loss: 0.4834\n",
            "Epoch [1/2], Step [24251/65307], Loss: 0.0223\n",
            "Epoch [1/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [24351/65307], Loss: 0.2527\n",
            "Epoch [1/2], Step [24401/65307], Loss: 0.1315\n",
            "Epoch [1/2], Step [24451/65307], Loss: 0.1656\n",
            "Epoch [1/2], Step [24501/65307], Loss: 0.0554\n",
            "Epoch [1/2], Step [24551/65307], Loss: 0.1652\n",
            "Epoch [1/2], Step [24601/65307], Loss: 0.1548\n",
            "Epoch [1/2], Step [24651/65307], Loss: 0.0323\n",
            "Epoch [1/2], Step [24701/65307], Loss: 0.0455\n",
            "Epoch [1/2], Step [24751/65307], Loss: 0.5137\n",
            "Epoch [1/2], Step [24801/65307], Loss: 0.0624\n",
            "Epoch [1/2], Step [24851/65307], Loss: 0.0681\n",
            "Epoch [1/2], Step [24901/65307], Loss: 0.0554\n",
            "Epoch [1/2], Step [24951/65307], Loss: 0.0413\n",
            "Epoch [1/2], Step [25001/65307], Loss: 0.7531\n",
            "Epoch [1/2], Step [25051/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [25101/65307], Loss: 0.2371\n",
            "Epoch [1/2], Step [25151/65307], Loss: 0.0817\n",
            "Epoch [1/2], Step [25201/65307], Loss: 0.1731\n",
            "Epoch [1/2], Step [25251/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [25301/65307], Loss: 0.3550\n",
            "Epoch [1/2], Step [25351/65307], Loss: 0.2886\n",
            "Epoch [1/2], Step [25401/65307], Loss: 0.2273\n",
            "Epoch [1/2], Step [25451/65307], Loss: 0.1246\n",
            "Epoch [1/2], Step [25501/65307], Loss: 0.0508\n",
            "Epoch [1/2], Step [25551/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [25601/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [25651/65307], Loss: 0.1267\n",
            "Epoch [1/2], Step [25701/65307], Loss: 0.0217\n",
            "Epoch [1/2], Step [25751/65307], Loss: 0.0608\n",
            "Epoch [1/2], Step [25801/65307], Loss: 0.0366\n",
            "Epoch [1/2], Step [25851/65307], Loss: 0.0611\n",
            "Epoch [1/2], Step [25901/65307], Loss: 0.0341\n",
            "Epoch [1/2], Step [25951/65307], Loss: 0.1371\n",
            "Epoch [1/2], Step [26001/65307], Loss: 0.3328\n",
            "Epoch [1/2], Step [26051/65307], Loss: 0.4588\n",
            "Epoch [1/2], Step [26101/65307], Loss: 0.0421\n",
            "Epoch [1/2], Step [26151/65307], Loss: 0.0301\n",
            "Epoch [1/2], Step [26201/65307], Loss: 0.1631\n",
            "Epoch [1/2], Step [26251/65307], Loss: 0.0401\n",
            "Epoch [1/2], Step [26301/65307], Loss: 0.4243\n",
            "Epoch [1/2], Step [26351/65307], Loss: 0.2530\n",
            "Epoch [1/2], Step [26401/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [26451/65307], Loss: 0.2705\n",
            "Epoch [1/2], Step [26501/65307], Loss: 0.1697\n",
            "Epoch [1/2], Step [26551/65307], Loss: 0.0203\n",
            "Epoch [1/2], Step [26601/65307], Loss: 0.0531\n",
            "Epoch [1/2], Step [26651/65307], Loss: 0.0323\n",
            "Epoch [1/2], Step [26701/65307], Loss: 0.0345\n",
            "Epoch [1/2], Step [26751/65307], Loss: 0.3624\n",
            "Epoch [1/2], Step [26801/65307], Loss: 0.3698\n",
            "Epoch [1/2], Step [26851/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [26901/65307], Loss: 0.0972\n",
            "Epoch [1/2], Step [26951/65307], Loss: 0.1593\n",
            "Epoch [1/2], Step [27001/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [27051/65307], Loss: 0.0515\n",
            "Epoch [1/2], Step [27101/65307], Loss: 0.0129\n",
            "Epoch [1/2], Step [27151/65307], Loss: 0.0119\n",
            "Epoch [1/2], Step [27201/65307], Loss: 0.0522\n",
            "Epoch [1/2], Step [27251/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [27301/65307], Loss: 0.1798\n",
            "Epoch [1/2], Step [27351/65307], Loss: 0.0091\n",
            "Epoch [1/2], Step [27401/65307], Loss: 0.4129\n",
            "Epoch [1/2], Step [27451/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [27501/65307], Loss: 0.0023\n",
            "Epoch [1/2], Step [27551/65307], Loss: 0.0657\n",
            "Epoch [1/2], Step [27601/65307], Loss: 0.2478\n",
            "Epoch [1/2], Step [27651/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [27701/65307], Loss: 0.2880\n",
            "Epoch [1/2], Step [27751/65307], Loss: 0.0251\n",
            "Epoch [1/2], Step [27801/65307], Loss: 0.0502\n",
            "Epoch [1/2], Step [27851/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [27901/65307], Loss: 0.1288\n",
            "Epoch [1/2], Step [27951/65307], Loss: 0.0032\n",
            "Epoch [1/2], Step [28001/65307], Loss: 0.2214\n",
            "Epoch [1/2], Step [28051/65307], Loss: 0.0831\n",
            "Epoch [1/2], Step [28101/65307], Loss: 0.0251\n",
            "Epoch [1/2], Step [28151/65307], Loss: 0.1360\n",
            "Epoch [1/2], Step [28201/65307], Loss: 0.1204\n",
            "Epoch [1/2], Step [28251/65307], Loss: 0.0718\n",
            "Epoch [1/2], Step [28301/65307], Loss: 0.1201\n",
            "Epoch [1/2], Step [28351/65307], Loss: 0.0231\n",
            "Epoch [1/2], Step [28401/65307], Loss: 0.0798\n",
            "Epoch [1/2], Step [28451/65307], Loss: 0.0975\n",
            "Epoch [1/2], Step [28501/65307], Loss: 0.2700\n",
            "Epoch [1/2], Step [28551/65307], Loss: 0.0501\n",
            "Epoch [1/2], Step [28601/65307], Loss: 0.0353\n",
            "Epoch [1/2], Step [28651/65307], Loss: 0.0080\n",
            "Epoch [1/2], Step [28701/65307], Loss: 0.1344\n",
            "Epoch [1/2], Step [28751/65307], Loss: 0.0082\n",
            "Epoch [1/2], Step [28801/65307], Loss: 0.1507\n",
            "Epoch [1/2], Step [28851/65307], Loss: 0.0175\n",
            "Epoch [1/2], Step [28901/65307], Loss: 0.1259\n",
            "Epoch [1/2], Step [28951/65307], Loss: 0.0261\n",
            "Epoch [1/2], Step [29001/65307], Loss: 0.5826\n",
            "Epoch [1/2], Step [29051/65307], Loss: 0.0259\n",
            "Epoch [1/2], Step [29101/65307], Loss: 0.0636\n",
            "Epoch [1/2], Step [29151/65307], Loss: 0.0063\n",
            "Epoch [1/2], Step [29201/65307], Loss: 0.0430\n",
            "Epoch [1/2], Step [29251/65307], Loss: 0.0025\n",
            "Epoch [1/2], Step [29301/65307], Loss: 0.1822\n",
            "Epoch [1/2], Step [29351/65307], Loss: 0.0847\n",
            "Epoch [1/2], Step [29401/65307], Loss: 0.1699\n",
            "Epoch [1/2], Step [29451/65307], Loss: 0.2234\n",
            "Epoch [1/2], Step [29501/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [29551/65307], Loss: 0.0455\n",
            "Epoch [1/2], Step [29601/65307], Loss: 0.0587\n",
            "Epoch [1/2], Step [29651/65307], Loss: 0.0645\n",
            "Epoch [1/2], Step [29701/65307], Loss: 0.0872\n",
            "Epoch [1/2], Step [29751/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [29801/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [29851/65307], Loss: 0.0031\n",
            "Epoch [1/2], Step [29901/65307], Loss: 0.2896\n",
            "Epoch [1/2], Step [29951/65307], Loss: 0.1605\n",
            "Epoch [1/2], Step [30001/65307], Loss: 0.0398\n",
            "Epoch [1/2], Step [30051/65307], Loss: 0.0802\n",
            "Epoch [1/2], Step [30101/65307], Loss: 0.0167\n",
            "Epoch [1/2], Step [30151/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [30201/65307], Loss: 0.0732\n",
            "Epoch [1/2], Step [30251/65307], Loss: 0.0101\n",
            "Epoch [1/2], Step [30301/65307], Loss: 0.0490\n",
            "Epoch [1/2], Step [30351/65307], Loss: 0.0635\n",
            "Epoch [1/2], Step [30401/65307], Loss: 0.0986\n",
            "Epoch [1/2], Step [30451/65307], Loss: 0.0125\n",
            "Epoch [1/2], Step [30501/65307], Loss: 0.0372\n",
            "Epoch [1/2], Step [30551/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [30601/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [30651/65307], Loss: 0.0037\n",
            "Epoch [1/2], Step [30701/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [30751/65307], Loss: 0.1954\n",
            "Epoch [1/2], Step [30801/65307], Loss: 0.0846\n",
            "Epoch [1/2], Step [30851/65307], Loss: 0.0689\n",
            "Epoch [1/2], Step [30901/65307], Loss: 0.0723\n",
            "Epoch [1/2], Step [30951/65307], Loss: 0.0392\n",
            "Epoch [1/2], Step [31001/65307], Loss: 0.0128\n",
            "Epoch [1/2], Step [31051/65307], Loss: 0.0525\n",
            "Epoch [1/2], Step [31101/65307], Loss: 0.0097\n",
            "Epoch [1/2], Step [31151/65307], Loss: 0.2541\n",
            "Epoch [1/2], Step [31201/65307], Loss: 0.0330\n",
            "Epoch [1/2], Step [31251/65307], Loss: 0.0494\n",
            "Epoch [1/2], Step [31301/65307], Loss: 0.1192\n",
            "Epoch [1/2], Step [31351/65307], Loss: 0.0603\n",
            "Epoch [1/2], Step [31401/65307], Loss: 0.0228\n",
            "Epoch [1/2], Step [31451/65307], Loss: 0.1612\n",
            "Epoch [1/2], Step [31501/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [31551/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [31601/65307], Loss: 0.0827\n",
            "Epoch [1/2], Step [31651/65307], Loss: 0.0185\n",
            "Epoch [1/2], Step [31701/65307], Loss: 0.0518\n",
            "Epoch [1/2], Step [31751/65307], Loss: 0.3181\n",
            "Epoch [1/2], Step [31801/65307], Loss: 0.3229\n",
            "Epoch [1/2], Step [31851/65307], Loss: 0.0112\n",
            "Epoch [1/2], Step [31901/65307], Loss: 0.0229\n",
            "Epoch [1/2], Step [31951/65307], Loss: 0.0812\n",
            "Epoch [1/2], Step [32001/65307], Loss: 0.1767\n",
            "Epoch [1/2], Step [32051/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [32101/65307], Loss: 0.0343\n",
            "Epoch [1/2], Step [32151/65307], Loss: 0.0285\n",
            "Epoch [1/2], Step [32201/65307], Loss: 0.1554\n",
            "Epoch [1/2], Step [32251/65307], Loss: 0.1829\n",
            "Epoch [1/2], Step [32301/65307], Loss: 0.0334\n",
            "Epoch [1/2], Step [32351/65307], Loss: 0.2743\n",
            "Epoch [1/2], Step [32401/65307], Loss: 0.0271\n",
            "Epoch [1/2], Step [32451/65307], Loss: 0.0314\n",
            "Epoch [1/2], Step [32501/65307], Loss: 0.1724\n",
            "Epoch [1/2], Step [32551/65307], Loss: 0.0626\n",
            "Epoch [1/2], Step [32601/65307], Loss: 0.0753\n",
            "Epoch [1/2], Step [32651/65307], Loss: 0.2384\n",
            "Epoch [1/2], Step [32701/65307], Loss: 0.6139\n",
            "Epoch [1/2], Step [32751/65307], Loss: 0.0232\n",
            "Epoch [1/2], Step [32801/65307], Loss: 0.0066\n",
            "Epoch [1/2], Step [32851/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [32901/65307], Loss: 0.0542\n",
            "Epoch [1/2], Step [32951/65307], Loss: 0.2202\n",
            "Epoch [1/2], Step [33001/65307], Loss: 0.2028\n",
            "Epoch [1/2], Step [33051/65307], Loss: 0.1998\n",
            "Epoch [1/2], Step [33101/65307], Loss: 0.0633\n",
            "Epoch [1/2], Step [33151/65307], Loss: 0.0851\n",
            "Epoch [1/2], Step [33201/65307], Loss: 0.2239\n",
            "Epoch [1/2], Step [33251/65307], Loss: 0.0163\n",
            "Epoch [1/2], Step [33301/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [33351/65307], Loss: 0.0034\n",
            "Epoch [1/2], Step [33401/65307], Loss: 0.0829\n",
            "Epoch [1/2], Step [33451/65307], Loss: 0.0564\n",
            "Epoch [1/2], Step [33501/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [33551/65307], Loss: 0.0030\n",
            "Epoch [1/2], Step [33601/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [33651/65307], Loss: 0.0095\n",
            "Epoch [1/2], Step [33701/65307], Loss: 0.0666\n",
            "Epoch [1/2], Step [33751/65307], Loss: 0.0329\n",
            "Epoch [1/2], Step [33801/65307], Loss: 0.0791\n",
            "Epoch [1/2], Step [33851/65307], Loss: 0.0032\n",
            "Epoch [1/2], Step [33901/65307], Loss: 0.0327\n",
            "Epoch [1/2], Step [33951/65307], Loss: 0.0311\n",
            "Epoch [1/2], Step [34001/65307], Loss: 0.0358\n",
            "Epoch [1/2], Step [34051/65307], Loss: 0.1505\n",
            "Epoch [1/2], Step [34101/65307], Loss: 0.0137\n",
            "Epoch [1/2], Step [34151/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [34201/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [34251/65307], Loss: 0.0692\n",
            "Epoch [1/2], Step [34301/65307], Loss: 0.0111\n",
            "Epoch [1/2], Step [34351/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [34401/65307], Loss: 0.0933\n",
            "Epoch [1/2], Step [34451/65307], Loss: 0.1665\n",
            "Epoch [1/2], Step [34501/65307], Loss: 0.2645\n",
            "Epoch [1/2], Step [34551/65307], Loss: 0.1169\n",
            "Epoch [1/2], Step [34601/65307], Loss: 0.6550\n",
            "Epoch [1/2], Step [34651/65307], Loss: 0.0501\n",
            "Epoch [1/2], Step [34701/65307], Loss: 0.0593\n",
            "Epoch [1/2], Step [34751/65307], Loss: 0.0958\n",
            "Epoch [1/2], Step [34801/65307], Loss: 0.3792\n",
            "Epoch [1/2], Step [34851/65307], Loss: 0.2735\n",
            "Epoch [1/2], Step [34901/65307], Loss: 0.0061\n",
            "Epoch [1/2], Step [34951/65307], Loss: 0.1172\n",
            "Epoch [1/2], Step [35001/65307], Loss: 0.0319\n",
            "Epoch [1/2], Step [35051/65307], Loss: 0.2177\n",
            "Epoch [1/2], Step [35101/65307], Loss: 0.1343\n",
            "Epoch [1/2], Step [35151/65307], Loss: 0.0639\n",
            "Epoch [1/2], Step [35201/65307], Loss: 0.0173\n",
            "Epoch [1/2], Step [35251/65307], Loss: 0.0774\n",
            "Epoch [1/2], Step [35301/65307], Loss: 0.0697\n",
            "Epoch [1/2], Step [35351/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [35401/65307], Loss: 0.1834\n",
            "Epoch [1/2], Step [35451/65307], Loss: 0.0208\n",
            "Epoch [1/2], Step [35501/65307], Loss: 0.1753\n",
            "Epoch [1/2], Step [35551/65307], Loss: 0.1628\n",
            "Epoch [1/2], Step [35601/65307], Loss: 0.0611\n",
            "Epoch [1/2], Step [35651/65307], Loss: 0.0478\n",
            "Epoch [1/2], Step [35701/65307], Loss: 0.0243\n",
            "Epoch [1/2], Step [35751/65307], Loss: 0.1577\n",
            "Epoch [1/2], Step [35801/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [35851/65307], Loss: 0.0265\n",
            "Epoch [1/2], Step [35901/65307], Loss: 0.0780\n",
            "Epoch [1/2], Step [35951/65307], Loss: 0.1077\n",
            "Epoch [1/2], Step [36001/65307], Loss: 0.0911\n",
            "Epoch [1/2], Step [36051/65307], Loss: 0.0289\n",
            "Epoch [1/2], Step [36101/65307], Loss: 0.0132\n",
            "Epoch [1/2], Step [36151/65307], Loss: 0.0423\n",
            "Epoch [1/2], Step [36201/65307], Loss: 0.1263\n",
            "Epoch [1/2], Step [36251/65307], Loss: 0.3260\n",
            "Epoch [1/2], Step [36301/65307], Loss: 0.1177\n",
            "Epoch [1/2], Step [36351/65307], Loss: 0.1149\n",
            "Epoch [1/2], Step [36401/65307], Loss: 0.1001\n",
            "Epoch [1/2], Step [36451/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [36501/65307], Loss: 0.0078\n",
            "Epoch [1/2], Step [36551/65307], Loss: 0.0477\n",
            "Epoch [1/2], Step [36601/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [36651/65307], Loss: 0.6082\n",
            "Epoch [1/2], Step [36701/65307], Loss: 0.1883\n",
            "Epoch [1/2], Step [36751/65307], Loss: 0.0513\n",
            "Epoch [1/2], Step [36801/65307], Loss: 0.1157\n",
            "Epoch [1/2], Step [36851/65307], Loss: 0.0199\n",
            "Epoch [1/2], Step [36901/65307], Loss: 0.1819\n",
            "Epoch [1/2], Step [36951/65307], Loss: 0.1301\n",
            "Epoch [1/2], Step [37001/65307], Loss: 0.0658\n",
            "Epoch [1/2], Step [37051/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [37101/65307], Loss: 0.0270\n",
            "Epoch [1/2], Step [37151/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [37201/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [37251/65307], Loss: 0.2389\n",
            "Epoch [1/2], Step [37301/65307], Loss: 0.2295\n",
            "Epoch [1/2], Step [37351/65307], Loss: 0.1900\n",
            "Epoch [1/2], Step [37401/65307], Loss: 0.0021\n",
            "Epoch [1/2], Step [37451/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [37501/65307], Loss: 0.1907\n",
            "Epoch [1/2], Step [37551/65307], Loss: 0.0156\n",
            "Epoch [1/2], Step [37601/65307], Loss: 0.3683\n",
            "Epoch [1/2], Step [37651/65307], Loss: 0.0307\n",
            "Epoch [1/2], Step [37701/65307], Loss: 0.1803\n",
            "Epoch [1/2], Step [37751/65307], Loss: 0.1017\n",
            "Epoch [1/2], Step [37801/65307], Loss: 0.0009\n",
            "Epoch [1/2], Step [37851/65307], Loss: 0.3338\n",
            "Epoch [1/2], Step [37901/65307], Loss: 0.0910\n",
            "Epoch [1/2], Step [37951/65307], Loss: 0.1154\n",
            "Epoch [1/2], Step [38001/65307], Loss: 0.0651\n",
            "Epoch [1/2], Step [38051/65307], Loss: 0.0518\n",
            "Epoch [1/2], Step [38101/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [38151/65307], Loss: 0.1209\n",
            "Epoch [1/2], Step [38201/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [38251/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [38301/65307], Loss: 0.0712\n",
            "Epoch [1/2], Step [38351/65307], Loss: 0.1919\n",
            "Epoch [1/2], Step [38401/65307], Loss: 0.0199\n",
            "Epoch [1/2], Step [38451/65307], Loss: 0.0189\n",
            "Epoch [1/2], Step [38501/65307], Loss: 0.0729\n",
            "Epoch [1/2], Step [38551/65307], Loss: 0.0087\n",
            "Epoch [1/2], Step [38601/65307], Loss: 0.1494\n",
            "Epoch [1/2], Step [38651/65307], Loss: 0.0522\n",
            "Epoch [1/2], Step [38701/65307], Loss: 0.0647\n",
            "Epoch [1/2], Step [38751/65307], Loss: 0.0650\n",
            "Epoch [1/2], Step [38801/65307], Loss: 0.1517\n",
            "Epoch [1/2], Step [38851/65307], Loss: 0.0648\n",
            "Epoch [1/2], Step [38901/65307], Loss: 0.1522\n",
            "Epoch [1/2], Step [38951/65307], Loss: 0.0305\n",
            "Epoch [1/2], Step [39001/65307], Loss: 0.1081\n",
            "Epoch [1/2], Step [39051/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [39101/65307], Loss: 0.0661\n",
            "Epoch [1/2], Step [39151/65307], Loss: 0.0905\n",
            "Epoch [1/2], Step [39201/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [39251/65307], Loss: 0.0848\n",
            "Epoch [1/2], Step [39301/65307], Loss: 0.0666\n",
            "Epoch [1/2], Step [39351/65307], Loss: 0.0257\n",
            "Epoch [1/2], Step [39401/65307], Loss: 0.0204\n",
            "Epoch [1/2], Step [39451/65307], Loss: 0.0110\n",
            "Epoch [1/2], Step [39501/65307], Loss: 0.2130\n",
            "Epoch [1/2], Step [39551/65307], Loss: 0.1123\n",
            "Epoch [1/2], Step [39601/65307], Loss: 0.0273\n",
            "Epoch [1/2], Step [39651/65307], Loss: 0.2615\n",
            "Epoch [1/2], Step [39701/65307], Loss: 0.0735\n",
            "Epoch [1/2], Step [39751/65307], Loss: 0.2336\n",
            "Epoch [1/2], Step [39801/65307], Loss: 0.0821\n",
            "Epoch [1/2], Step [39851/65307], Loss: 0.0468\n",
            "Epoch [1/2], Step [39901/65307], Loss: 0.2913\n",
            "Epoch [1/2], Step [39951/65307], Loss: 0.2797\n",
            "Epoch [1/2], Step [40001/65307], Loss: 0.2276\n",
            "Epoch [1/2], Step [40051/65307], Loss: 0.0232\n",
            "Epoch [1/2], Step [40101/65307], Loss: 0.0093\n",
            "Epoch [1/2], Step [40151/65307], Loss: 0.0493\n",
            "Epoch [1/2], Step [40201/65307], Loss: 0.0534\n",
            "Epoch [1/2], Step [40251/65307], Loss: 0.0820\n",
            "Epoch [1/2], Step [40301/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [40351/65307], Loss: 0.0907\n",
            "Epoch [1/2], Step [40401/65307], Loss: 0.0174\n",
            "Epoch [1/2], Step [40451/65307], Loss: 0.0070\n",
            "Epoch [1/2], Step [40501/65307], Loss: 0.1045\n",
            "Epoch [1/2], Step [40551/65307], Loss: 0.0051\n",
            "Epoch [1/2], Step [40601/65307], Loss: 0.1339\n",
            "Epoch [1/2], Step [40651/65307], Loss: 0.2458\n",
            "Epoch [1/2], Step [40701/65307], Loss: 0.0158\n",
            "Epoch [1/2], Step [40751/65307], Loss: 0.0782\n",
            "Epoch [1/2], Step [40801/65307], Loss: 0.1074\n",
            "Epoch [1/2], Step [40851/65307], Loss: 0.0513\n",
            "Epoch [1/2], Step [40901/65307], Loss: 0.0618\n",
            "Epoch [1/2], Step [40951/65307], Loss: 0.1497\n",
            "Epoch [1/2], Step [41001/65307], Loss: 0.0140\n",
            "Epoch [1/2], Step [41051/65307], Loss: 0.0638\n",
            "Epoch [1/2], Step [41101/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [41151/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [41201/65307], Loss: 0.0939\n",
            "Epoch [1/2], Step [41251/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [41301/65307], Loss: 0.0373\n",
            "Epoch [1/2], Step [41351/65307], Loss: 0.0817\n",
            "Epoch [1/2], Step [41401/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [41451/65307], Loss: 0.1814\n",
            "Epoch [1/2], Step [41501/65307], Loss: 0.0983\n",
            "Epoch [1/2], Step [41551/65307], Loss: 0.0352\n",
            "Epoch [1/2], Step [41601/65307], Loss: 0.0841\n",
            "Epoch [1/2], Step [41651/65307], Loss: 0.1380\n",
            "Epoch [1/2], Step [41701/65307], Loss: 0.0610\n",
            "Epoch [1/2], Step [41751/65307], Loss: 0.0302\n",
            "Epoch [1/2], Step [41801/65307], Loss: 0.2055\n",
            "Epoch [1/2], Step [41851/65307], Loss: 0.0753\n",
            "Epoch [1/2], Step [41901/65307], Loss: 0.1733\n",
            "Epoch [1/2], Step [41951/65307], Loss: 0.3131\n",
            "Epoch [1/2], Step [42001/65307], Loss: 0.1087\n",
            "Epoch [1/2], Step [42051/65307], Loss: 0.2709\n",
            "Epoch [1/2], Step [42101/65307], Loss: 0.0434\n",
            "Epoch [1/2], Step [42151/65307], Loss: 0.0699\n",
            "Epoch [1/2], Step [42201/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [42251/65307], Loss: 0.0409\n",
            "Epoch [1/2], Step [42301/65307], Loss: 0.0583\n",
            "Epoch [1/2], Step [42351/65307], Loss: 0.0178\n",
            "Epoch [1/2], Step [42401/65307], Loss: 0.0384\n",
            "Epoch [1/2], Step [42451/65307], Loss: 0.0238\n",
            "Epoch [1/2], Step [42501/65307], Loss: 0.1424\n",
            "Epoch [1/2], Step [42551/65307], Loss: 0.1117\n",
            "Epoch [1/2], Step [42601/65307], Loss: 0.0183\n",
            "Epoch [1/2], Step [42651/65307], Loss: 0.0564\n",
            "Epoch [1/2], Step [42701/65307], Loss: 0.0228\n",
            "Epoch [1/2], Step [42751/65307], Loss: 0.0974\n",
            "Epoch [1/2], Step [42801/65307], Loss: 0.0335\n",
            "Epoch [1/2], Step [42851/65307], Loss: 0.0803\n",
            "Epoch [1/2], Step [42901/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [42951/65307], Loss: 0.3228\n",
            "Epoch [1/2], Step [43001/65307], Loss: 0.1016\n",
            "Epoch [1/2], Step [43051/65307], Loss: 0.0975\n",
            "Epoch [1/2], Step [43101/65307], Loss: 0.1759\n",
            "Epoch [1/2], Step [43151/65307], Loss: 0.0745\n",
            "Epoch [1/2], Step [43201/65307], Loss: 0.0303\n",
            "Epoch [1/2], Step [43251/65307], Loss: 0.0459\n",
            "Epoch [1/2], Step [43301/65307], Loss: 0.0039\n",
            "Epoch [1/2], Step [43351/65307], Loss: 0.0649\n",
            "Epoch [1/2], Step [43401/65307], Loss: 0.0700\n",
            "Epoch [1/2], Step [43451/65307], Loss: 0.2015\n",
            "Epoch [1/2], Step [43501/65307], Loss: 0.0951\n",
            "Epoch [1/2], Step [43551/65307], Loss: 0.0298\n",
            "Epoch [1/2], Step [43601/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [43651/65307], Loss: 0.0067\n",
            "Epoch [1/2], Step [43701/65307], Loss: 0.0542\n",
            "Epoch [1/2], Step [43751/65307], Loss: 0.1461\n",
            "Epoch [1/2], Step [43801/65307], Loss: 0.2042\n",
            "Epoch [1/2], Step [43851/65307], Loss: 0.1613\n",
            "Epoch [1/2], Step [43901/65307], Loss: 0.1932\n",
            "Epoch [1/2], Step [43951/65307], Loss: 0.4691\n",
            "Epoch [1/2], Step [44001/65307], Loss: 0.0104\n",
            "Epoch [1/2], Step [44051/65307], Loss: 0.1663\n",
            "Epoch [1/2], Step [44101/65307], Loss: 0.2917\n",
            "Epoch [1/2], Step [44151/65307], Loss: 0.0920\n",
            "Epoch [1/2], Step [44201/65307], Loss: 0.0392\n",
            "Epoch [1/2], Step [44251/65307], Loss: 0.1448\n",
            "Epoch [1/2], Step [44301/65307], Loss: 0.0821\n",
            "Epoch [1/2], Step [44351/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [44401/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [44451/65307], Loss: 0.3250\n",
            "Epoch [1/2], Step [44501/65307], Loss: 0.0579\n",
            "Epoch [1/2], Step [44551/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [44601/65307], Loss: 0.1166\n",
            "Epoch [1/2], Step [44651/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [44701/65307], Loss: 0.0949\n",
            "Epoch [1/2], Step [44751/65307], Loss: 0.1171\n",
            "Epoch [1/2], Step [44801/65307], Loss: 0.3239\n",
            "Epoch [1/2], Step [44851/65307], Loss: 0.0831\n",
            "Epoch [1/2], Step [44901/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [44951/65307], Loss: 0.0622\n",
            "Epoch [1/2], Step [45001/65307], Loss: 0.2390\n",
            "Epoch [1/2], Step [45051/65307], Loss: 0.3536\n",
            "Epoch [1/2], Step [45101/65307], Loss: 0.0116\n",
            "Epoch [1/2], Step [45151/65307], Loss: 0.0268\n",
            "Epoch [1/2], Step [45201/65307], Loss: 0.1000\n",
            "Epoch [1/2], Step [45251/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [45301/65307], Loss: 0.1532\n",
            "Epoch [1/2], Step [45351/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [45401/65307], Loss: 0.0506\n",
            "Epoch [1/2], Step [45451/65307], Loss: 0.0379\n",
            "Epoch [1/2], Step [45501/65307], Loss: 0.1068\n",
            "Epoch [1/2], Step [45551/65307], Loss: 0.0439\n",
            "Epoch [1/2], Step [45601/65307], Loss: 0.2724\n",
            "Epoch [1/2], Step [45651/65307], Loss: 0.1056\n",
            "Epoch [1/2], Step [45701/65307], Loss: 0.2434\n",
            "Epoch [1/2], Step [45751/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [45801/65307], Loss: 0.0377\n",
            "Epoch [1/2], Step [45851/65307], Loss: 0.0058\n",
            "Epoch [1/2], Step [45901/65307], Loss: 0.0393\n",
            "Epoch [1/2], Step [45951/65307], Loss: 0.0767\n",
            "Epoch [1/2], Step [46001/65307], Loss: 0.0348\n",
            "Epoch [1/2], Step [46051/65307], Loss: 0.4867\n",
            "Epoch [1/2], Step [46101/65307], Loss: 0.1412\n",
            "Epoch [1/2], Step [46151/65307], Loss: 0.0526\n",
            "Epoch [1/2], Step [46201/65307], Loss: 0.0623\n",
            "Epoch [1/2], Step [46251/65307], Loss: 0.4883\n",
            "Epoch [1/2], Step [46301/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [46351/65307], Loss: 0.0248\n",
            "Epoch [1/2], Step [46401/65307], Loss: 0.5721\n",
            "Epoch [1/2], Step [46451/65307], Loss: 0.0181\n",
            "Epoch [1/2], Step [46501/65307], Loss: 0.2161\n",
            "Epoch [1/2], Step [46551/65307], Loss: 0.2673\n",
            "Epoch [1/2], Step [46601/65307], Loss: 0.0783\n",
            "Epoch [1/2], Step [46651/65307], Loss: 0.0988\n",
            "Epoch [1/2], Step [46701/65307], Loss: 0.1456\n",
            "Epoch [1/2], Step [46751/65307], Loss: 0.6606\n",
            "Epoch [1/2], Step [46801/65307], Loss: 0.0344\n",
            "Epoch [1/2], Step [46851/65307], Loss: 0.0787\n",
            "Epoch [1/2], Step [46901/65307], Loss: 0.0574\n",
            "Epoch [1/2], Step [46951/65307], Loss: 0.0973\n",
            "Epoch [1/2], Step [47001/65307], Loss: 0.1360\n",
            "Epoch [1/2], Step [47051/65307], Loss: 0.2656\n",
            "Epoch [1/2], Step [47101/65307], Loss: 0.0239\n",
            "Epoch [1/2], Step [47151/65307], Loss: 0.1734\n",
            "Epoch [1/2], Step [47201/65307], Loss: 0.0043\n",
            "Epoch [1/2], Step [47251/65307], Loss: 0.0153\n",
            "Epoch [1/2], Step [47301/65307], Loss: 0.4070\n",
            "Epoch [1/2], Step [47351/65307], Loss: 0.0191\n",
            "Epoch [1/2], Step [47401/65307], Loss: 0.0750\n",
            "Epoch [1/2], Step [47451/65307], Loss: 0.1455\n",
            "Epoch [1/2], Step [47501/65307], Loss: 0.2053\n",
            "Epoch [1/2], Step [47551/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [47601/65307], Loss: 0.0705\n",
            "Epoch [1/2], Step [47651/65307], Loss: 0.1784\n",
            "Epoch [1/2], Step [47701/65307], Loss: 0.1575\n",
            "Epoch [1/2], Step [47751/65307], Loss: 0.1772\n",
            "Epoch [1/2], Step [47801/65307], Loss: 0.0025\n",
            "Epoch [1/2], Step [47851/65307], Loss: 0.0657\n",
            "Epoch [1/2], Step [47901/65307], Loss: 0.0099\n",
            "Epoch [1/2], Step [47951/65307], Loss: 0.0059\n",
            "Epoch [1/2], Step [48001/65307], Loss: 0.1327\n",
            "Epoch [1/2], Step [48051/65307], Loss: 0.2552\n",
            "Epoch [1/2], Step [48101/65307], Loss: 0.1373\n",
            "Epoch [1/2], Step [48151/65307], Loss: 0.0166\n",
            "Epoch [1/2], Step [48201/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [48251/65307], Loss: 0.0233\n",
            "Epoch [1/2], Step [48301/65307], Loss: 0.0170\n",
            "Epoch [1/2], Step [48351/65307], Loss: 0.1493\n",
            "Epoch [1/2], Step [48401/65307], Loss: 0.4541\n",
            "Epoch [1/2], Step [48451/65307], Loss: 0.0785\n",
            "Epoch [1/2], Step [48501/65307], Loss: 0.1894\n",
            "Epoch [1/2], Step [48551/65307], Loss: 0.0631\n",
            "Epoch [1/2], Step [48601/65307], Loss: 0.0500\n",
            "Epoch [1/2], Step [48651/65307], Loss: 0.1693\n",
            "Epoch [1/2], Step [48701/65307], Loss: 0.0724\n",
            "Epoch [1/2], Step [48751/65307], Loss: 0.0134\n",
            "Epoch [1/2], Step [48801/65307], Loss: 0.0351\n",
            "Epoch [1/2], Step [48851/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [48901/65307], Loss: 0.0387\n",
            "Epoch [1/2], Step [48951/65307], Loss: 0.0212\n",
            "Epoch [1/2], Step [49001/65307], Loss: 0.0495\n",
            "Epoch [1/2], Step [49051/65307], Loss: 0.1146\n",
            "Epoch [1/2], Step [49101/65307], Loss: 0.0563\n",
            "Epoch [1/2], Step [49151/65307], Loss: 0.2681\n",
            "Epoch [1/2], Step [49201/65307], Loss: 0.0363\n",
            "Epoch [1/2], Step [49251/65307], Loss: 0.1086\n",
            "Epoch [1/2], Step [49301/65307], Loss: 0.1559\n",
            "Epoch [1/2], Step [49351/65307], Loss: 0.1137\n",
            "Epoch [1/2], Step [49401/65307], Loss: 0.2568\n",
            "Epoch [1/2], Step [49451/65307], Loss: 0.1089\n",
            "Epoch [1/2], Step [49501/65307], Loss: 0.1171\n",
            "Epoch [1/2], Step [49551/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [49601/65307], Loss: 0.1411\n",
            "Epoch [1/2], Step [49651/65307], Loss: 0.0677\n",
            "Epoch [1/2], Step [49701/65307], Loss: 0.0328\n",
            "Epoch [1/2], Step [49751/65307], Loss: 0.0519\n",
            "Epoch [1/2], Step [49801/65307], Loss: 0.1337\n",
            "Epoch [1/2], Step [49851/65307], Loss: 0.0744\n",
            "Epoch [1/2], Step [49901/65307], Loss: 0.0042\n",
            "Epoch [1/2], Step [49951/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [50001/65307], Loss: 0.0734\n",
            "Epoch [1/2], Step [50051/65307], Loss: 0.0310\n",
            "Epoch [1/2], Step [50101/65307], Loss: 0.0146\n",
            "Epoch [1/2], Step [50151/65307], Loss: 0.3077\n",
            "Epoch [1/2], Step [50201/65307], Loss: 0.0585\n",
            "Epoch [1/2], Step [50251/65307], Loss: 0.1695\n",
            "Epoch [1/2], Step [50301/65307], Loss: 0.0776\n",
            "Epoch [1/2], Step [50351/65307], Loss: 0.1190\n",
            "Epoch [1/2], Step [50401/65307], Loss: 0.1299\n",
            "Epoch [1/2], Step [50451/65307], Loss: 0.0934\n",
            "Epoch [1/2], Step [50501/65307], Loss: 0.0961\n",
            "Epoch [1/2], Step [50551/65307], Loss: 0.1630\n",
            "Epoch [1/2], Step [50601/65307], Loss: 0.0815\n",
            "Epoch [1/2], Step [50651/65307], Loss: 0.4115\n",
            "Epoch [1/2], Step [50701/65307], Loss: 0.0690\n",
            "Epoch [1/2], Step [50751/65307], Loss: 0.0214\n",
            "Epoch [1/2], Step [50801/65307], Loss: 0.0218\n",
            "Epoch [1/2], Step [50851/65307], Loss: 0.0026\n",
            "Epoch [1/2], Step [50901/65307], Loss: 0.1719\n",
            "Epoch [1/2], Step [50951/65307], Loss: 0.0596\n",
            "Epoch [1/2], Step [51001/65307], Loss: 0.0934\n",
            "Epoch [1/2], Step [51051/65307], Loss: 0.0787\n",
            "Epoch [1/2], Step [51101/65307], Loss: 0.1560\n",
            "Epoch [1/2], Step [51151/65307], Loss: 0.0274\n",
            "Epoch [1/2], Step [51201/65307], Loss: 0.3629\n",
            "Epoch [1/2], Step [51251/65307], Loss: 0.0118\n",
            "Epoch [1/2], Step [51301/65307], Loss: 0.1963\n",
            "Epoch [1/2], Step [51351/65307], Loss: 0.1266\n",
            "Epoch [1/2], Step [51401/65307], Loss: 0.2185\n",
            "Epoch [1/2], Step [51451/65307], Loss: 0.0201\n",
            "Epoch [1/2], Step [51501/65307], Loss: 0.0573\n",
            "Epoch [1/2], Step [51551/65307], Loss: 0.0034\n",
            "Epoch [1/2], Step [51601/65307], Loss: 0.0283\n",
            "Epoch [1/2], Step [51651/65307], Loss: 0.0606\n",
            "Epoch [1/2], Step [51701/65307], Loss: 0.2183\n",
            "Epoch [1/2], Step [51751/65307], Loss: 0.0565\n",
            "Epoch [1/2], Step [51801/65307], Loss: 0.0255\n",
            "Epoch [1/2], Step [51851/65307], Loss: 0.2476\n",
            "Epoch [1/2], Step [51901/65307], Loss: 0.0409\n",
            "Epoch [1/2], Step [51951/65307], Loss: 0.0061\n",
            "Epoch [1/2], Step [52001/65307], Loss: 0.1112\n",
            "Epoch [1/2], Step [52051/65307], Loss: 0.0202\n",
            "Epoch [1/2], Step [52101/65307], Loss: 0.1430\n",
            "Epoch [1/2], Step [52151/65307], Loss: 0.1691\n",
            "Epoch [1/2], Step [52201/65307], Loss: 0.0679\n",
            "Epoch [1/2], Step [52251/65307], Loss: 0.4666\n",
            "Epoch [1/2], Step [52301/65307], Loss: 0.1023\n",
            "Epoch [1/2], Step [52351/65307], Loss: 0.1540\n",
            "Epoch [1/2], Step [52401/65307], Loss: 0.0695\n",
            "Epoch [1/2], Step [52451/65307], Loss: 0.2607\n",
            "Epoch [1/2], Step [52501/65307], Loss: 0.0715\n",
            "Epoch [1/2], Step [52551/65307], Loss: 0.0147\n",
            "Epoch [1/2], Step [52601/65307], Loss: 0.2109\n",
            "Epoch [1/2], Step [52651/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [52701/65307], Loss: 0.0410\n",
            "Epoch [1/2], Step [52751/65307], Loss: 0.1210\n",
            "Epoch [1/2], Step [52801/65307], Loss: 0.0300\n",
            "Epoch [1/2], Step [52851/65307], Loss: 0.0269\n",
            "Epoch [1/2], Step [52901/65307], Loss: 0.0348\n",
            "Epoch [1/2], Step [52951/65307], Loss: 0.0697\n",
            "Epoch [1/2], Step [53001/65307], Loss: 0.1125\n",
            "Epoch [1/2], Step [53051/65307], Loss: 0.0836\n",
            "Epoch [1/2], Step [53101/65307], Loss: 0.2255\n",
            "Epoch [1/2], Step [53151/65307], Loss: 0.0877\n",
            "Epoch [1/2], Step [53201/65307], Loss: 0.1019\n",
            "Epoch [1/2], Step [53251/65307], Loss: 0.0619\n",
            "Epoch [1/2], Step [53301/65307], Loss: 0.0322\n",
            "Epoch [1/2], Step [53351/65307], Loss: 0.1752\n",
            "Epoch [1/2], Step [53401/65307], Loss: 0.0488\n",
            "Epoch [1/2], Step [53451/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [53501/65307], Loss: 0.2029\n",
            "Epoch [1/2], Step [53551/65307], Loss: 0.0321\n",
            "Epoch [1/2], Step [53601/65307], Loss: 0.0479\n",
            "Epoch [1/2], Step [53651/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [53701/65307], Loss: 0.0589\n",
            "Epoch [1/2], Step [53751/65307], Loss: 0.0033\n",
            "Epoch [1/2], Step [53801/65307], Loss: 0.0365\n",
            "Epoch [1/2], Step [53851/65307], Loss: 0.1604\n",
            "Epoch [1/2], Step [53901/65307], Loss: 0.0295\n",
            "Epoch [1/2], Step [53951/65307], Loss: 0.0456\n",
            "Epoch [1/2], Step [54001/65307], Loss: 0.0208\n",
            "Epoch [1/2], Step [54051/65307], Loss: 0.1910\n",
            "Epoch [1/2], Step [54101/65307], Loss: 0.0146\n",
            "Epoch [1/2], Step [54151/65307], Loss: 0.1049\n",
            "Epoch [1/2], Step [54201/65307], Loss: 0.0065\n",
            "Epoch [1/2], Step [54251/65307], Loss: 0.0241\n",
            "Epoch [1/2], Step [54301/65307], Loss: 0.1727\n",
            "Epoch [1/2], Step [54351/65307], Loss: 0.0043\n",
            "Epoch [1/2], Step [54401/65307], Loss: 0.0451\n",
            "Epoch [1/2], Step [54451/65307], Loss: 0.1514\n",
            "Epoch [1/2], Step [54501/65307], Loss: 0.0262\n",
            "Epoch [1/2], Step [54551/65307], Loss: 0.0030\n",
            "Epoch [1/2], Step [54601/65307], Loss: 0.3719\n",
            "Epoch [1/2], Step [54651/65307], Loss: 0.1186\n",
            "Epoch [1/2], Step [54701/65307], Loss: 0.0084\n",
            "Epoch [1/2], Step [54751/65307], Loss: 0.3882\n",
            "Epoch [1/2], Step [54801/65307], Loss: 0.0326\n",
            "Epoch [1/2], Step [54851/65307], Loss: 0.0362\n",
            "Epoch [1/2], Step [54901/65307], Loss: 0.0709\n",
            "Epoch [1/2], Step [54951/65307], Loss: 0.0204\n",
            "Epoch [1/2], Step [55001/65307], Loss: 0.0127\n",
            "Epoch [1/2], Step [55051/65307], Loss: 0.2503\n",
            "Epoch [1/2], Step [55101/65307], Loss: 0.0371\n",
            "Epoch [1/2], Step [55151/65307], Loss: 0.0503\n",
            "Epoch [1/2], Step [55201/65307], Loss: 0.0016\n",
            "Epoch [1/2], Step [55251/65307], Loss: 0.2772\n",
            "Epoch [1/2], Step [55301/65307], Loss: 0.0085\n",
            "Epoch [1/2], Step [55351/65307], Loss: 0.0313\n",
            "Epoch [1/2], Step [55401/65307], Loss: 0.0373\n",
            "Epoch [1/2], Step [55451/65307], Loss: 0.1217\n",
            "Epoch [1/2], Step [55501/65307], Loss: 0.0505\n",
            "Epoch [1/2], Step [55551/65307], Loss: 0.0779\n",
            "Epoch [1/2], Step [55601/65307], Loss: 0.4122\n",
            "Epoch [1/2], Step [55651/65307], Loss: 0.0969\n",
            "Epoch [1/2], Step [55701/65307], Loss: 0.0249\n",
            "Epoch [1/2], Step [55751/65307], Loss: 0.0052\n",
            "Epoch [1/2], Step [55801/65307], Loss: 0.1545\n",
            "Epoch [1/2], Step [55851/65307], Loss: 0.1711\n",
            "Epoch [1/2], Step [55901/65307], Loss: 0.0567\n",
            "Epoch [1/2], Step [55951/65307], Loss: 0.0844\n",
            "Epoch [1/2], Step [56001/65307], Loss: 0.0295\n",
            "Epoch [1/2], Step [56051/65307], Loss: 0.0353\n",
            "Epoch [1/2], Step [56101/65307], Loss: 0.0899\n",
            "Epoch [1/2], Step [56151/65307], Loss: 0.1479\n",
            "Epoch [1/2], Step [56201/65307], Loss: 0.0297\n",
            "Epoch [1/2], Step [56251/65307], Loss: 0.2369\n",
            "Epoch [1/2], Step [56301/65307], Loss: 0.0135\n",
            "Epoch [1/2], Step [56351/65307], Loss: 0.0053\n",
            "Epoch [1/2], Step [56401/65307], Loss: 0.0026\n",
            "Epoch [1/2], Step [56451/65307], Loss: 0.0151\n",
            "Epoch [1/2], Step [56501/65307], Loss: 0.0537\n",
            "Epoch [1/2], Step [56551/65307], Loss: 0.0725\n",
            "Epoch [1/2], Step [56601/65307], Loss: 0.0445\n",
            "Epoch [1/2], Step [56651/65307], Loss: 0.1785\n",
            "Epoch [1/2], Step [56701/65307], Loss: 0.1273\n",
            "Epoch [1/2], Step [56751/65307], Loss: 0.0465\n",
            "Epoch [1/2], Step [56801/65307], Loss: 0.0040\n",
            "Epoch [1/2], Step [56851/65307], Loss: 0.0266\n",
            "Epoch [1/2], Step [56901/65307], Loss: 0.0227\n",
            "Epoch [1/2], Step [56951/65307], Loss: 0.3006\n",
            "Epoch [1/2], Step [57001/65307], Loss: 0.2209\n",
            "Epoch [1/2], Step [57051/65307], Loss: 0.1704\n",
            "Epoch [1/2], Step [57101/65307], Loss: 0.0063\n",
            "Epoch [1/2], Step [57151/65307], Loss: 0.0760\n",
            "Epoch [1/2], Step [57201/65307], Loss: 0.1544\n",
            "Epoch [1/2], Step [57251/65307], Loss: 0.0206\n",
            "Epoch [1/2], Step [57301/65307], Loss: 0.0388\n",
            "Epoch [1/2], Step [57351/65307], Loss: 0.0286\n",
            "Epoch [1/2], Step [57401/65307], Loss: 0.2511\n",
            "Epoch [1/2], Step [57451/65307], Loss: 0.1214\n",
            "Epoch [1/2], Step [57501/65307], Loss: 0.0617\n",
            "Epoch [1/2], Step [57551/65307], Loss: 0.1573\n",
            "Epoch [1/2], Step [57601/65307], Loss: 0.0440\n",
            "Epoch [1/2], Step [57651/65307], Loss: 0.2066\n",
            "Epoch [1/2], Step [57701/65307], Loss: 0.1257\n",
            "Epoch [1/2], Step [57751/65307], Loss: 0.0083\n",
            "Epoch [1/2], Step [57801/65307], Loss: 0.0402\n",
            "Epoch [1/2], Step [57851/65307], Loss: 0.0111\n",
            "Epoch [1/2], Step [57901/65307], Loss: 0.3273\n",
            "Epoch [1/2], Step [57951/65307], Loss: 0.0329\n",
            "Epoch [1/2], Step [58001/65307], Loss: 0.0930\n",
            "Epoch [1/2], Step [58051/65307], Loss: 0.0538\n",
            "Epoch [1/2], Step [58101/65307], Loss: 0.0944\n",
            "Epoch [1/2], Step [58151/65307], Loss: 0.4440\n",
            "Epoch [1/2], Step [58201/65307], Loss: 0.1406\n",
            "Epoch [1/2], Step [58251/65307], Loss: 0.0695\n",
            "Epoch [1/2], Step [58301/65307], Loss: 0.0036\n",
            "Epoch [1/2], Step [58351/65307], Loss: 0.1516\n",
            "Epoch [1/2], Step [58401/65307], Loss: 0.2912\n",
            "Epoch [1/2], Step [58451/65307], Loss: 0.1621\n",
            "Epoch [1/2], Step [58501/65307], Loss: 0.0759\n",
            "Epoch [1/2], Step [58551/65307], Loss: 0.0038\n",
            "Epoch [1/2], Step [58601/65307], Loss: 0.1884\n",
            "Epoch [1/2], Step [58651/65307], Loss: 0.2023\n",
            "Epoch [1/2], Step [58701/65307], Loss: 0.0824\n",
            "Epoch [1/2], Step [58751/65307], Loss: 0.0475\n",
            "Epoch [1/2], Step [58801/65307], Loss: 0.1632\n",
            "Epoch [1/2], Step [58851/65307], Loss: 0.0244\n",
            "Epoch [1/2], Step [58901/65307], Loss: 0.0886\n",
            "Epoch [1/2], Step [58951/65307], Loss: 0.1873\n",
            "Epoch [1/2], Step [59001/65307], Loss: 0.2068\n",
            "Epoch [1/2], Step [59051/65307], Loss: 0.0018\n",
            "Epoch [1/2], Step [59101/65307], Loss: 0.0845\n",
            "Epoch [1/2], Step [59151/65307], Loss: 0.0709\n",
            "Epoch [1/2], Step [59201/65307], Loss: 0.0393\n",
            "Epoch [1/2], Step [59251/65307], Loss: 0.2321\n",
            "Epoch [1/2], Step [59301/65307], Loss: 0.1777\n",
            "Epoch [1/2], Step [59351/65307], Loss: 0.2487\n",
            "Epoch [1/2], Step [59401/65307], Loss: 0.0498\n",
            "Epoch [1/2], Step [59451/65307], Loss: 0.0674\n",
            "Epoch [1/2], Step [59501/65307], Loss: 0.2094\n",
            "Epoch [1/2], Step [59551/65307], Loss: 0.1210\n",
            "Epoch [1/2], Step [59601/65307], Loss: 0.0771\n",
            "Epoch [1/2], Step [59651/65307], Loss: 0.0278\n",
            "Epoch [1/2], Step [59701/65307], Loss: 0.0220\n",
            "Epoch [1/2], Step [59751/65307], Loss: 0.1411\n",
            "Epoch [1/2], Step [59801/65307], Loss: 0.0024\n",
            "Epoch [1/2], Step [59851/65307], Loss: 0.0294\n",
            "Epoch [1/2], Step [59901/65307], Loss: 0.0745\n",
            "Epoch [1/2], Step [59951/65307], Loss: 0.0730\n",
            "Epoch [1/2], Step [60001/65307], Loss: 0.1217\n",
            "Epoch [1/2], Step [60051/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [60101/65307], Loss: 0.0607\n",
            "Epoch [1/2], Step [60151/65307], Loss: 0.0019\n",
            "Epoch [1/2], Step [60201/65307], Loss: 0.0399\n",
            "Epoch [1/2], Step [60251/65307], Loss: 0.0138\n",
            "Epoch [1/2], Step [60301/65307], Loss: 0.1259\n",
            "Epoch [1/2], Step [60351/65307], Loss: 0.2423\n",
            "Epoch [1/2], Step [60401/65307], Loss: 0.0371\n",
            "Epoch [1/2], Step [60451/65307], Loss: 0.3240\n",
            "Epoch [1/2], Step [60501/65307], Loss: 0.0072\n",
            "Epoch [1/2], Step [60551/65307], Loss: 0.1274\n",
            "Epoch [1/2], Step [60601/65307], Loss: 0.0243\n",
            "Epoch [1/2], Step [60651/65307], Loss: 0.8457\n",
            "Epoch [1/2], Step [60701/65307], Loss: 0.3522\n",
            "Epoch [1/2], Step [60751/65307], Loss: 0.0230\n",
            "Epoch [1/2], Step [60801/65307], Loss: 0.0544\n",
            "Epoch [1/2], Step [60851/65307], Loss: 0.1606\n",
            "Epoch [1/2], Step [60901/65307], Loss: 0.0965\n",
            "Epoch [1/2], Step [60951/65307], Loss: 0.0089\n",
            "Epoch [1/2], Step [61001/65307], Loss: 0.0623\n",
            "Epoch [1/2], Step [61051/65307], Loss: 0.0035\n",
            "Epoch [1/2], Step [61101/65307], Loss: 0.1218\n",
            "Epoch [1/2], Step [61151/65307], Loss: 0.0151\n",
            "Epoch [1/2], Step [61201/65307], Loss: 0.0059\n",
            "Epoch [1/2], Step [61251/65307], Loss: 0.0553\n",
            "Epoch [1/2], Step [61301/65307], Loss: 0.0579\n",
            "Epoch [1/2], Step [61351/65307], Loss: 0.4781\n",
            "Epoch [1/2], Step [61401/65307], Loss: 0.0397\n",
            "Epoch [1/2], Step [61451/65307], Loss: 0.0174\n",
            "Epoch [1/2], Step [61501/65307], Loss: 0.0417\n",
            "Epoch [1/2], Step [61551/65307], Loss: 0.0041\n",
            "Epoch [1/2], Step [61601/65307], Loss: 0.2606\n",
            "Epoch [1/2], Step [61651/65307], Loss: 0.0433\n",
            "Epoch [1/2], Step [61701/65307], Loss: 0.0139\n",
            "Epoch [1/2], Step [61751/65307], Loss: 0.0096\n",
            "Epoch [1/2], Step [61801/65307], Loss: 0.0027\n",
            "Epoch [1/2], Step [61851/65307], Loss: 0.1125\n",
            "Epoch [1/2], Step [61901/65307], Loss: 0.4560\n",
            "Epoch [1/2], Step [61951/65307], Loss: 0.0950\n",
            "Epoch [1/2], Step [62001/65307], Loss: 0.0021\n",
            "Epoch [1/2], Step [62051/65307], Loss: 0.0141\n",
            "Epoch [1/2], Step [62101/65307], Loss: 0.0904\n",
            "Epoch [1/2], Step [62151/65307], Loss: 0.0546\n",
            "Epoch [1/2], Step [62201/65307], Loss: 0.0309\n",
            "Epoch [1/2], Step [62251/65307], Loss: 0.0189\n",
            "Epoch [1/2], Step [62301/65307], Loss: 0.0422\n",
            "Epoch [1/2], Step [62351/65307], Loss: 0.0284\n",
            "Epoch [1/2], Step [62401/65307], Loss: 0.0144\n",
            "Epoch [1/2], Step [62451/65307], Loss: 0.0366\n",
            "Epoch [1/2], Step [62501/65307], Loss: 0.5770\n",
            "Epoch [1/2], Step [62551/65307], Loss: 0.0079\n",
            "Epoch [1/2], Step [62601/65307], Loss: 0.1332\n",
            "Epoch [1/2], Step [62651/65307], Loss: 0.0234\n",
            "Epoch [1/2], Step [62701/65307], Loss: 0.0407\n",
            "Epoch [1/2], Step [62751/65307], Loss: 0.2956\n",
            "Epoch [1/2], Step [62801/65307], Loss: 0.0171\n",
            "Epoch [1/2], Step [62851/65307], Loss: 0.1624\n",
            "Epoch [1/2], Step [62901/65307], Loss: 0.0210\n",
            "Epoch [1/2], Step [62951/65307], Loss: 0.3939\n",
            "Epoch [1/2], Step [63001/65307], Loss: 0.0800\n",
            "Epoch [1/2], Step [63051/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [63101/65307], Loss: 0.0553\n",
            "Epoch [1/2], Step [63151/65307], Loss: 0.1223\n",
            "Epoch [1/2], Step [63201/65307], Loss: 0.0155\n",
            "Epoch [1/2], Step [63251/65307], Loss: 0.2605\n",
            "Epoch [1/2], Step [63301/65307], Loss: 0.3256\n",
            "Epoch [1/2], Step [63351/65307], Loss: 0.0038\n",
            "Epoch [1/2], Step [63401/65307], Loss: 0.0275\n",
            "Epoch [1/2], Step [63451/65307], Loss: 0.0056\n",
            "Epoch [1/2], Step [63501/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [63551/65307], Loss: 0.0051\n",
            "Epoch [1/2], Step [63601/65307], Loss: 0.2283\n",
            "Epoch [1/2], Step [63651/65307], Loss: 0.0952\n",
            "Epoch [1/2], Step [63701/65307], Loss: 0.0792\n",
            "Epoch [1/2], Step [63751/65307], Loss: 0.2192\n",
            "Epoch [1/2], Step [63801/65307], Loss: 0.3167\n",
            "Epoch [1/2], Step [63851/65307], Loss: 0.2534\n",
            "Epoch [1/2], Step [63901/65307], Loss: 0.0098\n",
            "Epoch [1/2], Step [63951/65307], Loss: 0.0102\n",
            "Epoch [1/2], Step [64001/65307], Loss: 0.1512\n",
            "Epoch [1/2], Step [64051/65307], Loss: 0.0742\n",
            "Epoch [1/2], Step [64101/65307], Loss: 0.0840\n",
            "Epoch [1/2], Step [64151/65307], Loss: 0.0064\n",
            "Epoch [1/2], Step [64201/65307], Loss: 0.0290\n",
            "Epoch [1/2], Step [64251/65307], Loss: 0.1230\n",
            "Epoch [1/2], Step [64301/65307], Loss: 0.0163\n",
            "Epoch [1/2], Step [64351/65307], Loss: 0.2260\n",
            "Epoch [1/2], Step [64401/65307], Loss: 0.0417\n",
            "Epoch [1/2], Step [64451/65307], Loss: 0.2053\n",
            "Epoch [1/2], Step [64501/65307], Loss: 0.1926\n",
            "Epoch [1/2], Step [64551/65307], Loss: 0.0179\n",
            "Epoch [1/2], Step [64601/65307], Loss: 0.1384\n",
            "Epoch [1/2], Step [64651/65307], Loss: 0.0816\n",
            "Epoch [1/2], Step [64701/65307], Loss: 0.0590\n",
            "Epoch [1/2], Step [64751/65307], Loss: 0.1042\n",
            "Epoch [1/2], Step [64801/65307], Loss: 0.0258\n",
            "Epoch [1/2], Step [64851/65307], Loss: 0.0271\n",
            "Epoch [1/2], Step [64901/65307], Loss: 0.1064\n",
            "Epoch [1/2], Step [64951/65307], Loss: 0.0588\n",
            "Epoch [1/2], Step [65001/65307], Loss: 0.0092\n",
            "Epoch [1/2], Step [65051/65307], Loss: 0.0900\n",
            "Epoch [1/2], Step [65101/65307], Loss: 0.0415\n",
            "Epoch [1/2], Step [65151/65307], Loss: 0.1000\n",
            "Epoch [1/2], Step [65201/65307], Loss: 0.0143\n",
            "Epoch [1/2], Step [65251/65307], Loss: 0.1091\n",
            "Epoch [1/2], Step [65301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [1/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [51/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [101/65307], Loss: 0.1922\n",
            "Epoch [2/2], Step [151/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [201/65307], Loss: 0.0339\n",
            "Epoch [2/2], Step [251/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [301/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [351/65307], Loss: 0.3302\n",
            "Epoch [2/2], Step [401/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [451/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [501/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [551/65307], Loss: 0.3201\n",
            "Epoch [2/2], Step [601/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [651/65307], Loss: 0.0659\n",
            "Epoch [2/2], Step [701/65307], Loss: 0.0328\n",
            "Epoch [2/2], Step [751/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [801/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [851/65307], Loss: 0.1465\n",
            "Epoch [2/2], Step [901/65307], Loss: 0.0929\n",
            "Epoch [2/2], Step [951/65307], Loss: 0.0388\n",
            "Epoch [2/2], Step [1001/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [1051/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [1101/65307], Loss: 0.0331\n",
            "Epoch [2/2], Step [1151/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [1201/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [1251/65307], Loss: 0.0314\n",
            "Epoch [2/2], Step [1301/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [1351/65307], Loss: 0.0263\n",
            "Epoch [2/2], Step [1401/65307], Loss: 0.0985\n",
            "Epoch [2/2], Step [1451/65307], Loss: 0.1664\n",
            "Epoch [2/2], Step [1501/65307], Loss: 0.0572\n",
            "Epoch [2/2], Step [1551/65307], Loss: 0.3268\n",
            "Epoch [2/2], Step [1601/65307], Loss: 0.0126\n",
            "Epoch [2/2], Step [1651/65307], Loss: 0.0563\n",
            "Epoch [2/2], Step [1701/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [1751/65307], Loss: 0.1266\n",
            "Epoch [2/2], Step [1801/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [1851/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [1901/65307], Loss: 0.0193\n",
            "Epoch [2/2], Step [1951/65307], Loss: 0.1068\n",
            "Epoch [2/2], Step [2001/65307], Loss: 0.2647\n",
            "Epoch [2/2], Step [2051/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [2101/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [2151/65307], Loss: 0.0482\n",
            "Epoch [2/2], Step [2201/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [2251/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [2301/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [2351/65307], Loss: 0.4913\n",
            "Epoch [2/2], Step [2401/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [2451/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [2501/65307], Loss: 0.3364\n",
            "Epoch [2/2], Step [2551/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [2601/65307], Loss: 0.1932\n",
            "Epoch [2/2], Step [2651/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [2701/65307], Loss: 0.0503\n",
            "Epoch [2/2], Step [2751/65307], Loss: 0.0874\n",
            "Epoch [2/2], Step [2801/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [2851/65307], Loss: 0.0890\n",
            "Epoch [2/2], Step [2901/65307], Loss: 0.0736\n",
            "Epoch [2/2], Step [2951/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [3001/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [3051/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [3101/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [3151/65307], Loss: 0.2131\n",
            "Epoch [2/2], Step [3201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [3251/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [3301/65307], Loss: 0.3098\n",
            "Epoch [2/2], Step [3351/65307], Loss: 0.0633\n",
            "Epoch [2/2], Step [3401/65307], Loss: 0.1278\n",
            "Epoch [2/2], Step [3451/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [3501/65307], Loss: 0.1219\n",
            "Epoch [2/2], Step [3551/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [3601/65307], Loss: 0.0269\n",
            "Epoch [2/2], Step [3651/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [3701/65307], Loss: 0.1225\n",
            "Epoch [2/2], Step [3751/65307], Loss: 0.2270\n",
            "Epoch [2/2], Step [3801/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [3851/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [3901/65307], Loss: 0.1383\n",
            "Epoch [2/2], Step [3951/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [4001/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [4051/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [4101/65307], Loss: 0.0131\n",
            "Epoch [2/2], Step [4151/65307], Loss: 0.0217\n",
            "Epoch [2/2], Step [4201/65307], Loss: 0.1036\n",
            "Epoch [2/2], Step [4251/65307], Loss: 0.0687\n",
            "Epoch [2/2], Step [4301/65307], Loss: 0.0873\n",
            "Epoch [2/2], Step [4351/65307], Loss: 0.0868\n",
            "Epoch [2/2], Step [4401/65307], Loss: 0.1172\n",
            "Epoch [2/2], Step [4451/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [4501/65307], Loss: 0.1841\n",
            "Epoch [2/2], Step [4551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [4601/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [4651/65307], Loss: 0.0413\n",
            "Epoch [2/2], Step [4701/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [4751/65307], Loss: 0.1318\n",
            "Epoch [2/2], Step [4801/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [4851/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [4901/65307], Loss: 0.0780\n",
            "Epoch [2/2], Step [4951/65307], Loss: 0.0171\n",
            "Epoch [2/2], Step [5001/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [5051/65307], Loss: 0.2719\n",
            "Epoch [2/2], Step [5101/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [5151/65307], Loss: 0.2240\n",
            "Epoch [2/2], Step [5201/65307], Loss: 0.0299\n",
            "Epoch [2/2], Step [5251/65307], Loss: 0.0622\n",
            "Epoch [2/2], Step [5301/65307], Loss: 0.2012\n",
            "Epoch [2/2], Step [5351/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [5401/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [5451/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [5501/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [5551/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [5601/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [5651/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [5701/65307], Loss: 0.0029\n",
            "Epoch [2/2], Step [5751/65307], Loss: 0.3523\n",
            "Epoch [2/2], Step [5801/65307], Loss: 0.0131\n",
            "Epoch [2/2], Step [5851/65307], Loss: 0.3471\n",
            "Epoch [2/2], Step [5901/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [5951/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [6001/65307], Loss: 0.0669\n",
            "Epoch [2/2], Step [6051/65307], Loss: 0.1431\n",
            "Epoch [2/2], Step [6101/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [6151/65307], Loss: 0.0211\n",
            "Epoch [2/2], Step [6201/65307], Loss: 0.2423\n",
            "Epoch [2/2], Step [6251/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [6301/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [6351/65307], Loss: 0.0047\n",
            "Epoch [2/2], Step [6401/65307], Loss: 0.0335\n",
            "Epoch [2/2], Step [6451/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [6501/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [6551/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [6601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [6651/65307], Loss: 0.1236\n",
            "Epoch [2/2], Step [6701/65307], Loss: 0.0359\n",
            "Epoch [2/2], Step [6751/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [6801/65307], Loss: 0.2053\n",
            "Epoch [2/2], Step [6851/65307], Loss: 0.0045\n",
            "Epoch [2/2], Step [6901/65307], Loss: 0.0375\n",
            "Epoch [2/2], Step [6951/65307], Loss: 0.3081\n",
            "Epoch [2/2], Step [7001/65307], Loss: 0.1964\n",
            "Epoch [2/2], Step [7051/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [7101/65307], Loss: 0.2859\n",
            "Epoch [2/2], Step [7151/65307], Loss: 0.1209\n",
            "Epoch [2/2], Step [7201/65307], Loss: 0.5270\n",
            "Epoch [2/2], Step [7251/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [7301/65307], Loss: 0.2146\n",
            "Epoch [2/2], Step [7351/65307], Loss: 0.0510\n",
            "Epoch [2/2], Step [7401/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [7451/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [7501/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [7551/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [7601/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [7651/65307], Loss: 0.1990\n",
            "Epoch [2/2], Step [7701/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [7751/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [7801/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [7851/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [7901/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [7951/65307], Loss: 0.0841\n",
            "Epoch [2/2], Step [8001/65307], Loss: 0.0592\n",
            "Epoch [2/2], Step [8051/65307], Loss: 0.0874\n",
            "Epoch [2/2], Step [8101/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [8151/65307], Loss: 0.0338\n",
            "Epoch [2/2], Step [8201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [8251/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [8301/65307], Loss: 0.0527\n",
            "Epoch [2/2], Step [8351/65307], Loss: 0.3808\n",
            "Epoch [2/2], Step [8401/65307], Loss: 0.0944\n",
            "Epoch [2/2], Step [8451/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [8501/65307], Loss: 0.0703\n",
            "Epoch [2/2], Step [8551/65307], Loss: 0.2920\n",
            "Epoch [2/2], Step [8601/65307], Loss: 0.0576\n",
            "Epoch [2/2], Step [8651/65307], Loss: 0.0087\n",
            "Epoch [2/2], Step [8701/65307], Loss: 0.2364\n",
            "Epoch [2/2], Step [8751/65307], Loss: 0.1117\n",
            "Epoch [2/2], Step [8801/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [8851/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [8901/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [8951/65307], Loss: 0.0888\n",
            "Epoch [2/2], Step [9001/65307], Loss: 0.1832\n",
            "Epoch [2/2], Step [9051/65307], Loss: 0.0723\n",
            "Epoch [2/2], Step [9101/65307], Loss: 0.0648\n",
            "Epoch [2/2], Step [9151/65307], Loss: 0.1219\n",
            "Epoch [2/2], Step [9201/65307], Loss: 0.0331\n",
            "Epoch [2/2], Step [9251/65307], Loss: 0.1834\n",
            "Epoch [2/2], Step [9301/65307], Loss: 0.0507\n",
            "Epoch [2/2], Step [9351/65307], Loss: 0.0006\n",
            "Epoch [2/2], Step [9401/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [9451/65307], Loss: 0.0521\n",
            "Epoch [2/2], Step [9501/65307], Loss: 0.0850\n",
            "Epoch [2/2], Step [9551/65307], Loss: 0.1717\n",
            "Epoch [2/2], Step [9601/65307], Loss: 0.2206\n",
            "Epoch [2/2], Step [9651/65307], Loss: 0.0704\n",
            "Epoch [2/2], Step [9701/65307], Loss: 0.2082\n",
            "Epoch [2/2], Step [9751/65307], Loss: 0.0729\n",
            "Epoch [2/2], Step [9801/65307], Loss: 0.0760\n",
            "Epoch [2/2], Step [9851/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [9901/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [9951/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [10001/65307], Loss: 0.0488\n",
            "Epoch [2/2], Step [10051/65307], Loss: 0.1980\n",
            "Epoch [2/2], Step [10101/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [10151/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [10201/65307], Loss: 0.1615\n",
            "Epoch [2/2], Step [10251/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [10301/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [10351/65307], Loss: 0.1998\n",
            "Epoch [2/2], Step [10401/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [10451/65307], Loss: 0.1279\n",
            "Epoch [2/2], Step [10501/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [10551/65307], Loss: 0.1348\n",
            "Epoch [2/2], Step [10601/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [10651/65307], Loss: 0.1563\n",
            "Epoch [2/2], Step [10701/65307], Loss: 0.0681\n",
            "Epoch [2/2], Step [10751/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [10801/65307], Loss: 0.0051\n",
            "Epoch [2/2], Step [10851/65307], Loss: 0.2470\n",
            "Epoch [2/2], Step [10901/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [10951/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [11001/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [11051/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [11101/65307], Loss: 0.0312\n",
            "Epoch [2/2], Step [11151/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [11201/65307], Loss: 0.1016\n",
            "Epoch [2/2], Step [11251/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [11301/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [11351/65307], Loss: 0.0439\n",
            "Epoch [2/2], Step [11401/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [11451/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [11501/65307], Loss: 0.0521\n",
            "Epoch [2/2], Step [11551/65307], Loss: 0.0233\n",
            "Epoch [2/2], Step [11601/65307], Loss: 0.0945\n",
            "Epoch [2/2], Step [11651/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [11701/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [11751/65307], Loss: 0.0514\n",
            "Epoch [2/2], Step [11801/65307], Loss: 0.0979\n",
            "Epoch [2/2], Step [11851/65307], Loss: 0.0247\n",
            "Epoch [2/2], Step [11901/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [11951/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [12001/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [12051/65307], Loss: 0.1084\n",
            "Epoch [2/2], Step [12101/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [12151/65307], Loss: 0.1305\n",
            "Epoch [2/2], Step [12201/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [12251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [12301/65307], Loss: 0.1349\n",
            "Epoch [2/2], Step [12301/65307], Loss: 0.1349\n",
            "Epoch [2/2], Step [12351/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [12351/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [12401/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [12401/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [12451/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [12451/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [12501/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [12501/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [12551/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [12551/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [12601/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12601/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12651/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [12651/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [12701/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [12701/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [12751/65307], Loss: 0.0865\n",
            "Epoch [2/2], Step [12751/65307], Loss: 0.0865\n",
            "Epoch [2/2], Step [12801/65307], Loss: 0.0903\n",
            "Epoch [2/2], Step [12801/65307], Loss: 0.0903\n",
            "Epoch [2/2], Step [12851/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12851/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [12901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [12901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [12951/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [12951/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [13001/65307], Loss: 0.0647\n",
            "Epoch [2/2], Step [13001/65307], Loss: 0.0647\n",
            "Epoch [2/2], Step [13051/65307], Loss: 0.0422\n",
            "Epoch [2/2], Step [13051/65307], Loss: 0.0422\n",
            "Epoch [2/2], Step [13101/65307], Loss: 0.0802\n",
            "Epoch [2/2], Step [13101/65307], Loss: 0.0802\n",
            "Epoch [2/2], Step [13151/65307], Loss: 0.2784\n",
            "Epoch [2/2], Step [13151/65307], Loss: 0.2784\n",
            "Epoch [2/2], Step [13201/65307], Loss: 0.0237\n",
            "Epoch [2/2], Step [13201/65307], Loss: 0.0237\n",
            "Epoch [2/2], Step [13251/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [13251/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [13301/65307], Loss: 0.1424\n",
            "Epoch [2/2], Step [13301/65307], Loss: 0.1424\n",
            "Epoch [2/2], Step [13351/65307], Loss: 0.0205\n",
            "Epoch [2/2], Step [13351/65307], Loss: 0.0205\n",
            "Epoch [2/2], Step [13401/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [13401/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [13451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [13451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [13501/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [13501/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [13551/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [13551/65307], Loss: 0.0163\n",
            "Epoch [2/2], Step [13601/65307], Loss: 0.1550\n",
            "Epoch [2/2], Step [13601/65307], Loss: 0.1550\n",
            "Epoch [2/2], Step [13651/65307], Loss: 0.0229\n",
            "Epoch [2/2], Step [13651/65307], Loss: 0.0229\n",
            "Epoch [2/2], Step [13701/65307], Loss: 0.2159\n",
            "Epoch [2/2], Step [13701/65307], Loss: 0.2159\n",
            "Epoch [2/2], Step [13751/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [13751/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [13801/65307], Loss: 0.1727\n",
            "Epoch [2/2], Step [13801/65307], Loss: 0.1727\n",
            "Epoch [2/2], Step [13851/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [13851/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [13901/65307], Loss: 0.0096\n",
            "Epoch [2/2], Step [13901/65307], Loss: 0.0096\n",
            "Epoch [2/2], Step [13951/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [13951/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [14001/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [14001/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [14051/65307], Loss: 0.1444\n",
            "Epoch [2/2], Step [14051/65307], Loss: 0.1444\n",
            "Epoch [2/2], Step [14101/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [14101/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [14151/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [14151/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [14201/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [14201/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [14251/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [14251/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [14301/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [14301/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [14351/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [14351/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [14401/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [14401/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [14451/65307], Loss: 0.0310\n",
            "Epoch [2/2], Step [14451/65307], Loss: 0.0310\n",
            "Epoch [2/2], Step [14501/65307], Loss: 0.3865\n",
            "Epoch [2/2], Step [14501/65307], Loss: 0.3865\n",
            "Epoch [2/2], Step [14551/65307], Loss: 0.3162\n",
            "Epoch [2/2], Step [14551/65307], Loss: 0.3162\n",
            "Epoch [2/2], Step [14601/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [14601/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [14651/65307], Loss: 0.0618\n",
            "Epoch [2/2], Step [14651/65307], Loss: 0.0618\n",
            "Epoch [2/2], Step [14701/65307], Loss: 0.0653\n",
            "Epoch [2/2], Step [14701/65307], Loss: 0.0653\n",
            "Epoch [2/2], Step [14751/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [14751/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [14801/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [14801/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [14851/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [14851/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [14901/65307], Loss: 0.1403\n",
            "Epoch [2/2], Step [14901/65307], Loss: 0.1403\n",
            "Epoch [2/2], Step [14951/65307], Loss: 0.0349\n",
            "Epoch [2/2], Step [14951/65307], Loss: 0.0349\n",
            "Epoch [2/2], Step [15001/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [15001/65307], Loss: 0.0646\n",
            "Epoch [2/2], Step [15051/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [15051/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [15101/65307], Loss: 0.0337\n",
            "Epoch [2/2], Step [15101/65307], Loss: 0.0337\n",
            "Epoch [2/2], Step [15151/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [15151/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [15201/65307], Loss: 0.0810\n",
            "Epoch [2/2], Step [15201/65307], Loss: 0.0810\n",
            "Epoch [2/2], Step [15251/65307], Loss: 0.3256\n",
            "Epoch [2/2], Step [15251/65307], Loss: 0.3256\n",
            "Epoch [2/2], Step [15301/65307], Loss: 0.0500\n",
            "Epoch [2/2], Step [15301/65307], Loss: 0.0500\n",
            "Epoch [2/2], Step [15351/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [15351/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [15401/65307], Loss: 0.0943\n",
            "Epoch [2/2], Step [15401/65307], Loss: 0.0943\n",
            "Epoch [2/2], Step [15451/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [15451/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [15501/65307], Loss: 0.0181\n",
            "Epoch [2/2], Step [15501/65307], Loss: 0.0181\n",
            "Epoch [2/2], Step [15551/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [15551/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [15601/65307], Loss: 0.0536\n",
            "Epoch [2/2], Step [15601/65307], Loss: 0.0536\n",
            "Epoch [2/2], Step [15651/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [15651/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [15701/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [15701/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [15751/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [15751/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [15801/65307], Loss: 0.1338\n",
            "Epoch [2/2], Step [15801/65307], Loss: 0.1338\n",
            "Epoch [2/2], Step [15851/65307], Loss: 0.0196\n",
            "Epoch [2/2], Step [15851/65307], Loss: 0.0196\n",
            "Epoch [2/2], Step [15901/65307], Loss: 0.1026\n",
            "Epoch [2/2], Step [15901/65307], Loss: 0.1026\n",
            "Epoch [2/2], Step [15951/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [15951/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [16001/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [16001/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [16051/65307], Loss: 0.2616\n",
            "Epoch [2/2], Step [16051/65307], Loss: 0.2616\n",
            "Epoch [2/2], Step [16101/65307], Loss: 0.1381\n",
            "Epoch [2/2], Step [16101/65307], Loss: 0.1381\n",
            "Epoch [2/2], Step [16151/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [16151/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [16201/65307], Loss: 0.0052\n",
            "Epoch [2/2], Step [16201/65307], Loss: 0.0052\n",
            "Epoch [2/2], Step [16251/65307], Loss: 0.1907\n",
            "Epoch [2/2], Step [16251/65307], Loss: 0.1907\n",
            "Epoch [2/2], Step [16301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [16301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [16351/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [16351/65307], Loss: 0.1409\n",
            "Epoch [2/2], Step [16401/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [16401/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [16451/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [16451/65307], Loss: 0.0150\n",
            "Epoch [2/2], Step [16501/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [16501/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [16551/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [16551/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [16601/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [16601/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [16651/65307], Loss: 0.1902\n",
            "Epoch [2/2], Step [16651/65307], Loss: 0.1902\n",
            "Epoch [2/2], Step [16701/65307], Loss: 0.0674\n",
            "Epoch [2/2], Step [16701/65307], Loss: 0.0674\n",
            "Epoch [2/2], Step [16751/65307], Loss: 0.5359\n",
            "Epoch [2/2], Step [16751/65307], Loss: 0.5359\n",
            "Epoch [2/2], Step [16801/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [16801/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [16851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [16851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [16901/65307], Loss: 0.1447\n",
            "Epoch [2/2], Step [16901/65307], Loss: 0.1447\n",
            "Epoch [2/2], Step [16951/65307], Loss: 0.0294\n",
            "Epoch [2/2], Step [16951/65307], Loss: 0.0294\n",
            "Epoch [2/2], Step [17001/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [17001/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [17051/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [17051/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [17101/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [17101/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [17151/65307], Loss: 0.0467\n",
            "Epoch [2/2], Step [17151/65307], Loss: 0.0467\n",
            "Epoch [2/2], Step [17201/65307], Loss: 0.1475\n",
            "Epoch [2/2], Step [17201/65307], Loss: 0.1475\n",
            "Epoch [2/2], Step [17251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [17251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [17301/65307], Loss: 0.0925\n",
            "Epoch [2/2], Step [17301/65307], Loss: 0.0925\n",
            "Epoch [2/2], Step [17351/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [17351/65307], Loss: 0.0786\n",
            "Epoch [2/2], Step [17401/65307], Loss: 0.0191\n",
            "Epoch [2/2], Step [17401/65307], Loss: 0.0191\n",
            "Epoch [2/2], Step [17451/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [17451/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [17501/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [17501/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [17551/65307], Loss: 0.2900\n",
            "Epoch [2/2], Step [17551/65307], Loss: 0.2900\n",
            "Epoch [2/2], Step [17601/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [17601/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [17651/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [17651/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [17701/65307], Loss: 0.1019\n",
            "Epoch [2/2], Step [17701/65307], Loss: 0.1019\n",
            "Epoch [2/2], Step [17751/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [17751/65307], Loss: 0.0301\n",
            "Epoch [2/2], Step [17801/65307], Loss: 0.0379\n",
            "Epoch [2/2], Step [17801/65307], Loss: 0.0379\n",
            "Epoch [2/2], Step [17851/65307], Loss: 0.1320\n",
            "Epoch [2/2], Step [17851/65307], Loss: 0.1320\n",
            "Epoch [2/2], Step [17901/65307], Loss: 0.0813\n",
            "Epoch [2/2], Step [17901/65307], Loss: 0.0813\n",
            "Epoch [2/2], Step [17951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [17951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [18001/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [18001/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [18051/65307], Loss: 0.1671\n",
            "Epoch [2/2], Step [18051/65307], Loss: 0.1671\n",
            "Epoch [2/2], Step [18101/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [18101/65307], Loss: 0.1735\n",
            "Epoch [2/2], Step [18151/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [18151/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [18201/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [18201/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [18251/65307], Loss: 0.0553\n",
            "Epoch [2/2], Step [18251/65307], Loss: 0.0553\n",
            "Epoch [2/2], Step [18301/65307], Loss: 0.1136\n",
            "Epoch [2/2], Step [18301/65307], Loss: 0.1136\n",
            "Epoch [2/2], Step [18351/65307], Loss: 0.0102\n",
            "Epoch [2/2], Step [18351/65307], Loss: 0.0102\n",
            "Epoch [2/2], Step [18401/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [18401/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [18451/65307], Loss: 0.0407\n",
            "Epoch [2/2], Step [18451/65307], Loss: 0.0407\n",
            "Epoch [2/2], Step [18501/65307], Loss: 0.0253\n",
            "Epoch [2/2], Step [18501/65307], Loss: 0.0253\n",
            "Epoch [2/2], Step [18551/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [18551/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [18601/65307], Loss: 0.2588\n",
            "Epoch [2/2], Step [18601/65307], Loss: 0.2588\n",
            "Epoch [2/2], Step [18651/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [18651/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [18701/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [18701/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [18751/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [18751/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [18801/65307], Loss: 0.0462\n",
            "Epoch [2/2], Step [18801/65307], Loss: 0.0462\n",
            "Epoch [2/2], Step [18851/65307], Loss: 0.2554\n",
            "Epoch [2/2], Step [18851/65307], Loss: 0.2554\n",
            "Epoch [2/2], Step [18901/65307], Loss: 0.0737\n",
            "Epoch [2/2], Step [18901/65307], Loss: 0.0737\n",
            "Epoch [2/2], Step [18951/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [18951/65307], Loss: 0.0105\n",
            "Epoch [2/2], Step [19001/65307], Loss: 0.1421\n",
            "Epoch [2/2], Step [19001/65307], Loss: 0.1421\n",
            "Epoch [2/2], Step [19051/65307], Loss: 0.0442\n",
            "Epoch [2/2], Step [19051/65307], Loss: 0.0442\n",
            "Epoch [2/2], Step [19101/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [19101/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [19151/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [19151/65307], Loss: 0.0325\n",
            "Epoch [2/2], Step [19201/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [19201/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [19251/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [19251/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [19301/65307], Loss: 0.0261\n",
            "Epoch [2/2], Step [19301/65307], Loss: 0.0261\n",
            "Epoch [2/2], Step [19351/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [19351/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [19401/65307], Loss: 0.4466\n",
            "Epoch [2/2], Step [19401/65307], Loss: 0.4466\n",
            "Epoch [2/2], Step [19451/65307], Loss: 0.1384\n",
            "Epoch [2/2], Step [19451/65307], Loss: 0.1384\n",
            "Epoch [2/2], Step [19501/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [19501/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [19551/65307], Loss: 0.0114\n",
            "Epoch [2/2], Step [19551/65307], Loss: 0.0114\n",
            "Epoch [2/2], Step [19601/65307], Loss: 0.2517\n",
            "Epoch [2/2], Step [19601/65307], Loss: 0.2517\n",
            "Epoch [2/2], Step [19651/65307], Loss: 0.0970\n",
            "Epoch [2/2], Step [19651/65307], Loss: 0.0970\n",
            "Epoch [2/2], Step [19701/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [19701/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [19751/65307], Loss: 0.0727\n",
            "Epoch [2/2], Step [19751/65307], Loss: 0.0727\n",
            "Epoch [2/2], Step [19801/65307], Loss: 0.1502\n",
            "Epoch [2/2], Step [19801/65307], Loss: 0.1502\n",
            "Epoch [2/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [19851/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [19901/65307], Loss: 0.0365\n",
            "Epoch [2/2], Step [19901/65307], Loss: 0.0365\n",
            "Epoch [2/2], Step [19951/65307], Loss: 0.1281\n",
            "Epoch [2/2], Step [19951/65307], Loss: 0.1281\n",
            "Epoch [2/2], Step [20001/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [20001/65307], Loss: 0.0074\n",
            "Epoch [2/2], Step [20051/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [20051/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [20101/65307], Loss: 0.2540\n",
            "Epoch [2/2], Step [20101/65307], Loss: 0.2540\n",
            "Epoch [2/2], Step [20151/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [20151/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [20201/65307], Loss: 0.0577\n",
            "Epoch [2/2], Step [20201/65307], Loss: 0.0577\n",
            "Epoch [2/2], Step [20251/65307], Loss: 0.0790\n",
            "Epoch [2/2], Step [20251/65307], Loss: 0.0790\n",
            "Epoch [2/2], Step [20301/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [20301/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [20351/65307], Loss: 0.0466\n",
            "Epoch [2/2], Step [20351/65307], Loss: 0.0466\n",
            "Epoch [2/2], Step [20401/65307], Loss: 0.3849\n",
            "Epoch [2/2], Step [20401/65307], Loss: 0.3849\n",
            "Epoch [2/2], Step [20451/65307], Loss: 0.0437\n",
            "Epoch [2/2], Step [20451/65307], Loss: 0.0437\n",
            "Epoch [2/2], Step [20501/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [20501/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [20551/65307], Loss: 0.0311\n",
            "Epoch [2/2], Step [20551/65307], Loss: 0.0311\n",
            "Epoch [2/2], Step [20601/65307], Loss: 0.0366\n",
            "Epoch [2/2], Step [20601/65307], Loss: 0.0366\n",
            "Epoch [2/2], Step [20651/65307], Loss: 0.0463\n",
            "Epoch [2/2], Step [20651/65307], Loss: 0.0463\n",
            "Epoch [2/2], Step [20701/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [20701/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [20751/65307], Loss: 0.0393\n",
            "Epoch [2/2], Step [20751/65307], Loss: 0.0393\n",
            "Epoch [2/2], Step [20801/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [20801/65307], Loss: 0.0284\n",
            "Epoch [2/2], Step [20851/65307], Loss: 0.0517\n",
            "Epoch [2/2], Step [20851/65307], Loss: 0.0517\n",
            "Epoch [2/2], Step [20901/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [20901/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [20951/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [20951/65307], Loss: 0.0396\n",
            "Epoch [2/2], Step [21001/65307], Loss: 0.2528\n",
            "Epoch [2/2], Step [21001/65307], Loss: 0.2528\n",
            "Epoch [2/2], Step [21051/65307], Loss: 0.1940\n",
            "Epoch [2/2], Step [21051/65307], Loss: 0.1940\n",
            "Epoch [2/2], Step [21101/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [21101/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [21151/65307], Loss: 0.1724\n",
            "Epoch [2/2], Step [21151/65307], Loss: 0.1724\n",
            "Epoch [2/2], Step [21201/65307], Loss: 0.3967\n",
            "Epoch [2/2], Step [21201/65307], Loss: 0.3967\n",
            "Epoch [2/2], Step [21251/65307], Loss: 0.1763\n",
            "Epoch [2/2], Step [21251/65307], Loss: 0.1763\n",
            "Epoch [2/2], Step [21301/65307], Loss: 0.0921\n",
            "Epoch [2/2], Step [21301/65307], Loss: 0.0921\n",
            "Epoch [2/2], Step [21351/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [21351/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [21401/65307], Loss: 0.1663\n",
            "Epoch [2/2], Step [21401/65307], Loss: 0.1663\n",
            "Epoch [2/2], Step [21451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [21451/65307], Loss: 0.0236\n",
            "Epoch [2/2], Step [21501/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [21501/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [21551/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [21551/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [21601/65307], Loss: 0.0744\n",
            "Epoch [2/2], Step [21601/65307], Loss: 0.0744\n",
            "Epoch [2/2], Step [21651/65307], Loss: 0.1831\n",
            "Epoch [2/2], Step [21651/65307], Loss: 0.1831\n",
            "Epoch [2/2], Step [21701/65307], Loss: 0.1045\n",
            "Epoch [2/2], Step [21701/65307], Loss: 0.1045\n",
            "Epoch [2/2], Step [21751/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [21751/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [21801/65307], Loss: 0.1750\n",
            "Epoch [2/2], Step [21801/65307], Loss: 0.1750\n",
            "Epoch [2/2], Step [21851/65307], Loss: 0.0042\n",
            "Epoch [2/2], Step [21851/65307], Loss: 0.0042\n",
            "Epoch [2/2], Step [21901/65307], Loss: 0.0954\n",
            "Epoch [2/2], Step [21901/65307], Loss: 0.0954\n",
            "Epoch [2/2], Step [21951/65307], Loss: 0.0733\n",
            "Epoch [2/2], Step [21951/65307], Loss: 0.0733\n",
            "Epoch [2/2], Step [22001/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [22001/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [22051/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [22051/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [22101/65307], Loss: 0.0620\n",
            "Epoch [2/2], Step [22101/65307], Loss: 0.0620\n",
            "Epoch [2/2], Step [22151/65307], Loss: 0.1406\n",
            "Epoch [2/2], Step [22151/65307], Loss: 0.1406\n",
            "Epoch [2/2], Step [22201/65307], Loss: 0.1177\n",
            "Epoch [2/2], Step [22201/65307], Loss: 0.1177\n",
            "Epoch [2/2], Step [22251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [22251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [22301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [22301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [22351/65307], Loss: 0.1587\n",
            "Epoch [2/2], Step [22351/65307], Loss: 0.1587\n",
            "Epoch [2/2], Step [22401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [22401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [22451/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [22451/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [22501/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22501/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22551/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [22551/65307], Loss: 0.1276\n",
            "Epoch [2/2], Step [22601/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [22601/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [22651/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [22651/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [22701/65307], Loss: 0.0881\n",
            "Epoch [2/2], Step [22701/65307], Loss: 0.0881\n",
            "Epoch [2/2], Step [22751/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [22751/65307], Loss: 0.1437\n",
            "Epoch [2/2], Step [22801/65307], Loss: 0.1060\n",
            "Epoch [2/2], Step [22801/65307], Loss: 0.1060\n",
            "Epoch [2/2], Step [22851/65307], Loss: 0.0597\n",
            "Epoch [2/2], Step [22851/65307], Loss: 0.0597\n",
            "Epoch [2/2], Step [22901/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22901/65307], Loss: 0.0356\n",
            "Epoch [2/2], Step [22951/65307], Loss: 0.1456\n",
            "Epoch [2/2], Step [22951/65307], Loss: 0.1456\n",
            "Epoch [2/2], Step [23001/65307], Loss: 0.0277\n",
            "Epoch [2/2], Step [23001/65307], Loss: 0.0277\n",
            "Epoch [2/2], Step [23051/65307], Loss: 0.0911\n",
            "Epoch [2/2], Step [23051/65307], Loss: 0.0911\n",
            "Epoch [2/2], Step [23101/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [23101/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [23151/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [23151/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [23201/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [23201/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [23251/65307], Loss: 0.1159\n",
            "Epoch [2/2], Step [23251/65307], Loss: 0.1159\n",
            "Epoch [2/2], Step [23301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [23301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [23351/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [23351/65307], Loss: 0.0224\n",
            "Epoch [2/2], Step [23401/65307], Loss: 0.0243\n",
            "Epoch [2/2], Step [23401/65307], Loss: 0.0243\n",
            "Epoch [2/2], Step [23451/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [23451/65307], Loss: 0.0270\n",
            "Epoch [2/2], Step [23501/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23501/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23551/65307], Loss: 0.3817\n",
            "Epoch [2/2], Step [23551/65307], Loss: 0.3817\n",
            "Epoch [2/2], Step [23601/65307], Loss: 0.0859\n",
            "Epoch [2/2], Step [23601/65307], Loss: 0.0859\n",
            "Epoch [2/2], Step [23651/65307], Loss: 0.0747\n",
            "Epoch [2/2], Step [23651/65307], Loss: 0.0747\n",
            "Epoch [2/2], Step [23701/65307], Loss: 0.1554\n",
            "Epoch [2/2], Step [23701/65307], Loss: 0.1554\n",
            "Epoch [2/2], Step [23751/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [23751/65307], Loss: 0.1005\n",
            "Epoch [2/2], Step [23801/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [23801/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [23851/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [23851/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [23901/65307], Loss: 0.0875\n",
            "Epoch [2/2], Step [23901/65307], Loss: 0.0875\n",
            "Epoch [2/2], Step [23951/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [23951/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [24001/65307], Loss: 0.0740\n",
            "Epoch [2/2], Step [24001/65307], Loss: 0.0740\n",
            "Epoch [2/2], Step [24051/65307], Loss: 0.0255\n",
            "Epoch [2/2], Step [24051/65307], Loss: 0.0255\n",
            "Epoch [2/2], Step [24101/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [24101/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [24151/65307], Loss: 0.0892\n",
            "Epoch [2/2], Step [24151/65307], Loss: 0.0892\n",
            "Epoch [2/2], Step [24201/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [24201/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [24251/65307], Loss: 0.1537\n",
            "Epoch [2/2], Step [24251/65307], Loss: 0.1537\n",
            "Epoch [2/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [2/2], Step [24301/65307], Loss: 0.0056\n",
            "Epoch [2/2], Step [24351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [24351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [24401/65307], Loss: 0.0530\n",
            "Epoch [2/2], Step [24401/65307], Loss: 0.0530\n",
            "Epoch [2/2], Step [24451/65307], Loss: 0.0258\n",
            "Epoch [2/2], Step [24451/65307], Loss: 0.0258\n",
            "Epoch [2/2], Step [24501/65307], Loss: 0.1511\n",
            "Epoch [2/2], Step [24501/65307], Loss: 0.1511\n",
            "Epoch [2/2], Step [24551/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [24551/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [24601/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [24601/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [24651/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [24651/65307], Loss: 0.0007\n",
            "Epoch [2/2], Step [24701/65307], Loss: 0.1368\n",
            "Epoch [2/2], Step [24701/65307], Loss: 0.1368\n",
            "Epoch [2/2], Step [24751/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [24751/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [24801/65307], Loss: 0.0691\n",
            "Epoch [2/2], Step [24801/65307], Loss: 0.0691\n",
            "Epoch [2/2], Step [24851/65307], Loss: 0.0159\n",
            "Epoch [2/2], Step [24851/65307], Loss: 0.0159\n",
            "Epoch [2/2], Step [24901/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [24901/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [24951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [24951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [25001/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [25001/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [25051/65307], Loss: 0.2888\n",
            "Epoch [2/2], Step [25051/65307], Loss: 0.2888\n",
            "Epoch [2/2], Step [25101/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [25101/65307], Loss: 0.2187\n",
            "Epoch [2/2], Step [25151/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [25151/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [25201/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [25201/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [25251/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [25251/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [25301/65307], Loss: 0.0403\n",
            "Epoch [2/2], Step [25301/65307], Loss: 0.0403\n",
            "Epoch [2/2], Step [25351/65307], Loss: 0.0578\n",
            "Epoch [2/2], Step [25351/65307], Loss: 0.0578\n",
            "Epoch [2/2], Step [25401/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [25401/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [25451/65307], Loss: 0.0405\n",
            "Epoch [2/2], Step [25451/65307], Loss: 0.0405\n",
            "Epoch [2/2], Step [25501/65307], Loss: 0.1298\n",
            "Epoch [2/2], Step [25501/65307], Loss: 0.1298\n",
            "Epoch [2/2], Step [25551/65307], Loss: 0.1440\n",
            "Epoch [2/2], Step [25551/65307], Loss: 0.1440\n",
            "Epoch [2/2], Step [25601/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [25601/65307], Loss: 0.0399\n",
            "Epoch [2/2], Step [25651/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [25651/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [25701/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [25701/65307], Loss: 0.0353\n",
            "Epoch [2/2], Step [25751/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [25751/65307], Loss: 0.0885\n",
            "Epoch [2/2], Step [25801/65307], Loss: 0.0472\n",
            "Epoch [2/2], Step [25801/65307], Loss: 0.0472\n",
            "Epoch [2/2], Step [25851/65307], Loss: 0.0117\n",
            "Epoch [2/2], Step [25851/65307], Loss: 0.0117\n",
            "Epoch [2/2], Step [25901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [25901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [25951/65307], Loss: 0.1624\n",
            "Epoch [2/2], Step [25951/65307], Loss: 0.1624\n",
            "Epoch [2/2], Step [26001/65307], Loss: 0.0142\n",
            "Epoch [2/2], Step [26001/65307], Loss: 0.0142\n",
            "Epoch [2/2], Step [26051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [26051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [26101/65307], Loss: 0.0495\n",
            "Epoch [2/2], Step [26101/65307], Loss: 0.0495\n",
            "Epoch [2/2], Step [26151/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [26151/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [26201/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [26201/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [26251/65307], Loss: 0.0809\n",
            "Epoch [2/2], Step [26251/65307], Loss: 0.0809\n",
            "Epoch [2/2], Step [26301/65307], Loss: 0.2137\n",
            "Epoch [2/2], Step [26301/65307], Loss: 0.2137\n",
            "Epoch [2/2], Step [26351/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [26351/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [26401/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [26401/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [26451/65307], Loss: 0.1083\n",
            "Epoch [2/2], Step [26451/65307], Loss: 0.1083\n",
            "Epoch [2/2], Step [26501/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [26501/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [26551/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [26551/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [26601/65307], Loss: 0.1928\n",
            "Epoch [2/2], Step [26601/65307], Loss: 0.1928\n",
            "Epoch [2/2], Step [26651/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [26651/65307], Loss: 0.0197\n",
            "Epoch [2/2], Step [26701/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [26701/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [26751/65307], Loss: 0.0213\n",
            "Epoch [2/2], Step [26751/65307], Loss: 0.0213\n",
            "Epoch [2/2], Step [26801/65307], Loss: 0.2139\n",
            "Epoch [2/2], Step [26801/65307], Loss: 0.2139\n",
            "Epoch [2/2], Step [26851/65307], Loss: 0.0127\n",
            "Epoch [2/2], Step [26851/65307], Loss: 0.0127\n",
            "Epoch [2/2], Step [26901/65307], Loss: 0.2513\n",
            "Epoch [2/2], Step [26901/65307], Loss: 0.2513\n",
            "Epoch [2/2], Step [26951/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [26951/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [27001/65307], Loss: 0.1542\n",
            "Epoch [2/2], Step [27001/65307], Loss: 0.1542\n",
            "Epoch [2/2], Step [27051/65307], Loss: 0.0491\n",
            "Epoch [2/2], Step [27051/65307], Loss: 0.0491\n",
            "Epoch [2/2], Step [27101/65307], Loss: 0.0777\n",
            "Epoch [2/2], Step [27101/65307], Loss: 0.0777\n",
            "Epoch [2/2], Step [27151/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [27151/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [27201/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [27201/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [27251/65307], Loss: 0.0410\n",
            "Epoch [2/2], Step [27251/65307], Loss: 0.0410\n",
            "Epoch [2/2], Step [27301/65307], Loss: 0.0715\n",
            "Epoch [2/2], Step [27301/65307], Loss: 0.0715\n",
            "Epoch [2/2], Step [27351/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [27351/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [27401/65307], Loss: 0.4865\n",
            "Epoch [2/2], Step [27401/65307], Loss: 0.4865\n",
            "Epoch [2/2], Step [27451/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [27451/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [27501/65307], Loss: 0.1147\n",
            "Epoch [2/2], Step [27501/65307], Loss: 0.1147\n",
            "Epoch [2/2], Step [27551/65307], Loss: 0.1313\n",
            "Epoch [2/2], Step [27551/65307], Loss: 0.1313\n",
            "Epoch [2/2], Step [27601/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [27601/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [27651/65307], Loss: 0.0768\n",
            "Epoch [2/2], Step [27651/65307], Loss: 0.0768\n",
            "Epoch [2/2], Step [27701/65307], Loss: 0.2505\n",
            "Epoch [2/2], Step [27701/65307], Loss: 0.2505\n",
            "Epoch [2/2], Step [27751/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [27751/65307], Loss: 0.0430\n",
            "Epoch [2/2], Step [27801/65307], Loss: 0.1976\n",
            "Epoch [2/2], Step [27801/65307], Loss: 0.1976\n",
            "Epoch [2/2], Step [27851/65307], Loss: 0.0978\n",
            "Epoch [2/2], Step [27851/65307], Loss: 0.0978\n",
            "Epoch [2/2], Step [27901/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [27901/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [27951/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [27951/65307], Loss: 0.1182\n",
            "Epoch [2/2], Step [28001/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [28001/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [28051/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [28051/65307], Loss: 0.1015\n",
            "Epoch [2/2], Step [28101/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [28101/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [28151/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [28151/65307], Loss: 0.0424\n",
            "Epoch [2/2], Step [28201/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [28201/65307], Loss: 0.0151\n",
            "Epoch [2/2], Step [28251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [28251/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [28301/65307], Loss: 0.0088\n",
            "Epoch [2/2], Step [28301/65307], Loss: 0.0088\n",
            "Epoch [2/2], Step [28351/65307], Loss: 0.0918\n",
            "Epoch [2/2], Step [28351/65307], Loss: 0.0918\n",
            "Epoch [2/2], Step [28401/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [28401/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [28451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [28451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [28501/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [28501/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [28551/65307], Loss: 0.0239\n",
            "Epoch [2/2], Step [28551/65307], Loss: 0.0239\n",
            "Epoch [2/2], Step [28601/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [28601/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [28651/65307], Loss: 0.1942\n",
            "Epoch [2/2], Step [28651/65307], Loss: 0.1942\n",
            "Epoch [2/2], Step [28701/65307], Loss: 0.0178\n",
            "Epoch [2/2], Step [28701/65307], Loss: 0.0178\n",
            "Epoch [2/2], Step [28751/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [28751/65307], Loss: 0.0318\n",
            "Epoch [2/2], Step [28801/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [28801/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [28851/65307], Loss: 0.0803\n",
            "Epoch [2/2], Step [28851/65307], Loss: 0.0803\n",
            "Epoch [2/2], Step [28901/65307], Loss: 0.2707\n",
            "Epoch [2/2], Step [28901/65307], Loss: 0.2707\n",
            "Epoch [2/2], Step [28951/65307], Loss: 0.2586\n",
            "Epoch [2/2], Step [28951/65307], Loss: 0.2586\n",
            "Epoch [2/2], Step [29001/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [29001/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [29051/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [29051/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [29101/65307], Loss: 0.1504\n",
            "Epoch [2/2], Step [29101/65307], Loss: 0.1504\n",
            "Epoch [2/2], Step [29151/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [29151/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [29201/65307], Loss: 0.1651\n",
            "Epoch [2/2], Step [29201/65307], Loss: 0.1651\n",
            "Epoch [2/2], Step [29251/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [29251/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [29301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [29301/65307], Loss: 0.0188\n",
            "Epoch [2/2], Step [29351/65307], Loss: 0.1769\n",
            "Epoch [2/2], Step [29351/65307], Loss: 0.1769\n",
            "Epoch [2/2], Step [29401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [29401/65307], Loss: 0.0326\n",
            "Epoch [2/2], Step [29451/65307], Loss: 0.2201\n",
            "Epoch [2/2], Step [29451/65307], Loss: 0.2201\n",
            "Epoch [2/2], Step [29501/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [29501/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [29551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [29551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [29601/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [29601/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [29651/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [29651/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [29701/65307], Loss: 0.0805\n",
            "Epoch [2/2], Step [29701/65307], Loss: 0.0805\n",
            "Epoch [2/2], Step [29751/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [29751/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [29801/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [29801/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [29851/65307], Loss: 0.0435\n",
            "Epoch [2/2], Step [29851/65307], Loss: 0.0435\n",
            "Epoch [2/2], Step [29901/65307], Loss: 0.0754\n",
            "Epoch [2/2], Step [29901/65307], Loss: 0.0754\n",
            "Epoch [2/2], Step [29951/65307], Loss: 0.3547\n",
            "Epoch [2/2], Step [29951/65307], Loss: 0.3547\n",
            "Epoch [2/2], Step [30001/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [30001/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [30051/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [30051/65307], Loss: 0.0012\n",
            "Epoch [2/2], Step [30101/65307], Loss: 0.0816\n",
            "Epoch [2/2], Step [30101/65307], Loss: 0.0816\n",
            "Epoch [2/2], Step [30151/65307], Loss: 0.0486\n",
            "Epoch [2/2], Step [30151/65307], Loss: 0.0486\n",
            "Epoch [2/2], Step [30201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [30201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [30251/65307], Loss: 0.1745\n",
            "Epoch [2/2], Step [30251/65307], Loss: 0.1745\n",
            "Epoch [2/2], Step [30301/65307], Loss: 0.0845\n",
            "Epoch [2/2], Step [30301/65307], Loss: 0.0845\n",
            "Epoch [2/2], Step [30351/65307], Loss: 0.1848\n",
            "Epoch [2/2], Step [30351/65307], Loss: 0.1848\n",
            "Epoch [2/2], Step [30401/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [30401/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [30451/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [30451/65307], Loss: 0.0533\n",
            "Epoch [2/2], Step [30501/65307], Loss: 0.2430\n",
            "Epoch [2/2], Step [30501/65307], Loss: 0.2430\n",
            "Epoch [2/2], Step [30551/65307], Loss: 0.2051\n",
            "Epoch [2/2], Step [30551/65307], Loss: 0.2051\n",
            "Epoch [2/2], Step [30601/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [30601/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [30651/65307], Loss: 0.1515\n",
            "Epoch [2/2], Step [30651/65307], Loss: 0.1515\n",
            "Epoch [2/2], Step [30701/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [30701/65307], Loss: 0.0343\n",
            "Epoch [2/2], Step [30751/65307], Loss: 0.0962\n",
            "Epoch [2/2], Step [30751/65307], Loss: 0.0962\n",
            "Epoch [2/2], Step [30801/65307], Loss: 0.0469\n",
            "Epoch [2/2], Step [30801/65307], Loss: 0.0469\n",
            "Epoch [2/2], Step [30851/65307], Loss: 0.0801\n",
            "Epoch [2/2], Step [30851/65307], Loss: 0.0801\n",
            "Epoch [2/2], Step [30901/65307], Loss: 0.1987\n",
            "Epoch [2/2], Step [30901/65307], Loss: 0.1987\n",
            "Epoch [2/2], Step [30951/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [30951/65307], Loss: 0.0291\n",
            "Epoch [2/2], Step [31001/65307], Loss: 0.0119\n",
            "Epoch [2/2], Step [31001/65307], Loss: 0.0119\n",
            "Epoch [2/2], Step [31051/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [31051/65307], Loss: 0.0145\n",
            "Epoch [2/2], Step [31101/65307], Loss: 0.0168\n",
            "Epoch [2/2], Step [31101/65307], Loss: 0.0168\n",
            "Epoch [2/2], Step [31151/65307], Loss: 0.0194\n",
            "Epoch [2/2], Step [31151/65307], Loss: 0.0194\n",
            "Epoch [2/2], Step [31201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [31201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [31251/65307], Loss: 0.0129\n",
            "Epoch [2/2], Step [31251/65307], Loss: 0.0129\n",
            "Epoch [2/2], Step [31301/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [31301/65307], Loss: 0.0480\n",
            "Epoch [2/2], Step [31351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [31351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [31401/65307], Loss: 0.1642\n",
            "Epoch [2/2], Step [31401/65307], Loss: 0.1642\n",
            "Epoch [2/2], Step [31451/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [31451/65307], Loss: 0.2422\n",
            "Epoch [2/2], Step [31501/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [31501/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [31551/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [31551/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [31601/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [31601/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [31651/65307], Loss: 0.1777\n",
            "Epoch [2/2], Step [31651/65307], Loss: 0.1777\n",
            "Epoch [2/2], Step [31701/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [31701/65307], Loss: 0.0599\n",
            "Epoch [2/2], Step [31751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [31751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [31801/65307], Loss: 0.4305\n",
            "Epoch [2/2], Step [31801/65307], Loss: 0.4305\n",
            "Epoch [2/2], Step [31851/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [31851/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [31901/65307], Loss: 0.0574\n",
            "Epoch [2/2], Step [31901/65307], Loss: 0.0574\n",
            "Epoch [2/2], Step [31951/65307], Loss: 0.0134\n",
            "Epoch [2/2], Step [31951/65307], Loss: 0.0134\n",
            "Epoch [2/2], Step [32001/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [32001/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [32051/65307], Loss: 0.0949\n",
            "Epoch [2/2], Step [32051/65307], Loss: 0.0949\n",
            "Epoch [2/2], Step [32101/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [32101/65307], Loss: 0.0046\n",
            "Epoch [2/2], Step [32151/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [32151/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [32201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [32201/65307], Loss: 0.0215\n",
            "Epoch [2/2], Step [32251/65307], Loss: 0.0408\n",
            "Epoch [2/2], Step [32251/65307], Loss: 0.0408\n",
            "Epoch [2/2], Step [32301/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [32301/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [32351/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [32351/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [32401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [32401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [32451/65307], Loss: 0.0214\n",
            "Epoch [2/2], Step [32451/65307], Loss: 0.0214\n",
            "Epoch [2/2], Step [32501/65307], Loss: 0.0444\n",
            "Epoch [2/2], Step [32501/65307], Loss: 0.0444\n",
            "Epoch [2/2], Step [32551/65307], Loss: 0.0019\n",
            "Epoch [2/2], Step [32551/65307], Loss: 0.0019\n",
            "Epoch [2/2], Step [32601/65307], Loss: 0.0861\n",
            "Epoch [2/2], Step [32601/65307], Loss: 0.0861\n",
            "Epoch [2/2], Step [32651/65307], Loss: 0.2033\n",
            "Epoch [2/2], Step [32651/65307], Loss: 0.2033\n",
            "Epoch [2/2], Step [32701/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [32701/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [32751/65307], Loss: 0.0822\n",
            "Epoch [2/2], Step [32751/65307], Loss: 0.0822\n",
            "Epoch [2/2], Step [32801/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [32801/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [32851/65307], Loss: 0.0409\n",
            "Epoch [2/2], Step [32851/65307], Loss: 0.0409\n",
            "Epoch [2/2], Step [32901/65307], Loss: 0.0794\n",
            "Epoch [2/2], Step [32901/65307], Loss: 0.0794\n",
            "Epoch [2/2], Step [32951/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [32951/65307], Loss: 0.0077\n",
            "Epoch [2/2], Step [33001/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [33001/65307], Loss: 0.0166\n",
            "Epoch [2/2], Step [33051/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [33051/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [33101/65307], Loss: 0.1031\n",
            "Epoch [2/2], Step [33101/65307], Loss: 0.1031\n",
            "Epoch [2/2], Step [33151/65307], Loss: 0.1065\n",
            "Epoch [2/2], Step [33151/65307], Loss: 0.1065\n",
            "Epoch [2/2], Step [33201/65307], Loss: 0.0128\n",
            "Epoch [2/2], Step [33201/65307], Loss: 0.0128\n",
            "Epoch [2/2], Step [33251/65307], Loss: 0.1675\n",
            "Epoch [2/2], Step [33251/65307], Loss: 0.1675\n",
            "Epoch [2/2], Step [33301/65307], Loss: 0.0797\n",
            "Epoch [2/2], Step [33301/65307], Loss: 0.0797\n",
            "Epoch [2/2], Step [33351/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [33351/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [33401/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [33401/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [33451/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [33451/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [33501/65307], Loss: 0.0776\n",
            "Epoch [2/2], Step [33501/65307], Loss: 0.0776\n",
            "Epoch [2/2], Step [33551/65307], Loss: 0.1588\n",
            "Epoch [2/2], Step [33551/65307], Loss: 0.1588\n",
            "Epoch [2/2], Step [33601/65307], Loss: 0.2607\n",
            "Epoch [2/2], Step [33601/65307], Loss: 0.2607\n",
            "Epoch [2/2], Step [33651/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [33651/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [33701/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [33701/65307], Loss: 0.0446\n",
            "Epoch [2/2], Step [33751/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [33751/65307], Loss: 0.0853\n",
            "Epoch [2/2], Step [33801/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [33801/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [33851/65307], Loss: 0.0041\n",
            "Epoch [2/2], Step [33851/65307], Loss: 0.0041\n",
            "Epoch [2/2], Step [33901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [33901/65307], Loss: 0.0055\n",
            "Epoch [2/2], Step [33951/65307], Loss: 0.2519\n",
            "Epoch [2/2], Step [33951/65307], Loss: 0.2519\n",
            "Epoch [2/2], Step [34001/65307], Loss: 0.1113\n",
            "Epoch [2/2], Step [34001/65307], Loss: 0.1113\n",
            "Epoch [2/2], Step [34051/65307], Loss: 0.3291\n",
            "Epoch [2/2], Step [34051/65307], Loss: 0.3291\n",
            "Epoch [2/2], Step [34101/65307], Loss: 0.4168\n",
            "Epoch [2/2], Step [34101/65307], Loss: 0.4168\n",
            "Epoch [2/2], Step [34151/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [34151/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [34201/65307], Loss: 0.1014\n",
            "Epoch [2/2], Step [34201/65307], Loss: 0.1014\n",
            "Epoch [2/2], Step [34251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [34251/65307], Loss: 0.0121\n",
            "Epoch [2/2], Step [34301/65307], Loss: 0.0319\n",
            "Epoch [2/2], Step [34301/65307], Loss: 0.0319\n",
            "Epoch [2/2], Step [34351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [34351/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [34401/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [34401/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [34451/65307], Loss: 0.1971\n",
            "Epoch [2/2], Step [34451/65307], Loss: 0.1971\n",
            "Epoch [2/2], Step [34501/65307], Loss: 0.0417\n",
            "Epoch [2/2], Step [34501/65307], Loss: 0.0417\n",
            "Epoch [2/2], Step [34551/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [34551/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [34601/65307], Loss: 0.0389\n",
            "Epoch [2/2], Step [34601/65307], Loss: 0.0389\n",
            "Epoch [2/2], Step [34651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [34651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [34701/65307], Loss: 0.3792\n",
            "Epoch [2/2], Step [34701/65307], Loss: 0.3792\n",
            "Epoch [2/2], Step [34751/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [34751/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [34801/65307], Loss: 0.1626\n",
            "Epoch [2/2], Step [34801/65307], Loss: 0.1626\n",
            "Epoch [2/2], Step [34851/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [34851/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [34901/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [34901/65307], Loss: 0.0373\n",
            "Epoch [2/2], Step [34951/65307], Loss: 0.0506\n",
            "Epoch [2/2], Step [34951/65307], Loss: 0.0506\n",
            "Epoch [2/2], Step [35001/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [35001/65307], Loss: 0.1189\n",
            "Epoch [2/2], Step [35051/65307], Loss: 0.0281\n",
            "Epoch [2/2], Step [35051/65307], Loss: 0.0281\n",
            "Epoch [2/2], Step [35101/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [35101/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [35151/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [35151/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [35201/65307], Loss: 0.0474\n",
            "Epoch [2/2], Step [35201/65307], Loss: 0.0474\n",
            "Epoch [2/2], Step [35251/65307], Loss: 0.0496\n",
            "Epoch [2/2], Step [35251/65307], Loss: 0.0496\n",
            "Epoch [2/2], Step [35301/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [35301/65307], Loss: 0.0182\n",
            "Epoch [2/2], Step [35351/65307], Loss: 0.1877\n",
            "Epoch [2/2], Step [35351/65307], Loss: 0.1877\n",
            "Epoch [2/2], Step [35401/65307], Loss: 0.1051\n",
            "Epoch [2/2], Step [35401/65307], Loss: 0.1051\n",
            "Epoch [2/2], Step [35451/65307], Loss: 0.1689\n",
            "Epoch [2/2], Step [35451/65307], Loss: 0.1689\n",
            "Epoch [2/2], Step [35501/65307], Loss: 0.0199\n",
            "Epoch [2/2], Step [35501/65307], Loss: 0.0199\n",
            "Epoch [2/2], Step [35551/65307], Loss: 0.0024\n",
            "Epoch [2/2], Step [35551/65307], Loss: 0.0024\n",
            "Epoch [2/2], Step [35601/65307], Loss: 0.2444\n",
            "Epoch [2/2], Step [35601/65307], Loss: 0.2444\n",
            "Epoch [2/2], Step [35651/65307], Loss: 0.4366\n",
            "Epoch [2/2], Step [35651/65307], Loss: 0.4366\n",
            "Epoch [2/2], Step [35701/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [35701/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [35751/65307], Loss: 0.1611\n",
            "Epoch [2/2], Step [35751/65307], Loss: 0.1611\n",
            "Epoch [2/2], Step [35801/65307], Loss: 0.0484\n",
            "Epoch [2/2], Step [35801/65307], Loss: 0.0484\n",
            "Epoch [2/2], Step [35851/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [35851/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [35901/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [35901/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [35951/65307], Loss: 0.1653\n",
            "Epoch [2/2], Step [35951/65307], Loss: 0.1653\n",
            "Epoch [2/2], Step [36001/65307], Loss: 0.1105\n",
            "Epoch [2/2], Step [36001/65307], Loss: 0.1105\n",
            "Epoch [2/2], Step [36051/65307], Loss: 0.3247\n",
            "Epoch [2/2], Step [36051/65307], Loss: 0.3247\n",
            "Epoch [2/2], Step [36101/65307], Loss: 0.3296\n",
            "Epoch [2/2], Step [36101/65307], Loss: 0.3296\n",
            "Epoch [2/2], Step [36151/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [36151/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [36201/65307], Loss: 0.0257\n",
            "Epoch [2/2], Step [36201/65307], Loss: 0.0257\n",
            "Epoch [2/2], Step [36251/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [36251/65307], Loss: 0.0147\n",
            "Epoch [2/2], Step [36301/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [36301/65307], Loss: 0.0489\n",
            "Epoch [2/2], Step [36351/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [36351/65307], Loss: 0.0302\n",
            "Epoch [2/2], Step [36401/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [36401/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [36451/65307], Loss: 0.0696\n",
            "Epoch [2/2], Step [36451/65307], Loss: 0.0696\n",
            "Epoch [2/2], Step [36501/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [36501/65307], Loss: 0.0238\n",
            "Epoch [2/2], Step [36551/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [36551/65307], Loss: 0.0049\n",
            "Epoch [2/2], Step [36601/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [36601/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [36651/65307], Loss: 0.0218\n",
            "Epoch [2/2], Step [36651/65307], Loss: 0.0218\n",
            "Epoch [2/2], Step [36701/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [36701/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [36751/65307], Loss: 0.4773\n",
            "Epoch [2/2], Step [36751/65307], Loss: 0.4773\n",
            "Epoch [2/2], Step [36801/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [36801/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [36851/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [36851/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [36901/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [36901/65307], Loss: 0.0524\n",
            "Epoch [2/2], Step [36951/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [36951/65307], Loss: 0.1170\n",
            "Epoch [2/2], Step [37001/65307], Loss: 0.0458\n",
            "Epoch [2/2], Step [37001/65307], Loss: 0.0458\n",
            "Epoch [2/2], Step [37051/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [37051/65307], Loss: 0.0603\n",
            "Epoch [2/2], Step [37101/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [37101/65307], Loss: 0.0103\n",
            "Epoch [2/2], Step [37151/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [37151/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [37201/65307], Loss: 0.0418\n",
            "Epoch [2/2], Step [37201/65307], Loss: 0.0418\n",
            "Epoch [2/2], Step [37251/65307], Loss: 0.2036\n",
            "Epoch [2/2], Step [37251/65307], Loss: 0.2036\n",
            "Epoch [2/2], Step [37301/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [37301/65307], Loss: 0.1768\n",
            "Epoch [2/2], Step [37351/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [37351/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [37401/65307], Loss: 0.0718\n",
            "Epoch [2/2], Step [37401/65307], Loss: 0.0718\n",
            "Epoch [2/2], Step [37451/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [37451/65307], Loss: 0.0278\n",
            "Epoch [2/2], Step [37501/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [37501/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [37551/65307], Loss: 0.1496\n",
            "Epoch [2/2], Step [37551/65307], Loss: 0.1496\n",
            "Epoch [2/2], Step [37601/65307], Loss: 0.5549\n",
            "Epoch [2/2], Step [37601/65307], Loss: 0.5549\n",
            "Epoch [2/2], Step [37651/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [37651/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [37701/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [37701/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [37751/65307], Loss: 0.1775\n",
            "Epoch [2/2], Step [37751/65307], Loss: 0.1775\n",
            "Epoch [2/2], Step [37801/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [37801/65307], Loss: 0.0639\n",
            "Epoch [2/2], Step [37851/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [37851/65307], Loss: 0.0922\n",
            "Epoch [2/2], Step [37901/65307], Loss: 0.4917\n",
            "Epoch [2/2], Step [37901/65307], Loss: 0.4917\n",
            "Epoch [2/2], Step [37951/65307], Loss: 0.2535\n",
            "Epoch [2/2], Step [37951/65307], Loss: 0.2535\n",
            "Epoch [2/2], Step [38001/65307], Loss: 0.0516\n",
            "Epoch [2/2], Step [38001/65307], Loss: 0.0516\n",
            "Epoch [2/2], Step [38051/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [38051/65307], Loss: 0.0251\n",
            "Epoch [2/2], Step [38101/65307], Loss: 0.1226\n",
            "Epoch [2/2], Step [38101/65307], Loss: 0.1226\n",
            "Epoch [2/2], Step [38151/65307], Loss: 0.0327\n",
            "Epoch [2/2], Step [38151/65307], Loss: 0.0327\n",
            "Epoch [2/2], Step [38201/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [38201/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [38251/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [38251/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [38301/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [38301/65307], Loss: 0.0065\n",
            "Epoch [2/2], Step [38351/65307], Loss: 0.1235\n",
            "Epoch [2/2], Step [38351/65307], Loss: 0.1235\n",
            "Epoch [2/2], Step [38401/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [38401/65307], Loss: 0.0397\n",
            "Epoch [2/2], Step [38451/65307], Loss: 0.0376\n",
            "Epoch [2/2], Step [38451/65307], Loss: 0.0376\n",
            "Epoch [2/2], Step [38501/65307], Loss: 0.0610\n",
            "Epoch [2/2], Step [38501/65307], Loss: 0.0610\n",
            "Epoch [2/2], Step [38551/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38551/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38601/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [38601/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [38651/65307], Loss: 0.0361\n",
            "Epoch [2/2], Step [38651/65307], Loss: 0.0361\n",
            "Epoch [2/2], Step [38701/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [38701/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [38751/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [38751/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [38801/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38801/65307], Loss: 0.0027\n",
            "Epoch [2/2], Step [38851/65307], Loss: 0.0711\n",
            "Epoch [2/2], Step [38851/65307], Loss: 0.0711\n",
            "Epoch [2/2], Step [38901/65307], Loss: 0.0123\n",
            "Epoch [2/2], Step [38901/65307], Loss: 0.0123\n",
            "Epoch [2/2], Step [38951/65307], Loss: 0.2177\n",
            "Epoch [2/2], Step [38951/65307], Loss: 0.2177\n",
            "Epoch [2/2], Step [39001/65307], Loss: 0.0368\n",
            "Epoch [2/2], Step [39001/65307], Loss: 0.0368\n",
            "Epoch [2/2], Step [39051/65307], Loss: 0.1826\n",
            "Epoch [2/2], Step [39051/65307], Loss: 0.1826\n",
            "Epoch [2/2], Step [39101/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [39101/65307], Loss: 0.0061\n",
            "Epoch [2/2], Step [39151/65307], Loss: 0.1132\n",
            "Epoch [2/2], Step [39151/65307], Loss: 0.1132\n",
            "Epoch [2/2], Step [39201/65307], Loss: 0.3011\n",
            "Epoch [2/2], Step [39201/65307], Loss: 0.3011\n",
            "Epoch [2/2], Step [39251/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [39251/65307], Loss: 0.2456\n",
            "Epoch [2/2], Step [39301/65307], Loss: 0.0554\n",
            "Epoch [2/2], Step [39301/65307], Loss: 0.0554\n",
            "Epoch [2/2], Step [39351/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [39351/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [39401/65307], Loss: 0.1627\n",
            "Epoch [2/2], Step [39401/65307], Loss: 0.1627\n",
            "Epoch [2/2], Step [39451/65307], Loss: 0.0184\n",
            "Epoch [2/2], Step [39451/65307], Loss: 0.0184\n",
            "Epoch [2/2], Step [39501/65307], Loss: 0.0148\n",
            "Epoch [2/2], Step [39501/65307], Loss: 0.0148\n",
            "Epoch [2/2], Step [39551/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [39551/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [39601/65307], Loss: 0.0775\n",
            "Epoch [2/2], Step [39601/65307], Loss: 0.0775\n",
            "Epoch [2/2], Step [39651/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [39651/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [39701/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [39701/65307], Loss: 0.0084\n",
            "Epoch [2/2], Step [39751/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [39751/65307], Loss: 0.1730\n",
            "Epoch [2/2], Step [39801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [39801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [39851/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [39851/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [39901/65307], Loss: 0.0313\n",
            "Epoch [2/2], Step [39901/65307], Loss: 0.0313\n",
            "Epoch [2/2], Step [39951/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [39951/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [40001/65307], Loss: 0.0800\n",
            "Epoch [2/2], Step [40001/65307], Loss: 0.0800\n",
            "Epoch [2/2], Step [40051/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [40051/65307], Loss: 0.0829\n",
            "Epoch [2/2], Step [40101/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [40101/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [40151/65307], Loss: 0.0538\n",
            "Epoch [2/2], Step [40151/65307], Loss: 0.0538\n",
            "Epoch [2/2], Step [40201/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [40201/65307], Loss: 0.0080\n",
            "Epoch [2/2], Step [40251/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [40251/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [40301/65307], Loss: 0.0369\n",
            "Epoch [2/2], Step [40301/65307], Loss: 0.0369\n",
            "Epoch [2/2], Step [40351/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [40351/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [40401/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [40401/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [40451/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [40451/65307], Loss: 0.0308\n",
            "Epoch [2/2], Step [40501/65307], Loss: 0.0787\n",
            "Epoch [2/2], Step [40501/65307], Loss: 0.0787\n",
            "Epoch [2/2], Step [40551/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [40551/65307], Loss: 0.0156\n",
            "Epoch [2/2], Step [40601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [40601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [40651/65307], Loss: 0.1458\n",
            "Epoch [2/2], Step [40651/65307], Loss: 0.1458\n",
            "Epoch [2/2], Step [40701/65307], Loss: 0.1862\n",
            "Epoch [2/2], Step [40701/65307], Loss: 0.1862\n",
            "Epoch [2/2], Step [40751/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [40751/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [40801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [40801/65307], Loss: 0.0020\n",
            "Epoch [2/2], Step [40851/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [40851/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [40901/65307], Loss: 0.0172\n",
            "Epoch [2/2], Step [40901/65307], Loss: 0.0172\n",
            "Epoch [2/2], Step [40951/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [40951/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [41001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [41001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [41051/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [41051/65307], Loss: 0.1191\n",
            "Epoch [2/2], Step [41101/65307], Loss: 0.0552\n",
            "Epoch [2/2], Step [41101/65307], Loss: 0.0552\n",
            "Epoch [2/2], Step [41151/65307], Loss: 0.3584\n",
            "Epoch [2/2], Step [41151/65307], Loss: 0.3584\n",
            "Epoch [2/2], Step [41201/65307], Loss: 0.0870\n",
            "Epoch [2/2], Step [41201/65307], Loss: 0.0870\n",
            "Epoch [2/2], Step [41251/65307], Loss: 0.0766\n",
            "Epoch [2/2], Step [41251/65307], Loss: 0.0766\n",
            "Epoch [2/2], Step [41301/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [41301/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [41351/65307], Loss: 0.0811\n",
            "Epoch [2/2], Step [41351/65307], Loss: 0.0811\n",
            "Epoch [2/2], Step [41401/65307], Loss: 0.0254\n",
            "Epoch [2/2], Step [41401/65307], Loss: 0.0254\n",
            "Epoch [2/2], Step [41451/65307], Loss: 0.2074\n",
            "Epoch [2/2], Step [41451/65307], Loss: 0.2074\n",
            "Epoch [2/2], Step [41501/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [41501/65307], Loss: 0.0590\n",
            "Epoch [2/2], Step [41551/65307], Loss: 0.3000\n",
            "Epoch [2/2], Step [41551/65307], Loss: 0.3000\n",
            "Epoch [2/2], Step [41601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [41601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [41651/65307], Loss: 0.0158\n",
            "Epoch [2/2], Step [41651/65307], Loss: 0.0158\n",
            "Epoch [2/2], Step [41701/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [41701/65307], Loss: 0.3446\n",
            "Epoch [2/2], Step [41751/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [41751/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [41801/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [41801/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [41851/65307], Loss: 0.1453\n",
            "Epoch [2/2], Step [41851/65307], Loss: 0.1453\n",
            "Epoch [2/2], Step [41901/65307], Loss: 0.0511\n",
            "Epoch [2/2], Step [41901/65307], Loss: 0.0511\n",
            "Epoch [2/2], Step [41951/65307], Loss: 0.1081\n",
            "Epoch [2/2], Step [41951/65307], Loss: 0.1081\n",
            "Epoch [2/2], Step [42001/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [42001/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [42051/65307], Loss: 0.0390\n",
            "Epoch [2/2], Step [42051/65307], Loss: 0.0390\n",
            "Epoch [2/2], Step [42101/65307], Loss: 0.1650\n",
            "Epoch [2/2], Step [42101/65307], Loss: 0.1650\n",
            "Epoch [2/2], Step [42151/65307], Loss: 0.1683\n",
            "Epoch [2/2], Step [42151/65307], Loss: 0.1683\n",
            "Epoch [2/2], Step [42201/65307], Loss: 0.3551\n",
            "Epoch [2/2], Step [42201/65307], Loss: 0.3551\n",
            "Epoch [2/2], Step [42251/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [42251/65307], Loss: 0.0743\n",
            "Epoch [2/2], Step [42301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [42301/65307], Loss: 0.0735\n",
            "Epoch [2/2], Step [42351/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [42351/65307], Loss: 0.1166\n",
            "Epoch [2/2], Step [42401/65307], Loss: 0.0936\n",
            "Epoch [2/2], Step [42401/65307], Loss: 0.0936\n",
            "Epoch [2/2], Step [42451/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [42451/65307], Loss: 0.0449\n",
            "Epoch [2/2], Step [42501/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [42501/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [42551/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [42551/65307], Loss: 0.0404\n",
            "Epoch [2/2], Step [42601/65307], Loss: 0.1201\n",
            "Epoch [2/2], Step [42601/65307], Loss: 0.1201\n",
            "Epoch [2/2], Step [42651/65307], Loss: 0.2641\n",
            "Epoch [2/2], Step [42651/65307], Loss: 0.2641\n",
            "Epoch [2/2], Step [42701/65307], Loss: 0.0685\n",
            "Epoch [2/2], Step [42701/65307], Loss: 0.0685\n",
            "Epoch [2/2], Step [42751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [42751/65307], Loss: 0.0401\n",
            "Epoch [2/2], Step [42801/65307], Loss: 0.0176\n",
            "Epoch [2/2], Step [42801/65307], Loss: 0.0176\n",
            "Epoch [2/2], Step [42851/65307], Loss: 0.2481\n",
            "Epoch [2/2], Step [42851/65307], Loss: 0.2481\n",
            "Epoch [2/2], Step [42901/65307], Loss: 0.1785\n",
            "Epoch [2/2], Step [42901/65307], Loss: 0.1785\n",
            "Epoch [2/2], Step [42951/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [42951/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [43001/65307], Loss: 0.0078\n",
            "Epoch [2/2], Step [43001/65307], Loss: 0.0078\n",
            "Epoch [2/2], Step [43051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [43051/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [43101/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43101/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43151/65307], Loss: 0.2728\n",
            "Epoch [2/2], Step [43151/65307], Loss: 0.2728\n",
            "Epoch [2/2], Step [43201/65307], Loss: 0.0930\n",
            "Epoch [2/2], Step [43201/65307], Loss: 0.0930\n",
            "Epoch [2/2], Step [43251/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43251/65307], Loss: 0.0370\n",
            "Epoch [2/2], Step [43301/65307], Loss: 0.4954\n",
            "Epoch [2/2], Step [43301/65307], Loss: 0.4954\n",
            "Epoch [2/2], Step [43351/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [43351/65307], Loss: 0.0882\n",
            "Epoch [2/2], Step [43401/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [43401/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [43451/65307], Loss: 0.0710\n",
            "Epoch [2/2], Step [43451/65307], Loss: 0.0710\n",
            "Epoch [2/2], Step [43501/65307], Loss: 0.1013\n",
            "Epoch [2/2], Step [43501/65307], Loss: 0.1013\n",
            "Epoch [2/2], Step [43551/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [43551/65307], Loss: 0.0286\n",
            "Epoch [2/2], Step [43601/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [43601/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [43651/65307], Loss: 0.0112\n",
            "Epoch [2/2], Step [43651/65307], Loss: 0.0112\n",
            "Epoch [2/2], Step [43701/65307], Loss: 0.2337\n",
            "Epoch [2/2], Step [43701/65307], Loss: 0.2337\n",
            "Epoch [2/2], Step [43751/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [43751/65307], Loss: 0.0165\n",
            "Epoch [2/2], Step [43801/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [43801/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [43851/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [43851/65307], Loss: 0.0886\n",
            "Epoch [2/2], Step [43901/65307], Loss: 0.2230\n",
            "Epoch [2/2], Step [43901/65307], Loss: 0.2230\n",
            "Epoch [2/2], Step [43951/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [43951/65307], Loss: 0.0033\n",
            "Epoch [2/2], Step [44001/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [44001/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [44051/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [44051/65307], Loss: 0.0624\n",
            "Epoch [2/2], Step [44101/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [44101/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [44151/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [44151/65307], Loss: 0.0162\n",
            "Epoch [2/2], Step [44201/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [44201/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [44251/65307], Loss: 0.2709\n",
            "Epoch [2/2], Step [44251/65307], Loss: 0.2709\n",
            "Epoch [2/2], Step [44301/65307], Loss: 0.4022\n",
            "Epoch [2/2], Step [44301/65307], Loss: 0.4022\n",
            "Epoch [2/2], Step [44351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [44351/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [44401/65307], Loss: 0.1215\n",
            "Epoch [2/2], Step [44401/65307], Loss: 0.1215\n",
            "Epoch [2/2], Step [44451/65307], Loss: 0.1006\n",
            "Epoch [2/2], Step [44451/65307], Loss: 0.1006\n",
            "Epoch [2/2], Step [44501/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [44501/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [44551/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [44551/65307], Loss: 0.0772\n",
            "Epoch [2/2], Step [44601/65307], Loss: 0.0100\n",
            "Epoch [2/2], Step [44601/65307], Loss: 0.0100\n",
            "Epoch [2/2], Step [44651/65307], Loss: 0.2645\n",
            "Epoch [2/2], Step [44651/65307], Loss: 0.2645\n",
            "Epoch [2/2], Step [44701/65307], Loss: 0.5550\n",
            "Epoch [2/2], Step [44701/65307], Loss: 0.5550\n",
            "Epoch [2/2], Step [44751/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [44751/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [44801/65307], Loss: 0.0522\n",
            "Epoch [2/2], Step [44801/65307], Loss: 0.0522\n",
            "Epoch [2/2], Step [44851/65307], Loss: 0.2464\n",
            "Epoch [2/2], Step [44851/65307], Loss: 0.2464\n",
            "Epoch [2/2], Step [44901/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [44901/65307], Loss: 0.1184\n",
            "Epoch [2/2], Step [44951/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [44951/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [45001/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [45001/65307], Loss: 0.0222\n",
            "Epoch [2/2], Step [45051/65307], Loss: 0.0765\n",
            "Epoch [2/2], Step [45051/65307], Loss: 0.0765\n",
            "Epoch [2/2], Step [45101/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [45101/65307], Loss: 0.1122\n",
            "Epoch [2/2], Step [45151/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [45151/65307], Loss: 0.0201\n",
            "Epoch [2/2], Step [45201/65307], Loss: 0.4553\n",
            "Epoch [2/2], Step [45201/65307], Loss: 0.4553\n",
            "Epoch [2/2], Step [45251/65307], Loss: 0.2395\n",
            "Epoch [2/2], Step [45251/65307], Loss: 0.2395\n",
            "Epoch [2/2], Step [45301/65307], Loss: 0.1239\n",
            "Epoch [2/2], Step [45301/65307], Loss: 0.1239\n",
            "Epoch [2/2], Step [45351/65307], Loss: 0.1291\n",
            "Epoch [2/2], Step [45351/65307], Loss: 0.1291\n",
            "Epoch [2/2], Step [45401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [45401/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [45451/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [45451/65307], Loss: 0.1145\n",
            "Epoch [2/2], Step [45501/65307], Loss: 0.0774\n",
            "Epoch [2/2], Step [45501/65307], Loss: 0.0774\n",
            "Epoch [2/2], Step [45551/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [45551/65307], Loss: 0.0075\n",
            "Epoch [2/2], Step [45601/65307], Loss: 0.1254\n",
            "Epoch [2/2], Step [45601/65307], Loss: 0.1254\n",
            "Epoch [2/2], Step [45651/65307], Loss: 0.0910\n",
            "Epoch [2/2], Step [45651/65307], Loss: 0.0910\n",
            "Epoch [2/2], Step [45701/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [45701/65307], Loss: 0.0010\n",
            "Epoch [2/2], Step [45751/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [45751/65307], Loss: 0.0454\n",
            "Epoch [2/2], Step [45801/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [45801/65307], Loss: 0.0347\n",
            "Epoch [2/2], Step [45851/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [45851/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [45901/65307], Loss: 0.2027\n",
            "Epoch [2/2], Step [45901/65307], Loss: 0.2027\n",
            "Epoch [2/2], Step [45951/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [45951/65307], Loss: 0.0793\n",
            "Epoch [2/2], Step [46001/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [46001/65307], Loss: 0.0107\n",
            "Epoch [2/2], Step [46051/65307], Loss: 0.0116\n",
            "Epoch [2/2], Step [46051/65307], Loss: 0.0116\n",
            "Epoch [2/2], Step [46101/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [46101/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [46151/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [46151/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [46201/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [46201/65307], Loss: 0.0309\n",
            "Epoch [2/2], Step [46251/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [46251/65307], Loss: 0.0071\n",
            "Epoch [2/2], Step [46301/65307], Loss: 0.0867\n",
            "Epoch [2/2], Step [46301/65307], Loss: 0.0867\n",
            "Epoch [2/2], Step [46351/65307], Loss: 0.0640\n",
            "Epoch [2/2], Step [46351/65307], Loss: 0.0640\n",
            "Epoch [2/2], Step [46401/65307], Loss: 0.0884\n",
            "Epoch [2/2], Step [46401/65307], Loss: 0.0884\n",
            "Epoch [2/2], Step [46451/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [46451/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [46501/65307], Loss: 0.2494\n",
            "Epoch [2/2], Step [46501/65307], Loss: 0.2494\n",
            "Epoch [2/2], Step [46551/65307], Loss: 0.0568\n",
            "Epoch [2/2], Step [46551/65307], Loss: 0.0568\n",
            "Epoch [2/2], Step [46601/65307], Loss: 0.0998\n",
            "Epoch [2/2], Step [46601/65307], Loss: 0.0998\n",
            "Epoch [2/2], Step [46651/65307], Loss: 0.4177\n",
            "Epoch [2/2], Step [46651/65307], Loss: 0.4177\n",
            "Epoch [2/2], Step [46701/65307], Loss: 0.0615\n",
            "Epoch [2/2], Step [46701/65307], Loss: 0.0615\n",
            "Epoch [2/2], Step [46751/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [46751/65307], Loss: 0.0551\n",
            "Epoch [2/2], Step [46801/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [46801/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [46851/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [46851/65307], Loss: 0.1161\n",
            "Epoch [2/2], Step [46901/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [46901/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [46951/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [46951/65307], Loss: 0.0594\n",
            "Epoch [2/2], Step [47001/65307], Loss: 0.1865\n",
            "Epoch [2/2], Step [47001/65307], Loss: 0.1865\n",
            "Epoch [2/2], Step [47051/65307], Loss: 0.0759\n",
            "Epoch [2/2], Step [47051/65307], Loss: 0.0759\n",
            "Epoch [2/2], Step [47101/65307], Loss: 0.1173\n",
            "Epoch [2/2], Step [47101/65307], Loss: 0.1173\n",
            "Epoch [2/2], Step [47151/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [47151/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [47201/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [47201/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [47251/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [47251/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [47301/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [47301/65307], Loss: 0.1242\n",
            "Epoch [2/2], Step [47351/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [47351/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [47401/65307], Loss: 0.0208\n",
            "Epoch [2/2], Step [47401/65307], Loss: 0.0208\n",
            "Epoch [2/2], Step [47451/65307], Loss: 0.0926\n",
            "Epoch [2/2], Step [47451/65307], Loss: 0.0926\n",
            "Epoch [2/2], Step [47501/65307], Loss: 0.0636\n",
            "Epoch [2/2], Step [47501/65307], Loss: 0.0636\n",
            "Epoch [2/2], Step [47551/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [47551/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [47601/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [47601/65307], Loss: 0.0492\n",
            "Epoch [2/2], Step [47651/65307], Loss: 0.1190\n",
            "Epoch [2/2], Step [47651/65307], Loss: 0.1190\n",
            "Epoch [2/2], Step [47701/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [47701/65307], Loss: 0.0018\n",
            "Epoch [2/2], Step [47751/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [47751/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [47801/65307], Loss: 0.5097\n",
            "Epoch [2/2], Step [47801/65307], Loss: 0.5097\n",
            "Epoch [2/2], Step [47851/65307], Loss: 0.0334\n",
            "Epoch [2/2], Step [47851/65307], Loss: 0.0334\n",
            "Epoch [2/2], Step [47901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [47901/65307], Loss: 0.0015\n",
            "Epoch [2/2], Step [47951/65307], Loss: 0.3728\n",
            "Epoch [2/2], Step [47951/65307], Loss: 0.3728\n",
            "Epoch [2/2], Step [48001/65307], Loss: 0.0483\n",
            "Epoch [2/2], Step [48001/65307], Loss: 0.0483\n",
            "Epoch [2/2], Step [48051/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [48051/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [48101/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [48101/65307], Loss: 0.0207\n",
            "Epoch [2/2], Step [48151/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48151/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48201/65307], Loss: 0.1059\n",
            "Epoch [2/2], Step [48201/65307], Loss: 0.1059\n",
            "Epoch [2/2], Step [48251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48251/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [48301/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [48301/65307], Loss: 0.0324\n",
            "Epoch [2/2], Step [48351/65307], Loss: 0.1137\n",
            "Epoch [2/2], Step [48351/65307], Loss: 0.1137\n",
            "Epoch [2/2], Step [48401/65307], Loss: 0.0719\n",
            "Epoch [2/2], Step [48401/65307], Loss: 0.0719\n",
            "Epoch [2/2], Step [48451/65307], Loss: 0.0478\n",
            "Epoch [2/2], Step [48451/65307], Loss: 0.0478\n",
            "Epoch [2/2], Step [48501/65307], Loss: 0.0099\n",
            "Epoch [2/2], Step [48501/65307], Loss: 0.0099\n",
            "Epoch [2/2], Step [48551/65307], Loss: 0.2995\n",
            "Epoch [2/2], Step [48551/65307], Loss: 0.2995\n",
            "Epoch [2/2], Step [48601/65307], Loss: 0.0734\n",
            "Epoch [2/2], Step [48601/65307], Loss: 0.0734\n",
            "Epoch [2/2], Step [48651/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [48651/65307], Loss: 0.0631\n",
            "Epoch [2/2], Step [48701/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [48701/65307], Loss: 0.0160\n",
            "Epoch [2/2], Step [48751/65307], Loss: 0.0141\n",
            "Epoch [2/2], Step [48751/65307], Loss: 0.0141\n",
            "Epoch [2/2], Step [48801/65307], Loss: 0.1248\n",
            "Epoch [2/2], Step [48801/65307], Loss: 0.1248\n",
            "Epoch [2/2], Step [48851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [48851/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [48901/65307], Loss: 0.0091\n",
            "Epoch [2/2], Step [48901/65307], Loss: 0.0091\n",
            "Epoch [2/2], Step [48951/65307], Loss: 0.1681\n",
            "Epoch [2/2], Step [48951/65307], Loss: 0.1681\n",
            "Epoch [2/2], Step [49001/65307], Loss: 0.0350\n",
            "Epoch [2/2], Step [49001/65307], Loss: 0.0350\n",
            "Epoch [2/2], Step [49051/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [49051/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [49101/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [49101/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [49151/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [49151/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [49201/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [49201/65307], Loss: 0.1289\n",
            "Epoch [2/2], Step [49251/65307], Loss: 0.0707\n",
            "Epoch [2/2], Step [49251/65307], Loss: 0.0707\n",
            "Epoch [2/2], Step [49301/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [49301/65307], Loss: 0.0550\n",
            "Epoch [2/2], Step [49351/65307], Loss: 0.0460\n",
            "Epoch [2/2], Step [49351/65307], Loss: 0.0460\n",
            "Epoch [2/2], Step [49401/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [49401/65307], Loss: 0.0206\n",
            "Epoch [2/2], Step [49451/65307], Loss: 0.2863\n",
            "Epoch [2/2], Step [49451/65307], Loss: 0.2863\n",
            "Epoch [2/2], Step [49501/65307], Loss: 0.1341\n",
            "Epoch [2/2], Step [49501/65307], Loss: 0.1341\n",
            "Epoch [2/2], Step [49551/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [49551/65307], Loss: 0.0855\n",
            "Epoch [2/2], Step [49601/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [49601/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [49651/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [49651/65307], Loss: 0.0398\n",
            "Epoch [2/2], Step [49701/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [49701/65307], Loss: 0.0264\n",
            "Epoch [2/2], Step [49751/65307], Loss: 0.0877\n",
            "Epoch [2/2], Step [49751/65307], Loss: 0.0877\n",
            "Epoch [2/2], Step [49801/65307], Loss: 0.1120\n",
            "Epoch [2/2], Step [49801/65307], Loss: 0.1120\n",
            "Epoch [2/2], Step [49851/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [49851/65307], Loss: 0.0058\n",
            "Epoch [2/2], Step [49901/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [49901/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [49951/65307], Loss: 0.0453\n",
            "Epoch [2/2], Step [49951/65307], Loss: 0.0453\n",
            "Epoch [2/2], Step [50001/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [50001/65307], Loss: 0.0392\n",
            "Epoch [2/2], Step [50051/65307], Loss: 0.1012\n",
            "Epoch [2/2], Step [50051/65307], Loss: 0.1012\n",
            "Epoch [2/2], Step [50101/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [50101/65307], Loss: 0.0234\n",
            "Epoch [2/2], Step [50151/65307], Loss: 0.0282\n",
            "Epoch [2/2], Step [50151/65307], Loss: 0.0282\n",
            "Epoch [2/2], Step [50201/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [50201/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [50251/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [50251/65307], Loss: 0.0175\n",
            "Epoch [2/2], Step [50301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [50301/65307], Loss: 0.1755\n",
            "Epoch [2/2], Step [50351/65307], Loss: 0.0913\n",
            "Epoch [2/2], Step [50351/65307], Loss: 0.0913\n",
            "Epoch [2/2], Step [50401/65307], Loss: 0.0586\n",
            "Epoch [2/2], Step [50401/65307], Loss: 0.0586\n",
            "Epoch [2/2], Step [50451/65307], Loss: 0.0767\n",
            "Epoch [2/2], Step [50451/65307], Loss: 0.0767\n",
            "Epoch [2/2], Step [50501/65307], Loss: 0.1093\n",
            "Epoch [2/2], Step [50501/65307], Loss: 0.1093\n",
            "Epoch [2/2], Step [50551/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [50551/65307], Loss: 0.1041\n",
            "Epoch [2/2], Step [50601/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [50601/65307], Loss: 0.0146\n",
            "Epoch [2/2], Step [50651/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [50651/65307], Loss: 0.0111\n",
            "Epoch [2/2], Step [50701/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [50701/65307], Loss: 0.0378\n",
            "Epoch [2/2], Step [50751/65307], Loss: 0.0385\n",
            "Epoch [2/2], Step [50751/65307], Loss: 0.0385\n",
            "Epoch [2/2], Step [50801/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [50801/65307], Loss: 0.1142\n",
            "Epoch [2/2], Step [50851/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [50851/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [50901/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [50901/65307], Loss: 0.0038\n",
            "Epoch [2/2], Step [50951/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [50951/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [51001/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [51001/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [51051/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [51051/65307], Loss: 0.0044\n",
            "Epoch [2/2], Step [51101/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [51101/65307], Loss: 0.0596\n",
            "Epoch [2/2], Step [51151/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [51151/65307], Loss: 0.0262\n",
            "Epoch [2/2], Step [51201/65307], Loss: 0.0306\n",
            "Epoch [2/2], Step [51201/65307], Loss: 0.0306\n",
            "Epoch [2/2], Step [51251/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [51251/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [51301/65307], Loss: 0.0499\n",
            "Epoch [2/2], Step [51301/65307], Loss: 0.0499\n",
            "Epoch [2/2], Step [51351/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [51351/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [51401/65307], Loss: 0.0652\n",
            "Epoch [2/2], Step [51401/65307], Loss: 0.0652\n",
            "Epoch [2/2], Step [51451/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [51451/65307], Loss: 0.1379\n",
            "Epoch [2/2], Step [51501/65307], Loss: 0.0452\n",
            "Epoch [2/2], Step [51501/65307], Loss: 0.0452\n",
            "Epoch [2/2], Step [51551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [51551/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [51601/65307], Loss: 0.0537\n",
            "Epoch [2/2], Step [51601/65307], Loss: 0.0537\n",
            "Epoch [2/2], Step [51651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [51651/65307], Loss: 0.0889\n",
            "Epoch [2/2], Step [51701/65307], Loss: 0.0749\n",
            "Epoch [2/2], Step [51701/65307], Loss: 0.0749\n",
            "Epoch [2/2], Step [51751/65307], Loss: 0.0009\n",
            "Epoch [2/2], Step [51751/65307], Loss: 0.0009\n",
            "Epoch [2/2], Step [51801/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [51801/65307], Loss: 0.0064\n",
            "Epoch [2/2], Step [51851/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [51851/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [51901/65307], Loss: 0.2776\n",
            "Epoch [2/2], Step [51901/65307], Loss: 0.2776\n",
            "Epoch [2/2], Step [51951/65307], Loss: 0.1714\n",
            "Epoch [2/2], Step [51951/65307], Loss: 0.1714\n",
            "Epoch [2/2], Step [52001/65307], Loss: 0.1529\n",
            "Epoch [2/2], Step [52001/65307], Loss: 0.1529\n",
            "Epoch [2/2], Step [52051/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [52051/65307], Loss: 0.0543\n",
            "Epoch [2/2], Step [52101/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [52101/65307], Loss: 0.0011\n",
            "Epoch [2/2], Step [52151/65307], Loss: 0.0069\n",
            "Epoch [2/2], Step [52151/65307], Loss: 0.0069\n",
            "Epoch [2/2], Step [52201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [52201/65307], Loss: 0.0034\n",
            "Epoch [2/2], Step [52251/65307], Loss: 0.1896\n",
            "Epoch [2/2], Step [52251/65307], Loss: 0.1896\n",
            "Epoch [2/2], Step [52301/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [52301/65307], Loss: 0.0079\n",
            "Epoch [2/2], Step [52351/65307], Loss: 0.0155\n",
            "Epoch [2/2], Step [52351/65307], Loss: 0.0155\n",
            "Epoch [2/2], Step [52401/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [52401/65307], Loss: 0.0062\n",
            "Epoch [2/2], Step [52451/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [52451/65307], Loss: 0.0089\n",
            "Epoch [2/2], Step [52501/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [52501/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [52551/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [52551/65307], Loss: 0.0028\n",
            "Epoch [2/2], Step [52601/65307], Loss: 0.0745\n",
            "Epoch [2/2], Step [52601/65307], Loss: 0.0745\n",
            "Epoch [2/2], Step [52651/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [52651/65307], Loss: 0.0200\n",
            "Epoch [2/2], Step [52701/65307], Loss: 0.1756\n",
            "Epoch [2/2], Step [52701/65307], Loss: 0.1756\n",
            "Epoch [2/2], Step [52751/65307], Loss: 0.0479\n",
            "Epoch [2/2], Step [52751/65307], Loss: 0.0479\n",
            "Epoch [2/2], Step [52801/65307], Loss: 0.0541\n",
            "Epoch [2/2], Step [52801/65307], Loss: 0.0541\n",
            "Epoch [2/2], Step [52851/65307], Loss: 0.1304\n",
            "Epoch [2/2], Step [52851/65307], Loss: 0.1304\n",
            "Epoch [2/2], Step [52901/65307], Loss: 0.4685\n",
            "Epoch [2/2], Step [52901/65307], Loss: 0.4685\n",
            "Epoch [2/2], Step [52951/65307], Loss: 0.1106\n",
            "Epoch [2/2], Step [52951/65307], Loss: 0.1106\n",
            "Epoch [2/2], Step [53001/65307], Loss: 0.1432\n",
            "Epoch [2/2], Step [53001/65307], Loss: 0.1432\n",
            "Epoch [2/2], Step [53051/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [53051/65307], Loss: 0.0700\n",
            "Epoch [2/2], Step [53101/65307], Loss: 0.2140\n",
            "Epoch [2/2], Step [53101/65307], Loss: 0.2140\n",
            "Epoch [2/2], Step [53151/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [53151/65307], Loss: 0.0613\n",
            "Epoch [2/2], Step [53201/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [53201/65307], Loss: 0.0180\n",
            "Epoch [2/2], Step [53251/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [53251/65307], Loss: 0.0110\n",
            "Epoch [2/2], Step [53301/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [53301/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [53351/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [53351/65307], Loss: 0.0106\n",
            "Epoch [2/2], Step [53401/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [53401/65307], Loss: 0.0192\n",
            "Epoch [2/2], Step [53451/65307], Loss: 0.0124\n",
            "Epoch [2/2], Step [53451/65307], Loss: 0.0124\n",
            "Epoch [2/2], Step [53501/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [53501/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [53551/65307], Loss: 0.1112\n",
            "Epoch [2/2], Step [53551/65307], Loss: 0.1112\n",
            "Epoch [2/2], Step [53601/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [53601/65307], Loss: 0.0190\n",
            "Epoch [2/2], Step [53651/65307], Loss: 0.0748\n",
            "Epoch [2/2], Step [53651/65307], Loss: 0.0748\n",
            "Epoch [2/2], Step [53701/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [53701/65307], Loss: 0.2057\n",
            "Epoch [2/2], Step [53751/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [53751/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [53801/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [53801/65307], Loss: 0.0283\n",
            "Epoch [2/2], Step [53851/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [53851/65307], Loss: 0.0477\n",
            "Epoch [2/2], Step [53901/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [53901/65307], Loss: 0.0242\n",
            "Epoch [2/2], Step [53951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [53951/65307], Loss: 0.0037\n",
            "Epoch [2/2], Step [54001/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [54001/65307], Loss: 0.0026\n",
            "Epoch [2/2], Step [54051/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [54051/65307], Loss: 0.0013\n",
            "Epoch [2/2], Step [54101/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54101/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54151/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [54151/65307], Loss: 0.0017\n",
            "Epoch [2/2], Step [54201/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [54201/65307], Loss: 0.0040\n",
            "Epoch [2/2], Step [54251/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [54251/65307], Loss: 0.0276\n",
            "Epoch [2/2], Step [54301/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [54301/65307], Loss: 0.0048\n",
            "Epoch [2/2], Step [54351/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [54351/65307], Loss: 0.0908\n",
            "Epoch [2/2], Step [54401/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [54401/65307], Loss: 0.0138\n",
            "Epoch [2/2], Step [54451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [54451/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [54501/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [54501/65307], Loss: 0.0120\n",
            "Epoch [2/2], Step [54551/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [54551/65307], Loss: 0.0135\n",
            "Epoch [2/2], Step [54601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [54601/65307], Loss: 0.0031\n",
            "Epoch [2/2], Step [54651/65307], Loss: 0.1306\n",
            "Epoch [2/2], Step [54651/65307], Loss: 0.1306\n",
            "Epoch [2/2], Step [54701/65307], Loss: 0.0971\n",
            "Epoch [2/2], Step [54701/65307], Loss: 0.0971\n",
            "Epoch [2/2], Step [54751/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54751/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [54801/65307], Loss: 0.1350\n",
            "Epoch [2/2], Step [54801/65307], Loss: 0.1350\n",
            "Epoch [2/2], Step [54851/65307], Loss: 0.0125\n",
            "Epoch [2/2], Step [54851/65307], Loss: 0.0125\n",
            "Epoch [2/2], Step [54901/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [54901/65307], Loss: 0.0732\n",
            "Epoch [2/2], Step [54951/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [54951/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [55001/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [55001/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [55051/65307], Loss: 0.2673\n",
            "Epoch [2/2], Step [55051/65307], Loss: 0.2673\n",
            "Epoch [2/2], Step [55101/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [55101/65307], Loss: 0.0203\n",
            "Epoch [2/2], Step [55151/65307], Loss: 0.0893\n",
            "Epoch [2/2], Step [55151/65307], Loss: 0.0893\n",
            "Epoch [2/2], Step [55201/65307], Loss: 0.1347\n",
            "Epoch [2/2], Step [55201/65307], Loss: 0.1347\n",
            "Epoch [2/2], Step [55251/65307], Loss: 0.0070\n",
            "Epoch [2/2], Step [55251/65307], Loss: 0.0070\n",
            "Epoch [2/2], Step [55301/65307], Loss: 0.0933\n",
            "Epoch [2/2], Step [55301/65307], Loss: 0.0933\n",
            "Epoch [2/2], Step [55351/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [55351/65307], Loss: 0.0098\n",
            "Epoch [2/2], Step [55401/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [55401/65307], Loss: 0.0411\n",
            "Epoch [2/2], Step [55451/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [55451/65307], Loss: 0.0050\n",
            "Epoch [2/2], Step [55501/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [55501/65307], Loss: 0.0059\n",
            "Epoch [2/2], Step [55551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [55551/65307], Loss: 0.0016\n",
            "Epoch [2/2], Step [55601/65307], Loss: 0.1037\n",
            "Epoch [2/2], Step [55601/65307], Loss: 0.1037\n",
            "Epoch [2/2], Step [55651/65307], Loss: 0.1508\n",
            "Epoch [2/2], Step [55651/65307], Loss: 0.1508\n",
            "Epoch [2/2], Step [55701/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [55701/65307], Loss: 0.0671\n",
            "Epoch [2/2], Step [55751/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [55751/65307], Loss: 0.0008\n",
            "Epoch [2/2], Step [55801/65307], Loss: 0.0621\n",
            "Epoch [2/2], Step [55801/65307], Loss: 0.0621\n",
            "Epoch [2/2], Step [55851/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [55851/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [55901/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [55901/65307], Loss: 0.0473\n",
            "Epoch [2/2], Step [55951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [55951/65307], Loss: 0.0414\n",
            "Epoch [2/2], Step [56001/65307], Loss: 0.3520\n",
            "Epoch [2/2], Step [56001/65307], Loss: 0.3520\n",
            "Epoch [2/2], Step [56051/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [56051/65307], Loss: 0.0260\n",
            "Epoch [2/2], Step [56101/65307], Loss: 0.1770\n",
            "Epoch [2/2], Step [56101/65307], Loss: 0.1770\n",
            "Epoch [2/2], Step [56151/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [56151/65307], Loss: 0.0082\n",
            "Epoch [2/2], Step [56201/65307], Loss: 0.1687\n",
            "Epoch [2/2], Step [56201/65307], Loss: 0.1687\n",
            "Epoch [2/2], Step [56251/65307], Loss: 0.2714\n",
            "Epoch [2/2], Step [56251/65307], Loss: 0.2714\n",
            "Epoch [2/2], Step [56301/65307], Loss: 0.0814\n",
            "Epoch [2/2], Step [56301/65307], Loss: 0.0814\n",
            "Epoch [2/2], Step [56351/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [56351/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [56401/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [56401/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [56451/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [56451/65307], Loss: 0.0285\n",
            "Epoch [2/2], Step [56501/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [56501/65307], Loss: 0.0344\n",
            "Epoch [2/2], Step [56551/65307], Loss: 0.1685\n",
            "Epoch [2/2], Step [56551/65307], Loss: 0.1685\n",
            "Epoch [2/2], Step [56601/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [56601/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [56651/65307], Loss: 0.0751\n",
            "Epoch [2/2], Step [56651/65307], Loss: 0.0751\n",
            "Epoch [2/2], Step [56701/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [56701/65307], Loss: 0.1389\n",
            "Epoch [2/2], Step [56751/65307], Loss: 0.0761\n",
            "Epoch [2/2], Step [56751/65307], Loss: 0.0761\n",
            "Epoch [2/2], Step [56801/65307], Loss: 0.0699\n",
            "Epoch [2/2], Step [56801/65307], Loss: 0.0699\n",
            "Epoch [2/2], Step [56851/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [56851/65307], Loss: 0.0387\n",
            "Epoch [2/2], Step [56901/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [56901/65307], Loss: 0.0419\n",
            "Epoch [2/2], Step [56951/65307], Loss: 0.1647\n",
            "Epoch [2/2], Step [56951/65307], Loss: 0.1647\n",
            "Epoch [2/2], Step [57001/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [57001/65307], Loss: 0.0252\n",
            "Epoch [2/2], Step [57051/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [57051/65307], Loss: 0.0412\n",
            "Epoch [2/2], Step [57101/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [57101/65307], Loss: 0.0030\n",
            "Epoch [2/2], Step [57151/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [57151/65307], Loss: 0.0402\n",
            "Epoch [2/2], Step [57201/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [57201/65307], Loss: 0.0067\n",
            "Epoch [2/2], Step [57251/65307], Loss: 0.1115\n",
            "Epoch [2/2], Step [57251/65307], Loss: 0.1115\n",
            "Epoch [2/2], Step [57301/65307], Loss: 0.1591\n",
            "Epoch [2/2], Step [57301/65307], Loss: 0.1591\n",
            "Epoch [2/2], Step [57351/65307], Loss: 0.0666\n",
            "Epoch [2/2], Step [57351/65307], Loss: 0.0666\n",
            "Epoch [2/2], Step [57401/65307], Loss: 0.2468\n",
            "Epoch [2/2], Step [57401/65307], Loss: 0.2468\n",
            "Epoch [2/2], Step [57451/65307], Loss: 0.1850\n",
            "Epoch [2/2], Step [57451/65307], Loss: 0.1850\n",
            "Epoch [2/2], Step [57501/65307], Loss: 0.0909\n",
            "Epoch [2/2], Step [57501/65307], Loss: 0.0909\n",
            "Epoch [2/2], Step [57551/65307], Loss: 0.0240\n",
            "Epoch [2/2], Step [57551/65307], Loss: 0.0240\n",
            "Epoch [2/2], Step [57601/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [57601/65307], Loss: 0.0869\n",
            "Epoch [2/2], Step [57651/65307], Loss: 0.0060\n",
            "Epoch [2/2], Step [57651/65307], Loss: 0.0060\n",
            "Epoch [2/2], Step [57701/65307], Loss: 0.3056\n",
            "Epoch [2/2], Step [57701/65307], Loss: 0.3056\n",
            "Epoch [2/2], Step [57751/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [57751/65307], Loss: 0.0136\n",
            "Epoch [2/2], Step [57801/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [57801/65307], Loss: 0.0292\n",
            "Epoch [2/2], Step [57851/65307], Loss: 0.0216\n",
            "Epoch [2/2], Step [57851/65307], Loss: 0.0216\n",
            "Epoch [2/2], Step [57901/65307], Loss: 0.0695\n",
            "Epoch [2/2], Step [57901/65307], Loss: 0.0695\n",
            "Epoch [2/2], Step [57951/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [57951/65307], Loss: 0.0470\n",
            "Epoch [2/2], Step [58001/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [58001/65307], Loss: 0.0724\n",
            "Epoch [2/2], Step [58051/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [58051/65307], Loss: 0.0617\n",
            "Epoch [2/2], Step [58101/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [58101/65307], Loss: 0.0330\n",
            "Epoch [2/2], Step [58151/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [58151/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [58201/65307], Loss: 0.2215\n",
            "Epoch [2/2], Step [58201/65307], Loss: 0.2215\n",
            "Epoch [2/2], Step [58251/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [58251/65307], Loss: 0.0494\n",
            "Epoch [2/2], Step [58301/65307], Loss: 0.2284\n",
            "Epoch [2/2], Step [58301/65307], Loss: 0.2284\n",
            "Epoch [2/2], Step [58351/65307], Loss: 0.0209\n",
            "Epoch [2/2], Step [58351/65307], Loss: 0.0209\n",
            "Epoch [2/2], Step [58401/65307], Loss: 0.0706\n",
            "Epoch [2/2], Step [58401/65307], Loss: 0.0706\n",
            "Epoch [2/2], Step [58451/65307], Loss: 0.1290\n",
            "Epoch [2/2], Step [58451/65307], Loss: 0.1290\n",
            "Epoch [2/2], Step [58501/65307], Loss: 0.3585\n",
            "Epoch [2/2], Step [58501/65307], Loss: 0.3585\n",
            "Epoch [2/2], Step [58551/65307], Loss: 0.0628\n",
            "Epoch [2/2], Step [58551/65307], Loss: 0.0628\n",
            "Epoch [2/2], Step [58601/65307], Loss: 0.1561\n",
            "Epoch [2/2], Step [58601/65307], Loss: 0.1561\n",
            "Epoch [2/2], Step [58651/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [58651/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [58701/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [58701/65307], Loss: 0.0266\n",
            "Epoch [2/2], Step [58751/65307], Loss: 0.1134\n",
            "Epoch [2/2], Step [58751/65307], Loss: 0.1134\n",
            "Epoch [2/2], Step [58801/65307], Loss: 0.1210\n",
            "Epoch [2/2], Step [58801/65307], Loss: 0.1210\n",
            "Epoch [2/2], Step [58851/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [58851/65307], Loss: 0.0101\n",
            "Epoch [2/2], Step [58901/65307], Loss: 0.1753\n",
            "Epoch [2/2], Step [58901/65307], Loss: 0.1753\n",
            "Epoch [2/2], Step [58951/65307], Loss: 0.0400\n",
            "Epoch [2/2], Step [58951/65307], Loss: 0.0400\n",
            "Epoch [2/2], Step [59001/65307], Loss: 0.0225\n",
            "Epoch [2/2], Step [59001/65307], Loss: 0.0225\n",
            "Epoch [2/2], Step [59051/65307], Loss: 0.1408\n",
            "Epoch [2/2], Step [59051/65307], Loss: 0.1408\n",
            "Epoch [2/2], Step [59101/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [59101/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [59151/65307], Loss: 0.0504\n",
            "Epoch [2/2], Step [59151/65307], Loss: 0.0504\n",
            "Epoch [2/2], Step [59201/65307], Loss: 0.0987\n",
            "Epoch [2/2], Step [59201/65307], Loss: 0.0987\n",
            "Epoch [2/2], Step [59251/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [59251/65307], Loss: 0.0053\n",
            "Epoch [2/2], Step [59301/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [59301/65307], Loss: 0.0994\n",
            "Epoch [2/2], Step [59351/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [59351/65307], Loss: 0.1079\n",
            "Epoch [2/2], Step [59401/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [59401/65307], Loss: 0.1025\n",
            "Epoch [2/2], Step [59451/65307], Loss: 0.0731\n",
            "Epoch [2/2], Step [59451/65307], Loss: 0.0731\n",
            "Epoch [2/2], Step [59501/65307], Loss: 0.0122\n",
            "Epoch [2/2], Step [59501/65307], Loss: 0.0122\n",
            "Epoch [2/2], Step [59551/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [59551/65307], Loss: 0.0566\n",
            "Epoch [2/2], Step [59601/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [59601/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [59651/65307], Loss: 0.0465\n",
            "Epoch [2/2], Step [59651/65307], Loss: 0.0465\n",
            "Epoch [2/2], Step [59701/65307], Loss: 0.0965\n",
            "Epoch [2/2], Step [59701/65307], Loss: 0.0965\n",
            "Epoch [2/2], Step [59751/65307], Loss: 0.0115\n",
            "Epoch [2/2], Step [59751/65307], Loss: 0.0115\n",
            "Epoch [2/2], Step [59801/65307], Loss: 0.0265\n",
            "Epoch [2/2], Step [59801/65307], Loss: 0.0265\n",
            "Epoch [2/2], Step [59851/65307], Loss: 0.1367\n",
            "Epoch [2/2], Step [59851/65307], Loss: 0.1367\n",
            "Epoch [2/2], Step [59901/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [59901/65307], Loss: 0.0634\n",
            "Epoch [2/2], Step [59951/65307], Loss: 0.1861\n",
            "Epoch [2/2], Step [59951/65307], Loss: 0.1861\n",
            "Epoch [2/2], Step [60001/65307], Loss: 0.0351\n",
            "Epoch [2/2], Step [60001/65307], Loss: 0.0351\n",
            "Epoch [2/2], Step [60051/65307], Loss: 0.1797\n",
            "Epoch [2/2], Step [60051/65307], Loss: 0.1797\n",
            "Epoch [2/2], Step [60101/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [60101/65307], Loss: 0.0783\n",
            "Epoch [2/2], Step [60151/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [60151/65307], Loss: 0.0371\n",
            "Epoch [2/2], Step [60201/65307], Loss: 0.1154\n",
            "Epoch [2/2], Step [60201/65307], Loss: 0.1154\n",
            "Epoch [2/2], Step [60251/65307], Loss: 0.1418\n",
            "Epoch [2/2], Step [60251/65307], Loss: 0.1418\n",
            "Epoch [2/2], Step [60301/65307], Loss: 0.1571\n",
            "Epoch [2/2], Step [60301/65307], Loss: 0.1571\n",
            "Epoch [2/2], Step [60351/65307], Loss: 0.0179\n",
            "Epoch [2/2], Step [60351/65307], Loss: 0.0179\n",
            "Epoch [2/2], Step [60401/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [60401/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [60451/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [60451/65307], Loss: 0.0941\n",
            "Epoch [2/2], Step [60501/65307], Loss: 0.0988\n",
            "Epoch [2/2], Step [60501/65307], Loss: 0.0988\n",
            "Epoch [2/2], Step [60551/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [60551/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [60601/65307], Loss: 0.1192\n",
            "Epoch [2/2], Step [60601/65307], Loss: 0.1192\n",
            "Epoch [2/2], Step [60651/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [60651/65307], Loss: 0.0066\n",
            "Epoch [2/2], Step [60701/65307], Loss: 0.5129\n",
            "Epoch [2/2], Step [60701/65307], Loss: 0.5129\n",
            "Epoch [2/2], Step [60751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [60751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [60801/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [60801/65307], Loss: 0.0231\n",
            "Epoch [2/2], Step [60851/65307], Loss: 0.0623\n",
            "Epoch [2/2], Step [60851/65307], Loss: 0.0623\n",
            "Epoch [2/2], Step [60901/65307], Loss: 0.0076\n",
            "Epoch [2/2], Step [60901/65307], Loss: 0.0076\n",
            "Epoch [2/2], Step [60951/65307], Loss: 0.0382\n",
            "Epoch [2/2], Step [60951/65307], Loss: 0.0382\n",
            "Epoch [2/2], Step [61001/65307], Loss: 0.1080\n",
            "Epoch [2/2], Step [61001/65307], Loss: 0.1080\n",
            "Epoch [2/2], Step [61051/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [61051/65307], Loss: 0.0573\n",
            "Epoch [2/2], Step [61101/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [61101/65307], Loss: 0.0642\n",
            "Epoch [2/2], Step [61151/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [61151/65307], Loss: 0.0429\n",
            "Epoch [2/2], Step [61201/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [61201/65307], Loss: 0.0428\n",
            "Epoch [2/2], Step [61251/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [61251/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [61301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61301/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61351/65307], Loss: 0.2706\n",
            "Epoch [2/2], Step [61351/65307], Loss: 0.2706\n",
            "Epoch [2/2], Step [61401/65307], Loss: 0.0959\n",
            "Epoch [2/2], Step [61401/65307], Loss: 0.0959\n",
            "Epoch [2/2], Step [61451/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [61451/65307], Loss: 0.0025\n",
            "Epoch [2/2], Step [61501/65307], Loss: 0.1230\n",
            "Epoch [2/2], Step [61501/65307], Loss: 0.1230\n",
            "Epoch [2/2], Step [61551/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [61551/65307], Loss: 0.0086\n",
            "Epoch [2/2], Step [61601/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61601/65307], Loss: 0.0032\n",
            "Epoch [2/2], Step [61651/65307], Loss: 0.1393\n",
            "Epoch [2/2], Step [61651/65307], Loss: 0.1393\n",
            "Epoch [2/2], Step [61701/65307], Loss: 0.0468\n",
            "Epoch [2/2], Step [61701/65307], Loss: 0.0468\n",
            "Epoch [2/2], Step [61751/65307], Loss: 0.1195\n",
            "Epoch [2/2], Step [61751/65307], Loss: 0.1195\n",
            "Epoch [2/2], Step [61801/65307], Loss: 0.0273\n",
            "Epoch [2/2], Step [61801/65307], Loss: 0.0273\n",
            "Epoch [2/2], Step [61851/65307], Loss: 0.1386\n",
            "Epoch [2/2], Step [61851/65307], Loss: 0.1386\n",
            "Epoch [2/2], Step [61901/65307], Loss: 0.1321\n",
            "Epoch [2/2], Step [61901/65307], Loss: 0.1321\n",
            "Epoch [2/2], Step [61951/65307], Loss: 0.1479\n",
            "Epoch [2/2], Step [61951/65307], Loss: 0.1479\n",
            "Epoch [2/2], Step [62001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [62001/65307], Loss: 0.0022\n",
            "Epoch [2/2], Step [62051/65307], Loss: 0.3153\n",
            "Epoch [2/2], Step [62051/65307], Loss: 0.3153\n",
            "Epoch [2/2], Step [62101/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [62101/65307], Loss: 0.1067\n",
            "Epoch [2/2], Step [62151/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [62151/65307], Loss: 0.1885\n",
            "Epoch [2/2], Step [62201/65307], Loss: 0.1534\n",
            "Epoch [2/2], Step [62201/65307], Loss: 0.1534\n",
            "Epoch [2/2], Step [62251/65307], Loss: 0.0307\n",
            "Epoch [2/2], Step [62251/65307], Loss: 0.0307\n",
            "Epoch [2/2], Step [62301/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [62301/65307], Loss: 0.0118\n",
            "Epoch [2/2], Step [62351/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62351/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62401/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [62401/65307], Loss: 0.0296\n",
            "Epoch [2/2], Step [62451/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [62451/65307], Loss: 0.0167\n",
            "Epoch [2/2], Step [62501/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62501/65307], Loss: 0.0358\n",
            "Epoch [2/2], Step [62551/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [62551/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [62601/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [62601/65307], Loss: 0.0043\n",
            "Epoch [2/2], Step [62651/65307], Loss: 0.1157\n",
            "Epoch [2/2], Step [62651/65307], Loss: 0.1157\n",
            "Epoch [2/2], Step [62701/65307], Loss: 0.0600\n",
            "Epoch [2/2], Step [62701/65307], Loss: 0.0600\n",
            "Epoch [2/2], Step [62751/65307], Loss: 0.1700\n",
            "Epoch [2/2], Step [62751/65307], Loss: 0.1700\n",
            "Epoch [2/2], Step [62801/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [62801/65307], Loss: 0.0872\n",
            "Epoch [2/2], Step [62851/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [62851/65307], Loss: 0.0139\n",
            "Epoch [2/2], Step [62901/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [62901/65307], Loss: 0.0068\n",
            "Epoch [2/2], Step [62951/65307], Loss: 0.1566\n",
            "Epoch [2/2], Step [62951/65307], Loss: 0.1566\n",
            "Epoch [2/2], Step [63001/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [63001/65307], Loss: 0.0856\n",
            "Epoch [2/2], Step [63051/65307], Loss: 0.0799\n",
            "Epoch [2/2], Step [63051/65307], Loss: 0.0799\n",
            "Epoch [2/2], Step [63101/65307], Loss: 0.0519\n",
            "Epoch [2/2], Step [63101/65307], Loss: 0.0519\n",
            "Epoch [2/2], Step [63151/65307], Loss: 0.0290\n",
            "Epoch [2/2], Step [63151/65307], Loss: 0.0290\n",
            "Epoch [2/2], Step [63201/65307], Loss: 0.0512\n",
            "Epoch [2/2], Step [63201/65307], Loss: 0.0512\n",
            "Epoch [2/2], Step [63251/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63251/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63301/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [63301/65307], Loss: 0.0212\n",
            "Epoch [2/2], Step [63351/65307], Loss: 0.1181\n",
            "Epoch [2/2], Step [63351/65307], Loss: 0.1181\n",
            "Epoch [2/2], Step [63401/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [63401/65307], Loss: 0.1541\n",
            "Epoch [2/2], Step [63451/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [63451/65307], Loss: 0.0054\n",
            "Epoch [2/2], Step [63501/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [63501/65307], Loss: 0.1407\n",
            "Epoch [2/2], Step [63551/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [63551/65307], Loss: 0.0073\n",
            "Epoch [2/2], Step [63601/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [63601/65307], Loss: 0.0316\n",
            "Epoch [2/2], Step [63651/65307], Loss: 0.0293\n",
            "Epoch [2/2], Step [63651/65307], Loss: 0.0293\n",
            "Epoch [2/2], Step [63701/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [63701/65307], Loss: 0.0383\n",
            "Epoch [2/2], Step [63751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63751/65307], Loss: 0.0675\n",
            "Epoch [2/2], Step [63801/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [63801/65307], Loss: 0.0287\n",
            "Epoch [2/2], Step [63851/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [63851/65307], Loss: 0.0039\n",
            "Epoch [2/2], Step [63901/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [63901/65307], Loss: 0.1035\n",
            "Epoch [2/2], Step [63951/65307], Loss: 0.2522\n",
            "Epoch [2/2], Step [63951/65307], Loss: 0.2522\n",
            "Epoch [2/2], Step [64001/65307], Loss: 0.1856\n",
            "Epoch [2/2], Step [64001/65307], Loss: 0.1856\n",
            "Epoch [2/2], Step [64051/65307], Loss: 0.0246\n",
            "Epoch [2/2], Step [64051/65307], Loss: 0.0246\n",
            "Epoch [2/2], Step [64101/65307], Loss: 0.1073\n",
            "Epoch [2/2], Step [64101/65307], Loss: 0.1073\n",
            "Epoch [2/2], Step [64151/65307], Loss: 0.1460\n",
            "Epoch [2/2], Step [64151/65307], Loss: 0.1460\n",
            "Epoch [2/2], Step [64201/65307], Loss: 0.0204\n",
            "Epoch [2/2], Step [64201/65307], Loss: 0.0204\n",
            "Epoch [2/2], Step [64251/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [64251/65307], Loss: 0.1140\n",
            "Epoch [2/2], Step [64301/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [64301/65307], Loss: 0.0372\n",
            "Epoch [2/2], Step [64351/65307], Loss: 0.0210\n",
            "Epoch [2/2], Step [64351/65307], Loss: 0.0210\n",
            "Epoch [2/2], Step [64401/65307], Loss: 0.1063\n",
            "Epoch [2/2], Step [64401/65307], Loss: 0.1063\n",
            "Epoch [2/2], Step [64451/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [64451/65307], Loss: 0.0083\n",
            "Epoch [2/2], Step [64501/65307], Loss: 0.1228\n",
            "Epoch [2/2], Step [64501/65307], Loss: 0.1228\n",
            "Epoch [2/2], Step [64551/65307], Loss: 0.0887\n",
            "Epoch [2/2], Step [64551/65307], Loss: 0.0887\n",
            "Epoch [2/2], Step [64601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [64601/65307], Loss: 0.0333\n",
            "Epoch [2/2], Step [64651/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [64651/65307], Loss: 0.0021\n",
            "Epoch [2/2], Step [64701/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [64701/65307], Loss: 0.1487\n",
            "Epoch [2/2], Step [64751/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [64751/65307], Loss: 0.0173\n",
            "Epoch [2/2], Step [64801/65307], Loss: 0.1936\n",
            "Epoch [2/2], Step [64801/65307], Loss: 0.1936\n",
            "Epoch [2/2], Step [64851/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [64851/65307], Loss: 0.0063\n",
            "Epoch [2/2], Step [64901/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [64901/65307], Loss: 0.0081\n",
            "Epoch [2/2], Step [64951/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [64951/65307], Loss: 0.0227\n",
            "Epoch [2/2], Step [65001/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [65001/65307], Loss: 0.0035\n",
            "Epoch [2/2], Step [65051/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [65051/65307], Loss: 0.0170\n",
            "Epoch [2/2], Step [65101/65307], Loss: 0.0920\n",
            "Epoch [2/2], Step [65101/65307], Loss: 0.0920\n",
            "Epoch [2/2], Step [65151/65307], Loss: 0.0680\n",
            "Epoch [2/2], Step [65151/65307], Loss: 0.0680\n",
            "Epoch [2/2], Step [65201/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [65201/65307], Loss: 0.0232\n",
            "Epoch [2/2], Step [65251/65307], Loss: 0.0244\n",
            "Epoch [2/2], Step [65251/65307], Loss: 0.0244\n",
            "Epoch [2/2], Step [65301/65307], Loss: 0.1451\n",
            "Epoch [2/2], Step [65301/65307], Loss: 0.1451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpgWKiwkHPaK"
      },
      "source": [
        "The above output has some duplicates because there is a socketIO error from my browser during runtime, but there is nothing wrong with the model that runs on the kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-yUQn5R5yO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3259d8c8-8e57-43e5-8dab-81832a4a303e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss_set)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('every 50 iterations')\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-o-9Is3AhLt"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7naz9vzBYI0"
      },
      "source": [
        "del train_dataloader"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8SPUxkPBsOv"
      },
      "source": [
        "del train_data"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyH9oexYByyj"
      },
      "source": [
        "del train_inputs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpYTSySzCN3T"
      },
      "source": [
        "del train_masks"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDu7IkYDDDB"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztqfa6mE_f9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca8ebf8-f651-442a-e3f8-c4f74a4df5d5"
      },
      "source": [
        "total_labels=[]\n",
        "total_predictions=[]\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      # print (outputs)\n",
        "      print(i)\n",
        "      prediction = torch.argmax(outputs[0],dim=1)\n",
        "      total_labels.append(b_labels)\n",
        "      total_predictions.append(prediction)\n",
        "      total += b_labels.size(0)\n",
        "      correct+=(prediction==b_labels).sum().item()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "11327\n",
            "11328\n",
            "11329\n",
            "11330\n",
            "11331\n",
            "11332\n",
            "11333\n",
            "11334\n",
            "11335\n",
            "11336\n",
            "11337\n",
            "11338\n",
            "11339\n",
            "11340\n",
            "11341\n",
            "11342\n",
            "11343\n",
            "11344\n",
            "11345\n",
            "11346\n",
            "11347\n",
            "11348\n",
            "11349\n",
            "11350\n",
            "11351\n",
            "11352\n",
            "11353\n",
            "11354\n",
            "11355\n",
            "11356\n",
            "11357\n",
            "11358\n",
            "11359\n",
            "11360\n",
            "11361\n",
            "11362\n",
            "11363\n",
            "11364\n",
            "11365\n",
            "11366\n",
            "11367\n",
            "11368\n",
            "11369\n",
            "11370\n",
            "11371\n",
            "11372\n",
            "11373\n",
            "11374\n",
            "11375\n",
            "11376\n",
            "11377\n",
            "11378\n",
            "11379\n",
            "11380\n",
            "11381\n",
            "11382\n",
            "11383\n",
            "11384\n",
            "11385\n",
            "11386\n",
            "11387\n",
            "11388\n",
            "11389\n",
            "11390\n",
            "11391\n",
            "11392\n",
            "11393\n",
            "11394\n",
            "11395\n",
            "11396\n",
            "11397\n",
            "11398\n",
            "11399\n",
            "11400\n",
            "11401\n",
            "11402\n",
            "11403\n",
            "11404\n",
            "11405\n",
            "11406\n",
            "11407\n",
            "11408\n",
            "11409\n",
            "11410\n",
            "11411\n",
            "11412\n",
            "11413\n",
            "11414\n",
            "11415\n",
            "11416\n",
            "11417\n",
            "11418\n",
            "11419\n",
            "11420\n",
            "11421\n",
            "11422\n",
            "11423\n",
            "11424\n",
            "11425\n",
            "11426\n",
            "11427\n",
            "11428\n",
            "11429\n",
            "11430\n",
            "11431\n",
            "11432\n",
            "11433\n",
            "11434\n",
            "11435\n",
            "11436\n",
            "11437\n",
            "11438\n",
            "11439\n",
            "11440\n",
            "11441\n",
            "11442\n",
            "11443\n",
            "11444\n",
            "11445\n",
            "11446\n",
            "11447\n",
            "11448\n",
            "11449\n",
            "11450\n",
            "11451\n",
            "11452\n",
            "11453\n",
            "11454\n",
            "11455\n",
            "11456\n",
            "11457\n",
            "11458\n",
            "11459\n",
            "11460\n",
            "11461\n",
            "11462\n",
            "11463\n",
            "11464\n",
            "11465\n",
            "11466\n",
            "11467\n",
            "11468\n",
            "11469\n",
            "11470\n",
            "11471\n",
            "11472\n",
            "11473\n",
            "11474\n",
            "11475\n",
            "11476\n",
            "11477\n",
            "11478\n",
            "11479\n",
            "11480\n",
            "11481\n",
            "11482\n",
            "11483\n",
            "11484\n",
            "11485\n",
            "11486\n",
            "11487\n",
            "11488\n",
            "11489\n",
            "11490\n",
            "11491\n",
            "11492\n",
            "11493\n",
            "11494\n",
            "11495\n",
            "11496\n",
            "11497\n",
            "11498\n",
            "11499\n",
            "11500\n",
            "11501\n",
            "11502\n",
            "11503\n",
            "11504\n",
            "11505\n",
            "11506\n",
            "11507\n",
            "11508\n",
            "11509\n",
            "11510\n",
            "11511\n",
            "11512\n",
            "11513\n",
            "11514\n",
            "11515\n",
            "11516\n",
            "11517\n",
            "11518\n",
            "11519\n",
            "11520\n",
            "11521\n",
            "11522\n",
            "11523\n",
            "11524\n",
            "11525\n",
            "11526\n",
            "11527\n",
            "11528\n",
            "11529\n",
            "11530\n",
            "11531\n",
            "11532\n",
            "11533\n",
            "11534\n",
            "11535\n",
            "11536\n",
            "11537\n",
            "11538\n",
            "11539\n",
            "11540\n",
            "11541\n",
            "11542\n",
            "11543\n",
            "11544\n",
            "11545\n",
            "11546\n",
            "11547\n",
            "11548\n",
            "11549\n",
            "11550\n",
            "11551\n",
            "11552\n",
            "11553\n",
            "11554\n",
            "11555\n",
            "11556\n",
            "11557\n",
            "11558\n",
            "11559\n",
            "11560\n",
            "11561\n",
            "11562\n",
            "11563\n",
            "11564\n",
            "11565\n",
            "11566\n",
            "11567\n",
            "11568\n",
            "11569\n",
            "11570\n",
            "11571\n",
            "11572\n",
            "11573\n",
            "11574\n",
            "11575\n",
            "11576\n",
            "11577\n",
            "11578\n",
            "11579\n",
            "11580\n",
            "11581\n",
            "11582\n",
            "11583\n",
            "11584\n",
            "11585\n",
            "11586\n",
            "11587\n",
            "11588\n",
            "11589\n",
            "11590\n",
            "11591\n",
            "11592\n",
            "11593\n",
            "11594\n",
            "11595\n",
            "11596\n",
            "11597\n",
            "11598\n",
            "11599\n",
            "11600\n",
            "11601\n",
            "11602\n",
            "11603\n",
            "11604\n",
            "11605\n",
            "11606\n",
            "11607\n",
            "11608\n",
            "11609\n",
            "11610\n",
            "11611\n",
            "11612\n",
            "11613\n",
            "11614\n",
            "11615\n",
            "11616\n",
            "11617\n",
            "11618\n",
            "11619\n",
            "11620\n",
            "11621\n",
            "11622\n",
            "11623\n",
            "11624\n",
            "11625\n",
            "11626\n",
            "11627\n",
            "11628\n",
            "11629\n",
            "11630\n",
            "11631\n",
            "11632\n",
            "11633\n",
            "11634\n",
            "11635\n",
            "11636\n",
            "11637\n",
            "11638\n",
            "11639\n",
            "11640\n",
            "11641\n",
            "11642\n",
            "11643\n",
            "11644\n",
            "11645\n",
            "11646\n",
            "11647\n",
            "11648\n",
            "11649\n",
            "11650\n",
            "11651\n",
            "11652\n",
            "11653\n",
            "11654\n",
            "11655\n",
            "11656\n",
            "11657\n",
            "11658\n",
            "11659\n",
            "11660\n",
            "11661\n",
            "11662\n",
            "11663\n",
            "11664\n",
            "11665\n",
            "11666\n",
            "11667\n",
            "11668\n",
            "11669\n",
            "11670\n",
            "11671\n",
            "11672\n",
            "11673\n",
            "11674\n",
            "11675\n",
            "11676\n",
            "11677\n",
            "11678\n",
            "11679\n",
            "11680\n",
            "11681\n",
            "11682\n",
            "11683\n",
            "11684\n",
            "11685\n",
            "11686\n",
            "11687\n",
            "11688\n",
            "11689\n",
            "11690\n",
            "11691\n",
            "11692\n",
            "11693\n",
            "11694\n",
            "11695\n",
            "11696\n",
            "11697\n",
            "11698\n",
            "11699\n",
            "11700\n",
            "11701\n",
            "11702\n",
            "11703\n",
            "11704\n",
            "11705\n",
            "11706\n",
            "11707\n",
            "11708\n",
            "11709\n",
            "11710\n",
            "11711\n",
            "11712\n",
            "11713\n",
            "11714\n",
            "11715\n",
            "11716\n",
            "11717\n",
            "11718\n",
            "11719\n",
            "11720\n",
            "11721\n",
            "11722\n",
            "11723\n",
            "11724\n",
            "11725\n",
            "11726\n",
            "11727\n",
            "11728\n",
            "11729\n",
            "11730\n",
            "11731\n",
            "11732\n",
            "11733\n",
            "11734\n",
            "11735\n",
            "11736\n",
            "11737\n",
            "11738\n",
            "11739\n",
            "11740\n",
            "11741\n",
            "11742\n",
            "11743\n",
            "11744\n",
            "11745\n",
            "11746\n",
            "11747\n",
            "11748\n",
            "11749\n",
            "11750\n",
            "11751\n",
            "11752\n",
            "11753\n",
            "11754\n",
            "11755\n",
            "11756\n",
            "11757\n",
            "11758\n",
            "11759\n",
            "11760\n",
            "11761\n",
            "11762\n",
            "11763\n",
            "11764\n",
            "11765\n",
            "11766\n",
            "11767\n",
            "11768\n",
            "11769\n",
            "11770\n",
            "11771\n",
            "11772\n",
            "11773\n",
            "11774\n",
            "11775\n",
            "11776\n",
            "11777\n",
            "11778\n",
            "11779\n",
            "11780\n",
            "11781\n",
            "11782\n",
            "11783\n",
            "11784\n",
            "11785\n",
            "11786\n",
            "11787\n",
            "11788\n",
            "11789\n",
            "11790\n",
            "11791\n",
            "11792\n",
            "11793\n",
            "11794\n",
            "11795\n",
            "11796\n",
            "11797\n",
            "11798\n",
            "11799\n",
            "11800\n",
            "11801\n",
            "11802\n",
            "11803\n",
            "11804\n",
            "11805\n",
            "11806\n",
            "11807\n",
            "11808\n",
            "11809\n",
            "11810\n",
            "11811\n",
            "11812\n",
            "11813\n",
            "11814\n",
            "11815\n",
            "11816\n",
            "11817\n",
            "11818\n",
            "11819\n",
            "11820\n",
            "11821\n",
            "11822\n",
            "11823\n",
            "11824\n",
            "11825\n",
            "11826\n",
            "11827\n",
            "11828\n",
            "11829\n",
            "11830\n",
            "11831\n",
            "11832\n",
            "11833\n",
            "11834\n",
            "11835\n",
            "11836\n",
            "11837\n",
            "11838\n",
            "11839\n",
            "11840\n",
            "11841\n",
            "11842\n",
            "11843\n",
            "11844\n",
            "11845\n",
            "11846\n",
            "11847\n",
            "11848\n",
            "11849\n",
            "11850\n",
            "11851\n",
            "11852\n",
            "11853\n",
            "11854\n",
            "11855\n",
            "11856\n",
            "11857\n",
            "11858\n",
            "11859\n",
            "11860\n",
            "11861\n",
            "11862\n",
            "11863\n",
            "11864\n",
            "11865\n",
            "11866\n",
            "11867\n",
            "11868\n",
            "11869\n",
            "11870\n",
            "11871\n",
            "11872\n",
            "11873\n",
            "11874\n",
            "11875\n",
            "11876\n",
            "11877\n",
            "11878\n",
            "11879\n",
            "11880\n",
            "11881\n",
            "11882\n",
            "11883\n",
            "11884\n",
            "11885\n",
            "11886\n",
            "11887\n",
            "11888\n",
            "11889\n",
            "11890\n",
            "11891\n",
            "11892\n",
            "11893\n",
            "11894\n",
            "11895\n",
            "11896\n",
            "11897\n",
            "11898\n",
            "11899\n",
            "11900\n",
            "11901\n",
            "11902\n",
            "11903\n",
            "11904\n",
            "11905\n",
            "11906\n",
            "11907\n",
            "11908\n",
            "11909\n",
            "11910\n",
            "11911\n",
            "11912\n",
            "11913\n",
            "11914\n",
            "11915\n",
            "11916\n",
            "11917\n",
            "11918\n",
            "11919\n",
            "11920\n",
            "11921\n",
            "11922\n",
            "11923\n",
            "11924\n",
            "11925\n",
            "11926\n",
            "11927\n",
            "11928\n",
            "11929\n",
            "11930\n",
            "11931\n",
            "11932\n",
            "11933\n",
            "11934\n",
            "11935\n",
            "11936\n",
            "11937\n",
            "11938\n",
            "11939\n",
            "11940\n",
            "11941\n",
            "11942\n",
            "11943\n",
            "11944\n",
            "11945\n",
            "11946\n",
            "11947\n",
            "11948\n",
            "11949\n",
            "11950\n",
            "11951\n",
            "11952\n",
            "11953\n",
            "11954\n",
            "11955\n",
            "11956\n",
            "11957\n",
            "11958\n",
            "11959\n",
            "11960\n",
            "11961\n",
            "11962\n",
            "11963\n",
            "11964\n",
            "11965\n",
            "11966\n",
            "11967\n",
            "11968\n",
            "11969\n",
            "11970\n",
            "11971\n",
            "11972\n",
            "11973\n",
            "11974\n",
            "11975\n",
            "11976\n",
            "11977\n",
            "11978\n",
            "11979\n",
            "11980\n",
            "11981\n",
            "11982\n",
            "11983\n",
            "11984\n",
            "11985\n",
            "11986\n",
            "11987\n",
            "11988\n",
            "11989\n",
            "11990\n",
            "11991\n",
            "11992\n",
            "11993\n",
            "11994\n",
            "11995\n",
            "11996\n",
            "11997\n",
            "11998\n",
            "11999\n",
            "12000\n",
            "12001\n",
            "12002\n",
            "12003\n",
            "12004\n",
            "12005\n",
            "12006\n",
            "12007\n",
            "12008\n",
            "12009\n",
            "12010\n",
            "12011\n",
            "12012\n",
            "12013\n",
            "12014\n",
            "12015\n",
            "12016\n",
            "12017\n",
            "12018\n",
            "12019\n",
            "12020\n",
            "12021\n",
            "12022\n",
            "12023\n",
            "12024\n",
            "12025\n",
            "12026\n",
            "12027\n",
            "12028\n",
            "12029\n",
            "12030\n",
            "12031\n",
            "12032\n",
            "12033\n",
            "12034\n",
            "12035\n",
            "12036\n",
            "12037\n",
            "12038\n",
            "12039\n",
            "12040\n",
            "12041\n",
            "12042\n",
            "12043\n",
            "12044\n",
            "12045\n",
            "12046\n",
            "12047\n",
            "12048\n",
            "12049\n",
            "12050\n",
            "12051\n",
            "12052\n",
            "12053\n",
            "12054\n",
            "12055\n",
            "12056\n",
            "12057\n",
            "12058\n",
            "12059\n",
            "12060\n",
            "12061\n",
            "12062\n",
            "12063\n",
            "12064\n",
            "12065\n",
            "12066\n",
            "12067\n",
            "12068\n",
            "12069\n",
            "12070\n",
            "12071\n",
            "12072\n",
            "12073\n",
            "12074\n",
            "12075\n",
            "12076\n",
            "12077\n",
            "12078\n",
            "12079\n",
            "12080\n",
            "12081\n",
            "12082\n",
            "12083\n",
            "12084\n",
            "12085\n",
            "12086\n",
            "12087\n",
            "12088\n",
            "12089\n",
            "12090\n",
            "12091\n",
            "12092\n",
            "12093\n",
            "12094\n",
            "12095\n",
            "12096\n",
            "12097\n",
            "12098\n",
            "12099\n",
            "12100\n",
            "12101\n",
            "12102\n",
            "12103\n",
            "12104\n",
            "12105\n",
            "12106\n",
            "12107\n",
            "12108\n",
            "12109\n",
            "12110\n",
            "12111\n",
            "12112\n",
            "12113\n",
            "12114\n",
            "12115\n",
            "12116\n",
            "12117\n",
            "12118\n",
            "12119\n",
            "12120\n",
            "12121\n",
            "12122\n",
            "12123\n",
            "12124\n",
            "12125\n",
            "12126\n",
            "12127\n",
            "12128\n",
            "12129\n",
            "12130\n",
            "12131\n",
            "12132\n",
            "12133\n",
            "12134\n",
            "12135\n",
            "12136\n",
            "12137\n",
            "12138\n",
            "12139\n",
            "12140\n",
            "12141\n",
            "12142\n",
            "12143\n",
            "12144\n",
            "12145\n",
            "12146\n",
            "12147\n",
            "12148\n",
            "12149\n",
            "12150\n",
            "12151\n",
            "12152\n",
            "12153\n",
            "12154\n",
            "12155\n",
            "12156\n",
            "12157\n",
            "12158\n",
            "12159\n",
            "12160\n",
            "12161\n",
            "12162\n",
            "12163\n",
            "12164\n",
            "12165\n",
            "12166\n",
            "12167\n",
            "12168\n",
            "12169\n",
            "12170\n",
            "12171\n",
            "12172\n",
            "12173\n",
            "12174\n",
            "12175\n",
            "12176\n",
            "12177\n",
            "12178\n",
            "12179\n",
            "12180\n",
            "12181\n",
            "12182\n",
            "12183\n",
            "12184\n",
            "12185\n",
            "12186\n",
            "12187\n",
            "12188\n",
            "12189\n",
            "12190\n",
            "12191\n",
            "12192\n",
            "12193\n",
            "12194\n",
            "12195\n",
            "12196\n",
            "12197\n",
            "12198\n",
            "12199\n",
            "12200\n",
            "12201\n",
            "12202\n",
            "12203\n",
            "12204\n",
            "12205\n",
            "12206\n",
            "12207\n",
            "12208\n",
            "12209\n",
            "12210\n",
            "12211\n",
            "12212\n",
            "12213\n",
            "12214\n",
            "12215\n",
            "12216\n",
            "12217\n",
            "12218\n",
            "12219\n",
            "12220\n",
            "12221\n",
            "12222\n",
            "12223\n",
            "12224\n",
            "12225\n",
            "12226\n",
            "12227\n",
            "12228\n",
            "12229\n",
            "12230\n",
            "12231\n",
            "12232\n",
            "12233\n",
            "12234\n",
            "12235\n",
            "12236\n",
            "12237\n",
            "12238\n",
            "12239\n",
            "12240\n",
            "12241\n",
            "12242\n",
            "12243\n",
            "12244\n",
            "12245\n",
            "12246\n",
            "12247\n",
            "12248\n",
            "12249\n",
            "12250\n",
            "12251\n",
            "12252\n",
            "12253\n",
            "12254\n",
            "12255\n",
            "12256\n",
            "12257\n",
            "12258\n",
            "12259\n",
            "12260\n",
            "12261\n",
            "12262\n",
            "12263\n",
            "12264\n",
            "12265\n",
            "12266\n",
            "12267\n",
            "12268\n",
            "12269\n",
            "12270\n",
            "12271\n",
            "12272\n",
            "12273\n",
            "12274\n",
            "12275\n",
            "12276\n",
            "12277\n",
            "12278\n",
            "12279\n",
            "12280\n",
            "12281\n",
            "12282\n",
            "12283\n",
            "12284\n",
            "12285\n",
            "12286\n",
            "12287\n",
            "12288\n",
            "12289\n",
            "12290\n",
            "12291\n",
            "12292\n",
            "12293\n",
            "12294\n",
            "12295\n",
            "12296\n",
            "12297\n",
            "12298\n",
            "12299\n",
            "12300\n",
            "12301\n",
            "12302\n",
            "12303\n",
            "12304\n",
            "12305\n",
            "12306\n",
            "12307\n",
            "12308\n",
            "12309\n",
            "12310\n",
            "12311\n",
            "12312\n",
            "12313\n",
            "12314\n",
            "12315\n",
            "12316\n",
            "12317\n",
            "12318\n",
            "12319\n",
            "12320\n",
            "12321\n",
            "12322\n",
            "12323\n",
            "12324\n",
            "12325\n",
            "12326\n",
            "12327\n",
            "12328\n",
            "12329\n",
            "12330\n",
            "12331\n",
            "12332\n",
            "12333\n",
            "12334\n",
            "12335\n",
            "12336\n",
            "12337\n",
            "12338\n",
            "12339\n",
            "12340\n",
            "12341\n",
            "12342\n",
            "12343\n",
            "12344\n",
            "12345\n",
            "12346\n",
            "12347\n",
            "12348\n",
            "12349\n",
            "12350\n",
            "12351\n",
            "12352\n",
            "12353\n",
            "12354\n",
            "12355\n",
            "12356\n",
            "12357\n",
            "12358\n",
            "12359\n",
            "12360\n",
            "12361\n",
            "12362\n",
            "12363\n",
            "12364\n",
            "12365\n",
            "12366\n",
            "12367\n",
            "12368\n",
            "12369\n",
            "12370\n",
            "12371\n",
            "12372\n",
            "12373\n",
            "12374\n",
            "12375\n",
            "12376\n",
            "12377\n",
            "12378\n",
            "12379\n",
            "12380\n",
            "12381\n",
            "12382\n",
            "12383\n",
            "12384\n",
            "12385\n",
            "12386\n",
            "12387\n",
            "12388\n",
            "12389\n",
            "12390\n",
            "12391\n",
            "12392\n",
            "12393\n",
            "12394\n",
            "12395\n",
            "12396\n",
            "12397\n",
            "12398\n",
            "12399\n",
            "12400\n",
            "12401\n",
            "12402\n",
            "12403\n",
            "12404\n",
            "12405\n",
            "12406\n",
            "12407\n",
            "12408\n",
            "12409\n",
            "12410\n",
            "12411\n",
            "12412\n",
            "12413\n",
            "12414\n",
            "12415\n",
            "12416\n",
            "12417\n",
            "12418\n",
            "12419\n",
            "12420\n",
            "12421\n",
            "12422\n",
            "12423\n",
            "12424\n",
            "12425\n",
            "12426\n",
            "12427\n",
            "12428\n",
            "12429\n",
            "12430\n",
            "12431\n",
            "12432\n",
            "12433\n",
            "12434\n",
            "12435\n",
            "12436\n",
            "12437\n",
            "12438\n",
            "12439\n",
            "12440\n",
            "12441\n",
            "12442\n",
            "12443\n",
            "12444\n",
            "12445\n",
            "12446\n",
            "12447\n",
            "12448\n",
            "12449\n",
            "12450\n",
            "12451\n",
            "12452\n",
            "12453\n",
            "12454\n",
            "12455\n",
            "12456\n",
            "12457\n",
            "12458\n",
            "12459\n",
            "12460\n",
            "12461\n",
            "12462\n",
            "12463\n",
            "12464\n",
            "12465\n",
            "12466\n",
            "12467\n",
            "12468\n",
            "12469\n",
            "12470\n",
            "12471\n",
            "12472\n",
            "12473\n",
            "12474\n",
            "12475\n",
            "12476\n",
            "12477\n",
            "12478\n",
            "12479\n",
            "12480\n",
            "12481\n",
            "12482\n",
            "12483\n",
            "12484\n",
            "12485\n",
            "12486\n",
            "12487\n",
            "12488\n",
            "12489\n",
            "12490\n",
            "12491\n",
            "12492\n",
            "12493\n",
            "12494\n",
            "12495\n",
            "12496\n",
            "12497\n",
            "12498\n",
            "12499\n",
            "12500\n",
            "12501\n",
            "12502\n",
            "12503\n",
            "12504\n",
            "12505\n",
            "12506\n",
            "12507\n",
            "12508\n",
            "12509\n",
            "12510\n",
            "12511\n",
            "12512\n",
            "12513\n",
            "12514\n",
            "12515\n",
            "12516\n",
            "12517\n",
            "12518\n",
            "12519\n",
            "12520\n",
            "12521\n",
            "12522\n",
            "12523\n",
            "12524\n",
            "12525\n",
            "12526\n",
            "12527\n",
            "12528\n",
            "12529\n",
            "12530\n",
            "12531\n",
            "12532\n",
            "12533\n",
            "12534\n",
            "12535\n",
            "12536\n",
            "12537\n",
            "12538\n",
            "12539\n",
            "12540\n",
            "12541\n",
            "12542\n",
            "12543\n",
            "12544\n",
            "12545\n",
            "12546\n",
            "12547\n",
            "12548\n",
            "12549\n",
            "12550\n",
            "12551\n",
            "12552\n",
            "12553\n",
            "12554\n",
            "12555\n",
            "12556\n",
            "12557\n",
            "12558\n",
            "12559\n",
            "12560\n",
            "12561\n",
            "12562\n",
            "12563\n",
            "12564\n",
            "12565\n",
            "12566\n",
            "12567\n",
            "12568\n",
            "12569\n",
            "12570\n",
            "12571\n",
            "12572\n",
            "12573\n",
            "12574\n",
            "12575\n",
            "12576\n",
            "12577\n",
            "12578\n",
            "12579\n",
            "12580\n",
            "12581\n",
            "12582\n",
            "12583\n",
            "12584\n",
            "12585\n",
            "12586\n",
            "12587\n",
            "12588\n",
            "12589\n",
            "12590\n",
            "12591\n",
            "12592\n",
            "12593\n",
            "12594\n",
            "12595\n",
            "12596\n",
            "12597\n",
            "12598\n",
            "12599\n",
            "12600\n",
            "12601\n",
            "12602\n",
            "12603\n",
            "12604\n",
            "12605\n",
            "12606\n",
            "12607\n",
            "12608\n",
            "12609\n",
            "12610\n",
            "12611\n",
            "12612\n",
            "12613\n",
            "12614\n",
            "12615\n",
            "12616\n",
            "12617\n",
            "12618\n",
            "12619\n",
            "12620\n",
            "12621\n",
            "12622\n",
            "12623\n",
            "12624\n",
            "12625\n",
            "12626\n",
            "12627\n",
            "12628\n",
            "12629\n",
            "12630\n",
            "12631\n",
            "12632\n",
            "12633\n",
            "12634\n",
            "12635\n",
            "12636\n",
            "12637\n",
            "12638\n",
            "12639\n",
            "12640\n",
            "12641\n",
            "12642\n",
            "12643\n",
            "12644\n",
            "12645\n",
            "12646\n",
            "12647\n",
            "12648\n",
            "12649\n",
            "12650\n",
            "12651\n",
            "12652\n",
            "12653\n",
            "12654\n",
            "12655\n",
            "12656\n",
            "12657\n",
            "12658\n",
            "12659\n",
            "12660\n",
            "12661\n",
            "12662\n",
            "12663\n",
            "12664\n",
            "12665\n",
            "12666\n",
            "12667\n",
            "12668\n",
            "12669\n",
            "12670\n",
            "12671\n",
            "12672\n",
            "12673\n",
            "12674\n",
            "12675\n",
            "12676\n",
            "12677\n",
            "12678\n",
            "12679\n",
            "12680\n",
            "12681\n",
            "12682\n",
            "12683\n",
            "12684\n",
            "12685\n",
            "12686\n",
            "12687\n",
            "12688\n",
            "12689\n",
            "12690\n",
            "12691\n",
            "12692\n",
            "12693\n",
            "12694\n",
            "12695\n",
            "12696\n",
            "12697\n",
            "12698\n",
            "12699\n",
            "12700\n",
            "12701\n",
            "12702\n",
            "12703\n",
            "12704\n",
            "12705\n",
            "12706\n",
            "12707\n",
            "12708\n",
            "12709\n",
            "12710\n",
            "12711\n",
            "12712\n",
            "12713\n",
            "12714\n",
            "12715\n",
            "12716\n",
            "12717\n",
            "12718\n",
            "12719\n",
            "12720\n",
            "12721\n",
            "12722\n",
            "12723\n",
            "12724\n",
            "12725\n",
            "12726\n",
            "12727\n",
            "12728\n",
            "12729\n",
            "12730\n",
            "12731\n",
            "12732\n",
            "12733\n",
            "12734\n",
            "12735\n",
            "12736\n",
            "12737\n",
            "12738\n",
            "12739\n",
            "12740\n",
            "12741\n",
            "12742\n",
            "12743\n",
            "12744\n",
            "12745\n",
            "12746\n",
            "12747\n",
            "12748\n",
            "12749\n",
            "12750\n",
            "12751\n",
            "12752\n",
            "12753\n",
            "12754\n",
            "12755\n",
            "12756\n",
            "12757\n",
            "12758\n",
            "12759\n",
            "12760\n",
            "12761\n",
            "12762\n",
            "12763\n",
            "12764\n",
            "12765\n",
            "12766\n",
            "12767\n",
            "12768\n",
            "12769\n",
            "12770\n",
            "12771\n",
            "12772\n",
            "12773\n",
            "12774\n",
            "12775\n",
            "12776\n",
            "12777\n",
            "12778\n",
            "12779\n",
            "12780\n",
            "12781\n",
            "12782\n",
            "12783\n",
            "12784\n",
            "12785\n",
            "12786\n",
            "12787\n",
            "12788\n",
            "12789\n",
            "12790\n",
            "12791\n",
            "12792\n",
            "12793\n",
            "12794\n",
            "12795\n",
            "12796\n",
            "12797\n",
            "12798\n",
            "12799\n",
            "12800\n",
            "12801\n",
            "12802\n",
            "12803\n",
            "12804\n",
            "12805\n",
            "12806\n",
            "12807\n",
            "12808\n",
            "12809\n",
            "12810\n",
            "12811\n",
            "12812\n",
            "12813\n",
            "12814\n",
            "12815\n",
            "12816\n",
            "12817\n",
            "12818\n",
            "12819\n",
            "12820\n",
            "12821\n",
            "12822\n",
            "12823\n",
            "12824\n",
            "12825\n",
            "12826\n",
            "12827\n",
            "12828\n",
            "12829\n",
            "12830\n",
            "12831\n",
            "12832\n",
            "12833\n",
            "12834\n",
            "12835\n",
            "12836\n",
            "12837\n",
            "12838\n",
            "12839\n",
            "12840\n",
            "12841\n",
            "12842\n",
            "12843\n",
            "12844\n",
            "12845\n",
            "12846\n",
            "12847\n",
            "12848\n",
            "12849\n",
            "12850\n",
            "12851\n",
            "12852\n",
            "12853\n",
            "12854\n",
            "12855\n",
            "12856\n",
            "12857\n",
            "12858\n",
            "12859\n",
            "12860\n",
            "12861\n",
            "12862\n",
            "12863\n",
            "12864\n",
            "12865\n",
            "12866\n",
            "12867\n",
            "12868\n",
            "12869\n",
            "12870\n",
            "12871\n",
            "12872\n",
            "12873\n",
            "12874\n",
            "12875\n",
            "12876\n",
            "12877\n",
            "12878\n",
            "12879\n",
            "12880\n",
            "12881\n",
            "12882\n",
            "12883\n",
            "12884\n",
            "12885\n",
            "12886\n",
            "12887\n",
            "12888\n",
            "12889\n",
            "12890\n",
            "12891\n",
            "12892\n",
            "12893\n",
            "12894\n",
            "12895\n",
            "12896\n",
            "12897\n",
            "12898\n",
            "12899\n",
            "12900\n",
            "12901\n",
            "12902\n",
            "12903\n",
            "12904\n",
            "12905\n",
            "12906\n",
            "12907\n",
            "12908\n",
            "12909\n",
            "12910\n",
            "12911\n",
            "12912\n",
            "12913\n",
            "12914\n",
            "12915\n",
            "12916\n",
            "12917\n",
            "12918\n",
            "12919\n",
            "12920\n",
            "12921\n",
            "12922\n",
            "12923\n",
            "12924\n",
            "12925\n",
            "12926\n",
            "12927\n",
            "12928\n",
            "12929\n",
            "12930\n",
            "12931\n",
            "12932\n",
            "12933\n",
            "12934\n",
            "12935\n",
            "12936\n",
            "12937\n",
            "12938\n",
            "12939\n",
            "12940\n",
            "12941\n",
            "12942\n",
            "12943\n",
            "12944\n",
            "12945\n",
            "12946\n",
            "12947\n",
            "12948\n",
            "12949\n",
            "12950\n",
            "12951\n",
            "12952\n",
            "12953\n",
            "12954\n",
            "12955\n",
            "12956\n",
            "12957\n",
            "12958\n",
            "12959\n",
            "12960\n",
            "12961\n",
            "12962\n",
            "12963\n",
            "12964\n",
            "12965\n",
            "12966\n",
            "12967\n",
            "12968\n",
            "12969\n",
            "12970\n",
            "12971\n",
            "12972\n",
            "12973\n",
            "12974\n",
            "12975\n",
            "12976\n",
            "12977\n",
            "12978\n",
            "12979\n",
            "12980\n",
            "12981\n",
            "12982\n",
            "12983\n",
            "12984\n",
            "12985\n",
            "12986\n",
            "12987\n",
            "12988\n",
            "12989\n",
            "12990\n",
            "12991\n",
            "12992\n",
            "12993\n",
            "12994\n",
            "12995\n",
            "12996\n",
            "12997\n",
            "12998\n",
            "12999\n",
            "13000\n",
            "13001\n",
            "13002\n",
            "13003\n",
            "13004\n",
            "13005\n",
            "13006\n",
            "13007\n",
            "13008\n",
            "13009\n",
            "13010\n",
            "13011\n",
            "13012\n",
            "13013\n",
            "13014\n",
            "13015\n",
            "13016\n",
            "13017\n",
            "13018\n",
            "13019\n",
            "13020\n",
            "13021\n",
            "13022\n",
            "13023\n",
            "13024\n",
            "13025\n",
            "13026\n",
            "13027\n",
            "13028\n",
            "13029\n",
            "13030\n",
            "13031\n",
            "13032\n",
            "13033\n",
            "13034\n",
            "13035\n",
            "13036\n",
            "13037\n",
            "13038\n",
            "13039\n",
            "13040\n",
            "13041\n",
            "13042\n",
            "13043\n",
            "13044\n",
            "13045\n",
            "13046\n",
            "13047\n",
            "13048\n",
            "13049\n",
            "13050\n",
            "13051\n",
            "13052\n",
            "13053\n",
            "13054\n",
            "13055\n",
            "13056\n",
            "13057\n",
            "13058\n",
            "13059\n",
            "13060\n",
            "13061\n",
            "13062\n",
            "13063\n",
            "13064\n",
            "13065\n",
            "13066\n",
            "13067\n",
            "13068\n",
            "13069\n",
            "13070\n",
            "13071\n",
            "13072\n",
            "13073\n",
            "13074\n",
            "13075\n",
            "13076\n",
            "13077\n",
            "13078\n",
            "13079\n",
            "13080\n",
            "13081\n",
            "13082\n",
            "13083\n",
            "13084\n",
            "13085\n",
            "13086\n",
            "13087\n",
            "13088\n",
            "13089\n",
            "13090\n",
            "13091\n",
            "13092\n",
            "13093\n",
            "13094\n",
            "13095\n",
            "13096\n",
            "13097\n",
            "13098\n",
            "13099\n",
            "13100\n",
            "13101\n",
            "13102\n",
            "13103\n",
            "13104\n",
            "13105\n",
            "13106\n",
            "13107\n",
            "13108\n",
            "13109\n",
            "13110\n",
            "13111\n",
            "13112\n",
            "13113\n",
            "13114\n",
            "13115\n",
            "13116\n",
            "13117\n",
            "13118\n",
            "13119\n",
            "13120\n",
            "13121\n",
            "13122\n",
            "13123\n",
            "13124\n",
            "13125\n",
            "13126\n",
            "13127\n",
            "13128\n",
            "13129\n",
            "13130\n",
            "13131\n",
            "13132\n",
            "13133\n",
            "13134\n",
            "13135\n",
            "13136\n",
            "13137\n",
            "13138\n",
            "13139\n",
            "13140\n",
            "13141\n",
            "13142\n",
            "13143\n",
            "13144\n",
            "13145\n",
            "13146\n",
            "13147\n",
            "13148\n",
            "13149\n",
            "13150\n",
            "13151\n",
            "13152\n",
            "13153\n",
            "13154\n",
            "13155\n",
            "13156\n",
            "13157\n",
            "13158\n",
            "13159\n",
            "13160\n",
            "13161\n",
            "13162\n",
            "13163\n",
            "13164\n",
            "13165\n",
            "13166\n",
            "13167\n",
            "13168\n",
            "13169\n",
            "13170\n",
            "13171\n",
            "13172\n",
            "13173\n",
            "13174\n",
            "13175\n",
            "13176\n",
            "13177\n",
            "13178\n",
            "13179\n",
            "13180\n",
            "13181\n",
            "13182\n",
            "13183\n",
            "13184\n",
            "13185\n",
            "13186\n",
            "13187\n",
            "13188\n",
            "13189\n",
            "13190\n",
            "13191\n",
            "13192\n",
            "13193\n",
            "13194\n",
            "13195\n",
            "13196\n",
            "13197\n",
            "13198\n",
            "13199\n",
            "13200\n",
            "13201\n",
            "13202\n",
            "13203\n",
            "13204\n",
            "13205\n",
            "13206\n",
            "13207\n",
            "13208\n",
            "13209\n",
            "13210\n",
            "13211\n",
            "13212\n",
            "13213\n",
            "13214\n",
            "13215\n",
            "13216\n",
            "13217\n",
            "13218\n",
            "13219\n",
            "13220\n",
            "13221\n",
            "13222\n",
            "13223\n",
            "13224\n",
            "13225\n",
            "13226\n",
            "13227\n",
            "13228\n",
            "13229\n",
            "13230\n",
            "13231\n",
            "13232\n",
            "13233\n",
            "13234\n",
            "13235\n",
            "13236\n",
            "13237\n",
            "13238\n",
            "13239\n",
            "13240\n",
            "13241\n",
            "13242\n",
            "13243\n",
            "13244\n",
            "13245\n",
            "13246\n",
            "13247\n",
            "13248\n",
            "13249\n",
            "13250\n",
            "13251\n",
            "13252\n",
            "13253\n",
            "13254\n",
            "13255\n",
            "13256\n",
            "13257\n",
            "13258\n",
            "13259\n",
            "13260\n",
            "13261\n",
            "13262\n",
            "13263\n",
            "13264\n",
            "13265\n",
            "13266\n",
            "13267\n",
            "13268\n",
            "13269\n",
            "13270\n",
            "13271\n",
            "13272\n",
            "13273\n",
            "13274\n",
            "13275\n",
            "13276\n",
            "13277\n",
            "13278\n",
            "13279\n",
            "13280\n",
            "13281\n",
            "13282\n",
            "13283\n",
            "13284\n",
            "13285\n",
            "13286\n",
            "13287\n",
            "13288\n",
            "13289\n",
            "13290\n",
            "13291\n",
            "13292\n",
            "13293\n",
            "13294\n",
            "13295\n",
            "13296\n",
            "13297\n",
            "13298\n",
            "13299\n",
            "13300\n",
            "13301\n",
            "13302\n",
            "13303\n",
            "13304\n",
            "13305\n",
            "13306\n",
            "13307\n",
            "13308\n",
            "13309\n",
            "13310\n",
            "13311\n",
            "13312\n",
            "13313\n",
            "13314\n",
            "13315\n",
            "13316\n",
            "13317\n",
            "13318\n",
            "13319\n",
            "13320\n",
            "13321\n",
            "13322\n",
            "13323\n",
            "13324\n",
            "13325\n",
            "13326\n",
            "13327\n",
            "13328\n",
            "13329\n",
            "13330\n",
            "13331\n",
            "13332\n",
            "13333\n",
            "13334\n",
            "13335\n",
            "13336\n",
            "13337\n",
            "13338\n",
            "13339\n",
            "13340\n",
            "13341\n",
            "13342\n",
            "13343\n",
            "13344\n",
            "13345\n",
            "13346\n",
            "13347\n",
            "13348\n",
            "13349\n",
            "13350\n",
            "13351\n",
            "13352\n",
            "13353\n",
            "13354\n",
            "13355\n",
            "13356\n",
            "13357\n",
            "13358\n",
            "13359\n",
            "13360\n",
            "13361\n",
            "13362\n",
            "13363\n",
            "13364\n",
            "13365\n",
            "13366\n",
            "13367\n",
            "13368\n",
            "13369\n",
            "13370\n",
            "13371\n",
            "13372\n",
            "13373\n",
            "13374\n",
            "13375\n",
            "13376\n",
            "13377\n",
            "13378\n",
            "13379\n",
            "13380\n",
            "13381\n",
            "13382\n",
            "13383\n",
            "13384\n",
            "13385\n",
            "13386\n",
            "13387\n",
            "13388\n",
            "13389\n",
            "13390\n",
            "13391\n",
            "13392\n",
            "13393\n",
            "13394\n",
            "13395\n",
            "13396\n",
            "13397\n",
            "13398\n",
            "13399\n",
            "13400\n",
            "13401\n",
            "13402\n",
            "13403\n",
            "13404\n",
            "13405\n",
            "13406\n",
            "13407\n",
            "13408\n",
            "13409\n",
            "13410\n",
            "13411\n",
            "13412\n",
            "13413\n",
            "13414\n",
            "13415\n",
            "13416\n",
            "13417\n",
            "13418\n",
            "13419\n",
            "13420\n",
            "13421\n",
            "13422\n",
            "13423\n",
            "13424\n",
            "13425\n",
            "13426\n",
            "13427\n",
            "13428\n",
            "13429\n",
            "13430\n",
            "13431\n",
            "13432\n",
            "13433\n",
            "13434\n",
            "13435\n",
            "13436\n",
            "13437\n",
            "13438\n",
            "13439\n",
            "13440\n",
            "13441\n",
            "13442\n",
            "13443\n",
            "13444\n",
            "13445\n",
            "13446\n",
            "13447\n",
            "13448\n",
            "13449\n",
            "13450\n",
            "13451\n",
            "13452\n",
            "13453\n",
            "13454\n",
            "13455\n",
            "13456\n",
            "13457\n",
            "13458\n",
            "13459\n",
            "13460\n",
            "13461\n",
            "13462\n",
            "13463\n",
            "13464\n",
            "13465\n",
            "13466\n",
            "13467\n",
            "13468\n",
            "13469\n",
            "13470\n",
            "13471\n",
            "13472\n",
            "13473\n",
            "13474\n",
            "13475\n",
            "13476\n",
            "13477\n",
            "13478\n",
            "13479\n",
            "13480\n",
            "13481\n",
            "13482\n",
            "13483\n",
            "13484\n",
            "13485\n",
            "13486\n",
            "13487\n",
            "13488\n",
            "13489\n",
            "13490\n",
            "13491\n",
            "13492\n",
            "13493\n",
            "13494\n",
            "13495\n",
            "13496\n",
            "13497\n",
            "13498\n",
            "13499\n",
            "13500\n",
            "13501\n",
            "13502\n",
            "13503\n",
            "13504\n",
            "13505\n",
            "13506\n",
            "13507\n",
            "13508\n",
            "13509\n",
            "13510\n",
            "13511\n",
            "13512\n",
            "13513\n",
            "13514\n",
            "13515\n",
            "13516\n",
            "13517\n",
            "13518\n",
            "13519\n",
            "13520\n",
            "13521\n",
            "13522\n",
            "13523\n",
            "13524\n",
            "13525\n",
            "13526\n",
            "13527\n",
            "13528\n",
            "13529\n",
            "13530\n",
            "13531\n",
            "13532\n",
            "13533\n",
            "13534\n",
            "13535\n",
            "13536\n",
            "13537\n",
            "13538\n",
            "13539\n",
            "13540\n",
            "13541\n",
            "13542\n",
            "13543\n",
            "13544\n",
            "13545\n",
            "13546\n",
            "13547\n",
            "13548\n",
            "13549\n",
            "13550\n",
            "13551\n",
            "13552\n",
            "13553\n",
            "13554\n",
            "13555\n",
            "13556\n",
            "13557\n",
            "13558\n",
            "13559\n",
            "13560\n",
            "13561\n",
            "13562\n",
            "13563\n",
            "13564\n",
            "13565\n",
            "13566\n",
            "13567\n",
            "13568\n",
            "13569\n",
            "13570\n",
            "13571\n",
            "13572\n",
            "13573\n",
            "13574\n",
            "13575\n",
            "13576\n",
            "13577\n",
            "13578\n",
            "13579\n",
            "13580\n",
            "13581\n",
            "13582\n",
            "13583\n",
            "13584\n",
            "13585\n",
            "13586\n",
            "13587\n",
            "13588\n",
            "13589\n",
            "13590\n",
            "13591\n",
            "13592\n",
            "13593\n",
            "13594\n",
            "13595\n",
            "13596\n",
            "13597\n",
            "13598\n",
            "13599\n",
            "13600\n",
            "13601\n",
            "13602\n",
            "13603\n",
            "13604\n",
            "13605\n",
            "13606\n",
            "13607\n",
            "13608\n",
            "13609\n",
            "13610\n",
            "13611\n",
            "13612\n",
            "13613\n",
            "13614\n",
            "13615\n",
            "13616\n",
            "13617\n",
            "13618\n",
            "13619\n",
            "13620\n",
            "13621\n",
            "13622\n",
            "13623\n",
            "13624\n",
            "13625\n",
            "13626\n",
            "13627\n",
            "13628\n",
            "13629\n",
            "13630\n",
            "13631\n",
            "13632\n",
            "13633\n",
            "13634\n",
            "13635\n",
            "13636\n",
            "13637\n",
            "13638\n",
            "13639\n",
            "13640\n",
            "13641\n",
            "13642\n",
            "13643\n",
            "13644\n",
            "13645\n",
            "13646\n",
            "13647\n",
            "13648\n",
            "13649\n",
            "13650\n",
            "13651\n",
            "13652\n",
            "13653\n",
            "13654\n",
            "13655\n",
            "13656\n",
            "13657\n",
            "13658\n",
            "13659\n",
            "13660\n",
            "13661\n",
            "13662\n",
            "13663\n",
            "13664\n",
            "13665\n",
            "13666\n",
            "13667\n",
            "13668\n",
            "13669\n",
            "13670\n",
            "13671\n",
            "13672\n",
            "13673\n",
            "13674\n",
            "13675\n",
            "13676\n",
            "13677\n",
            "13678\n",
            "13679\n",
            "13680\n",
            "13681\n",
            "13682\n",
            "13683\n",
            "13684\n",
            "13685\n",
            "13686\n",
            "13687\n",
            "13688\n",
            "13689\n",
            "13690\n",
            "13691\n",
            "13692\n",
            "13693\n",
            "13694\n",
            "13695\n",
            "13696\n",
            "13697\n",
            "13698\n",
            "13699\n",
            "13700\n",
            "13701\n",
            "13702\n",
            "13703\n",
            "13704\n",
            "13705\n",
            "13706\n",
            "13707\n",
            "13708\n",
            "13709\n",
            "13710\n",
            "13711\n",
            "13712\n",
            "13713\n",
            "13714\n",
            "13715\n",
            "13716\n",
            "13717\n",
            "13718\n",
            "13719\n",
            "13720\n",
            "13721\n",
            "13722\n",
            "13723\n",
            "13724\n",
            "13725\n",
            "13726\n",
            "13727\n",
            "13728\n",
            "13729\n",
            "13730\n",
            "13731\n",
            "13732\n",
            "13733\n",
            "13734\n",
            "13735\n",
            "13736\n",
            "13737\n",
            "13738\n",
            "13739\n",
            "13740\n",
            "13741\n",
            "13742\n",
            "13743\n",
            "13744\n",
            "13745\n",
            "13746\n",
            "13747\n",
            "13748\n",
            "13749\n",
            "13750\n",
            "13751\n",
            "13752\n",
            "13753\n",
            "13754\n",
            "13755\n",
            "13756\n",
            "13757\n",
            "13758\n",
            "13759\n",
            "13760\n",
            "13761\n",
            "13762\n",
            "13763\n",
            "13764\n",
            "13765\n",
            "13766\n",
            "13767\n",
            "13768\n",
            "13769\n",
            "13770\n",
            "13771\n",
            "13772\n",
            "13773\n",
            "13774\n",
            "13775\n",
            "13776\n",
            "13777\n",
            "13778\n",
            "13779\n",
            "13780\n",
            "13781\n",
            "13782\n",
            "13783\n",
            "13784\n",
            "13785\n",
            "13786\n",
            "13787\n",
            "13788\n",
            "13789\n",
            "13790\n",
            "13791\n",
            "13792\n",
            "13793\n",
            "13794\n",
            "13795\n",
            "13796\n",
            "13797\n",
            "13798\n",
            "13799\n",
            "13800\n",
            "13801\n",
            "13802\n",
            "13803\n",
            "13804\n",
            "13805\n",
            "13806\n",
            "13807\n",
            "13808\n",
            "13809\n",
            "13810\n",
            "13811\n",
            "13812\n",
            "13813\n",
            "13814\n",
            "13815\n",
            "13816\n",
            "13817\n",
            "13818\n",
            "13819\n",
            "13820\n",
            "13821\n",
            "13822\n",
            "13823\n",
            "13824\n",
            "13825\n",
            "13826\n",
            "13827\n",
            "13828\n",
            "13829\n",
            "13830\n",
            "13831\n",
            "13832\n",
            "13833\n",
            "13834\n",
            "13835\n",
            "13836\n",
            "13837\n",
            "13838\n",
            "13839\n",
            "13840\n",
            "13841\n",
            "13842\n",
            "13843\n",
            "13844\n",
            "13845\n",
            "13846\n",
            "13847\n",
            "13848\n",
            "13849\n",
            "13850\n",
            "13851\n",
            "13852\n",
            "13853\n",
            "13854\n",
            "13855\n",
            "13856\n",
            "13857\n",
            "13858\n",
            "13859\n",
            "13860\n",
            "13861\n",
            "13862\n",
            "13863\n",
            "13864\n",
            "13865\n",
            "13866\n",
            "13867\n",
            "13868\n",
            "13869\n",
            "13870\n",
            "13871\n",
            "13872\n",
            "13873\n",
            "13874\n",
            "13875\n",
            "13876\n",
            "13877\n",
            "13878\n",
            "13879\n",
            "13880\n",
            "13881\n",
            "13882\n",
            "13883\n",
            "13884\n",
            "13885\n",
            "13886\n",
            "13887\n",
            "13888\n",
            "13889\n",
            "13890\n",
            "13891\n",
            "13892\n",
            "13893\n",
            "13894\n",
            "13895\n",
            "13896\n",
            "13897\n",
            "13898\n",
            "13899\n",
            "13900\n",
            "13901\n",
            "13902\n",
            "13903\n",
            "13904\n",
            "13905\n",
            "13906\n",
            "13907\n",
            "13908\n",
            "13909\n",
            "13910\n",
            "13911\n",
            "13912\n",
            "13913\n",
            "13914\n",
            "13915\n",
            "13916\n",
            "13917\n",
            "13918\n",
            "13919\n",
            "13920\n",
            "13921\n",
            "13922\n",
            "13923\n",
            "13924\n",
            "13925\n",
            "13926\n",
            "13927\n",
            "13928\n",
            "13929\n",
            "13930\n",
            "13931\n",
            "13932\n",
            "13933\n",
            "13934\n",
            "13935\n",
            "13936\n",
            "13937\n",
            "13938\n",
            "13939\n",
            "13940\n",
            "13941\n",
            "13942\n",
            "13943\n",
            "13944\n",
            "13945\n",
            "13946\n",
            "13947\n",
            "13948\n",
            "13949\n",
            "13950\n",
            "13951\n",
            "13952\n",
            "13953\n",
            "13954\n",
            "13955\n",
            "13956\n",
            "13957\n",
            "13958\n",
            "13959\n",
            "13960\n",
            "13961\n",
            "13962\n",
            "13963\n",
            "13964\n",
            "13965\n",
            "13966\n",
            "13967\n",
            "13968\n",
            "13969\n",
            "13970\n",
            "13971\n",
            "13972\n",
            "13973\n",
            "13974\n",
            "13975\n",
            "13976\n",
            "13977\n",
            "13978\n",
            "13979\n",
            "13980\n",
            "13981\n",
            "13982\n",
            "13983\n",
            "13984\n",
            "13985\n",
            "13986\n",
            "13987\n",
            "13988\n",
            "13989\n",
            "13990\n",
            "13991\n",
            "13992\n",
            "13993\n",
            "13994\n",
            "13995\n",
            "13996\n",
            "13997\n",
            "13998\n",
            "13999\n",
            "14000\n",
            "14001\n",
            "14002\n",
            "14003\n",
            "14004\n",
            "14005\n",
            "14006\n",
            "14007\n",
            "14008\n",
            "14009\n",
            "14010\n",
            "14011\n",
            "14012\n",
            "14013\n",
            "14014\n",
            "14015\n",
            "14016\n",
            "14017\n",
            "14018\n",
            "14019\n",
            "14020\n",
            "14021\n",
            "14022\n",
            "14023\n",
            "14024\n",
            "14025\n",
            "14026\n",
            "14027\n",
            "14028\n",
            "14029\n",
            "14030\n",
            "14031\n",
            "14032\n",
            "14033\n",
            "14034\n",
            "14035\n",
            "14036\n",
            "14037\n",
            "14038\n",
            "14039\n",
            "14040\n",
            "14041\n",
            "14042\n",
            "14043\n",
            "14044\n",
            "14045\n",
            "14046\n",
            "14047\n",
            "14048\n",
            "14049\n",
            "14050\n",
            "14051\n",
            "14052\n",
            "14053\n",
            "14054\n",
            "14055\n",
            "14056\n",
            "14057\n",
            "14058\n",
            "14059\n",
            "14060\n",
            "14061\n",
            "14062\n",
            "14063\n",
            "14064\n",
            "14065\n",
            "14066\n",
            "14067\n",
            "14068\n",
            "14069\n",
            "14070\n",
            "14071\n",
            "14072\n",
            "14073\n",
            "14074\n",
            "14075\n",
            "14076\n",
            "14077\n",
            "14078\n",
            "14079\n",
            "14080\n",
            "14081\n",
            "14082\n",
            "14083\n",
            "14084\n",
            "14085\n",
            "14086\n",
            "14087\n",
            "14088\n",
            "14089\n",
            "14090\n",
            "14091\n",
            "14092\n",
            "14093\n",
            "14094\n",
            "14095\n",
            "14096\n",
            "14097\n",
            "14098\n",
            "14099\n",
            "14100\n",
            "14101\n",
            "14102\n",
            "14103\n",
            "14104\n",
            "14105\n",
            "14106\n",
            "14107\n",
            "14108\n",
            "14109\n",
            "14110\n",
            "14111\n",
            "14112\n",
            "14113\n",
            "14114\n",
            "14115\n",
            "14116\n",
            "14117\n",
            "14118\n",
            "14119\n",
            "14120\n",
            "14121\n",
            "14122\n",
            "14123\n",
            "14124\n",
            "14125\n",
            "14126\n",
            "14127\n",
            "14128\n",
            "14129\n",
            "14130\n",
            "14131\n",
            "14132\n",
            "14133\n",
            "14134\n",
            "14135\n",
            "14136\n",
            "14137\n",
            "14138\n",
            "14139\n",
            "14140\n",
            "14141\n",
            "14142\n",
            "14143\n",
            "14144\n",
            "14145\n",
            "14146\n",
            "14147\n",
            "14148\n",
            "14149\n",
            "14150\n",
            "14151\n",
            "14152\n",
            "14153\n",
            "14154\n",
            "14155\n",
            "14156\n",
            "14157\n",
            "14158\n",
            "14159\n",
            "14160\n",
            "14161\n",
            "14162\n",
            "14163\n",
            "14164\n",
            "14165\n",
            "14166\n",
            "14167\n",
            "14168\n",
            "14169\n",
            "14170\n",
            "14171\n",
            "14172\n",
            "14173\n",
            "14174\n",
            "14175\n",
            "14176\n",
            "14177\n",
            "14178\n",
            "14179\n",
            "14180\n",
            "14181\n",
            "14182\n",
            "14183\n",
            "14184\n",
            "14185\n",
            "14186\n",
            "14187\n",
            "14188\n",
            "14189\n",
            "14190\n",
            "14191\n",
            "14192\n",
            "14193\n",
            "14194\n",
            "14195\n",
            "14196\n",
            "14197\n",
            "14198\n",
            "14199\n",
            "14200\n",
            "14201\n",
            "14202\n",
            "14203\n",
            "14204\n",
            "14205\n",
            "14206\n",
            "14207\n",
            "14208\n",
            "14209\n",
            "14210\n",
            "14211\n",
            "14212\n",
            "14213\n",
            "14214\n",
            "14215\n",
            "14216\n",
            "14217\n",
            "14218\n",
            "14219\n",
            "14220\n",
            "14221\n",
            "14222\n",
            "14223\n",
            "14224\n",
            "14225\n",
            "14226\n",
            "14227\n",
            "14228\n",
            "14229\n",
            "14230\n",
            "14231\n",
            "14232\n",
            "14233\n",
            "14234\n",
            "14235\n",
            "14236\n",
            "14237\n",
            "14238\n",
            "14239\n",
            "14240\n",
            "14241\n",
            "14242\n",
            "14243\n",
            "14244\n",
            "14245\n",
            "14246\n",
            "14247\n",
            "14248\n",
            "14249\n",
            "14250\n",
            "14251\n",
            "14252\n",
            "14253\n",
            "14254\n",
            "14255\n",
            "14256\n",
            "14257\n",
            "14258\n",
            "14259\n",
            "14260\n",
            "14261\n",
            "14262\n",
            "14263\n",
            "14264\n",
            "14265\n",
            "14266\n",
            "14267\n",
            "14268\n",
            "14269\n",
            "14270\n",
            "14271\n",
            "14272\n",
            "14273\n",
            "14274\n",
            "14275\n",
            "14276\n",
            "14277\n",
            "14278\n",
            "14279\n",
            "14280\n",
            "14281\n",
            "14282\n",
            "14283\n",
            "14284\n",
            "14285\n",
            "14286\n",
            "14287\n",
            "14288\n",
            "14289\n",
            "14290\n",
            "14291\n",
            "14292\n",
            "14293\n",
            "14294\n",
            "14295\n",
            "14296\n",
            "14297\n",
            "14298\n",
            "14299\n",
            "14300\n",
            "14301\n",
            "14302\n",
            "14303\n",
            "14304\n",
            "14305\n",
            "14306\n",
            "14307\n",
            "14308\n",
            "14309\n",
            "14310\n",
            "14311\n",
            "14312\n",
            "14313\n",
            "14314\n",
            "14315\n",
            "14316\n",
            "14317\n",
            "14318\n",
            "14319\n",
            "14320\n",
            "14321\n",
            "14322\n",
            "14323\n",
            "14324\n",
            "14325\n",
            "14326\n",
            "14327\n",
            "14328\n",
            "14329\n",
            "14330\n",
            "14331\n",
            "14332\n",
            "14333\n",
            "14334\n",
            "14335\n",
            "14336\n",
            "14337\n",
            "14338\n",
            "14339\n",
            "14340\n",
            "14341\n",
            "14342\n",
            "14343\n",
            "14344\n",
            "14345\n",
            "14346\n",
            "14347\n",
            "14348\n",
            "14349\n",
            "14350\n",
            "14351\n",
            "14352\n",
            "14353\n",
            "14354\n",
            "14355\n",
            "14356\n",
            "14357\n",
            "14358\n",
            "14359\n",
            "14360\n",
            "14361\n",
            "14362\n",
            "14363\n",
            "14364\n",
            "14365\n",
            "14366\n",
            "14367\n",
            "14368\n",
            "14369\n",
            "14370\n",
            "14371\n",
            "14372\n",
            "14373\n",
            "14374\n",
            "14375\n",
            "14376\n",
            "14377\n",
            "14378\n",
            "14379\n",
            "14380\n",
            "14381\n",
            "14382\n",
            "14383\n",
            "14384\n",
            "14385\n",
            "14386\n",
            "14387\n",
            "14388\n",
            "14389\n",
            "14390\n",
            "14391\n",
            "14392\n",
            "14393\n",
            "14394\n",
            "14395\n",
            "14396\n",
            "14397\n",
            "14398\n",
            "14399\n",
            "14400\n",
            "14401\n",
            "14402\n",
            "14403\n",
            "14404\n",
            "14405\n",
            "14406\n",
            "14407\n",
            "14408\n",
            "14409\n",
            "14410\n",
            "14411\n",
            "14412\n",
            "14413\n",
            "14414\n",
            "14415\n",
            "14416\n",
            "14417\n",
            "14418\n",
            "14419\n",
            "14420\n",
            "14421\n",
            "14422\n",
            "14423\n",
            "14424\n",
            "14425\n",
            "14426\n",
            "14427\n",
            "14428\n",
            "14429\n",
            "14430\n",
            "14431\n",
            "14432\n",
            "14433\n",
            "14434\n",
            "14435\n",
            "14436\n",
            "14437\n",
            "14438\n",
            "14439\n",
            "14440\n",
            "14441\n",
            "14442\n",
            "14443\n",
            "14444\n",
            "14445\n",
            "14446\n",
            "14447\n",
            "14448\n",
            "14449\n",
            "14450\n",
            "14451\n",
            "14452\n",
            "14453\n",
            "14454\n",
            "14455\n",
            "14456\n",
            "14457\n",
            "14458\n",
            "14459\n",
            "14460\n",
            "14461\n",
            "14462\n",
            "14463\n",
            "14464\n",
            "14465\n",
            "14466\n",
            "14467\n",
            "14468\n",
            "14469\n",
            "14470\n",
            "14471\n",
            "14472\n",
            "14473\n",
            "14474\n",
            "14475\n",
            "14476\n",
            "14477\n",
            "14478\n",
            "14479\n",
            "14480\n",
            "14481\n",
            "14482\n",
            "14483\n",
            "14484\n",
            "14485\n",
            "14486\n",
            "14487\n",
            "14488\n",
            "14489\n",
            "14490\n",
            "14491\n",
            "14492\n",
            "14493\n",
            "14494\n",
            "14495\n",
            "14496\n",
            "14497\n",
            "14498\n",
            "14499\n",
            "14500\n",
            "14501\n",
            "14502\n",
            "14503\n",
            "14504\n",
            "14505\n",
            "14506\n",
            "14507\n",
            "14508\n",
            "14509\n",
            "14510\n",
            "14511\n",
            "14512\n",
            "14513\n",
            "14514\n",
            "14515\n",
            "14516\n",
            "14517\n",
            "14518\n",
            "14519\n",
            "14520\n",
            "14521\n",
            "14522\n",
            "14523\n",
            "14524\n",
            "14525\n",
            "14526\n",
            "14527\n",
            "14528\n",
            "14529\n",
            "14530\n",
            "14531\n",
            "14532\n",
            "14533\n",
            "14534\n",
            "14535\n",
            "14536\n",
            "14537\n",
            "14538\n",
            "14539\n",
            "14540\n",
            "14541\n",
            "14542\n",
            "14543\n",
            "14544\n",
            "14545\n",
            "14546\n",
            "14547\n",
            "14548\n",
            "14549\n",
            "14550\n",
            "14551\n",
            "14552\n",
            "14553\n",
            "14554\n",
            "14555\n",
            "14556\n",
            "14557\n",
            "14558\n",
            "14559\n",
            "14560\n",
            "14561\n",
            "14562\n",
            "14563\n",
            "14564\n",
            "14565\n",
            "14566\n",
            "14567\n",
            "14568\n",
            "14569\n",
            "14570\n",
            "14571\n",
            "14572\n",
            "14573\n",
            "14574\n",
            "14575\n",
            "14576\n",
            "14577\n",
            "14578\n",
            "14579\n",
            "14580\n",
            "14581\n",
            "14582\n",
            "14583\n",
            "14584\n",
            "14585\n",
            "14586\n",
            "14587\n",
            "14588\n",
            "14589\n",
            "14590\n",
            "14591\n",
            "14592\n",
            "14593\n",
            "14594\n",
            "14595\n",
            "14596\n",
            "14597\n",
            "14598\n",
            "14599\n",
            "14600\n",
            "14601\n",
            "14602\n",
            "14603\n",
            "14604\n",
            "14605\n",
            "14606\n",
            "14607\n",
            "14608\n",
            "14609\n",
            "14610\n",
            "14611\n",
            "14612\n",
            "14613\n",
            "14614\n",
            "14615\n",
            "14616\n",
            "14617\n",
            "14618\n",
            "14619\n",
            "14620\n",
            "14621\n",
            "14622\n",
            "14623\n",
            "14624\n",
            "14625\n",
            "14626\n",
            "14627\n",
            "14628\n",
            "14629\n",
            "14630\n",
            "14631\n",
            "14632\n",
            "14633\n",
            "14634\n",
            "14635\n",
            "14636\n",
            "14637\n",
            "14638\n",
            "14639\n",
            "14640\n",
            "14641\n",
            "14642\n",
            "14643\n",
            "14644\n",
            "14645\n",
            "14646\n",
            "14647\n",
            "14648\n",
            "14649\n",
            "14650\n",
            "14651\n",
            "14652\n",
            "14653\n",
            "14654\n",
            "14655\n",
            "14656\n",
            "14657\n",
            "14658\n",
            "14659\n",
            "14660\n",
            "14661\n",
            "14662\n",
            "14663\n",
            "14664\n",
            "14665\n",
            "14666\n",
            "14667\n",
            "14668\n",
            "14669\n",
            "14670\n",
            "14671\n",
            "14672\n",
            "14673\n",
            "14674\n",
            "14675\n",
            "14676\n",
            "14677\n",
            "14678\n",
            "14679\n",
            "14680\n",
            "14681\n",
            "14682\n",
            "14683\n",
            "14684\n",
            "14685\n",
            "14686\n",
            "14687\n",
            "14688\n",
            "14689\n",
            "14690\n",
            "14691\n",
            "14692\n",
            "14693\n",
            "14694\n",
            "14695\n",
            "14696\n",
            "14697\n",
            "14698\n",
            "14699\n",
            "14700\n",
            "14701\n",
            "14702\n",
            "14703\n",
            "14704\n",
            "14705\n",
            "14706\n",
            "14707\n",
            "14708\n",
            "14709\n",
            "14710\n",
            "14711\n",
            "14712\n",
            "14713\n",
            "14714\n",
            "14715\n",
            "14716\n",
            "14717\n",
            "14718\n",
            "14719\n",
            "14720\n",
            "14721\n",
            "14722\n",
            "14723\n",
            "14724\n",
            "14725\n",
            "14726\n",
            "14727\n",
            "14728\n",
            "14729\n",
            "14730\n",
            "14731\n",
            "14732\n",
            "14733\n",
            "14734\n",
            "14735\n",
            "14736\n",
            "14737\n",
            "14738\n",
            "14739\n",
            "14740\n",
            "14741\n",
            "14742\n",
            "14743\n",
            "14744\n",
            "14745\n",
            "14746\n",
            "14747\n",
            "14748\n",
            "14749\n",
            "14750\n",
            "14751\n",
            "14752\n",
            "14753\n",
            "14754\n",
            "14755\n",
            "14756\n",
            "14757\n",
            "14758\n",
            "14759\n",
            "14760\n",
            "14761\n",
            "14762\n",
            "14763\n",
            "14764\n",
            "14765\n",
            "14766\n",
            "14767\n",
            "14768\n",
            "14769\n",
            "14770\n",
            "14771\n",
            "14772\n",
            "14773\n",
            "14774\n",
            "14775\n",
            "14776\n",
            "14777\n",
            "14778\n",
            "14779\n",
            "14780\n",
            "14781\n",
            "14782\n",
            "14783\n",
            "14784\n",
            "14785\n",
            "14786\n",
            "14787\n",
            "14788\n",
            "14789\n",
            "14790\n",
            "14791\n",
            "14792\n",
            "14793\n",
            "14794\n",
            "14795\n",
            "14796\n",
            "14797\n",
            "14798\n",
            "14799\n",
            "14800\n",
            "14801\n",
            "14802\n",
            "14803\n",
            "14804\n",
            "14805\n",
            "14806\n",
            "14807\n",
            "14808\n",
            "14809\n",
            "14810\n",
            "14811\n",
            "14812\n",
            "14813\n",
            "14814\n",
            "14815\n",
            "14816\n",
            "14817\n",
            "14818\n",
            "14819\n",
            "14820\n",
            "14821\n",
            "14822\n",
            "14823\n",
            "14824\n",
            "14825\n",
            "14826\n",
            "14827\n",
            "14828\n",
            "14829\n",
            "14830\n",
            "14831\n",
            "14832\n",
            "14833\n",
            "14834\n",
            "14835\n",
            "14836\n",
            "14837\n",
            "14838\n",
            "14839\n",
            "14840\n",
            "14841\n",
            "14842\n",
            "14843\n",
            "14844\n",
            "14845\n",
            "14846\n",
            "14847\n",
            "14848\n",
            "14849\n",
            "14850\n",
            "14851\n",
            "14852\n",
            "14853\n",
            "14854\n",
            "14855\n",
            "14856\n",
            "14857\n",
            "14858\n",
            "14859\n",
            "14860\n",
            "14861\n",
            "14862\n",
            "14863\n",
            "14864\n",
            "14865\n",
            "14866\n",
            "14867\n",
            "14868\n",
            "14869\n",
            "14870\n",
            "14871\n",
            "14872\n",
            "14873\n",
            "14874\n",
            "14875\n",
            "14876\n",
            "14877\n",
            "14878\n",
            "14879\n",
            "14880\n",
            "14881\n",
            "14882\n",
            "14883\n",
            "14884\n",
            "14885\n",
            "14886\n",
            "14887\n",
            "14888\n",
            "14889\n",
            "14890\n",
            "14891\n",
            "14892\n",
            "14893\n",
            "14894\n",
            "14895\n",
            "14896\n",
            "14897\n",
            "14898\n",
            "14899\n",
            "14900\n",
            "14901\n",
            "14902\n",
            "14903\n",
            "14904\n",
            "14905\n",
            "14906\n",
            "14907\n",
            "14908\n",
            "14909\n",
            "14910\n",
            "14911\n",
            "14912\n",
            "14913\n",
            "14914\n",
            "14915\n",
            "14916\n",
            "14917\n",
            "14918\n",
            "14919\n",
            "14920\n",
            "14921\n",
            "14922\n",
            "14923\n",
            "14924\n",
            "14925\n",
            "14926\n",
            "14927\n",
            "14928\n",
            "14929\n",
            "14930\n",
            "14931\n",
            "14932\n",
            "14933\n",
            "14934\n",
            "14935\n",
            "14936\n",
            "14937\n",
            "14938\n",
            "14939\n",
            "14940\n",
            "14941\n",
            "14942\n",
            "14943\n",
            "14944\n",
            "14945\n",
            "14946\n",
            "14947\n",
            "14948\n",
            "14949\n",
            "14950\n",
            "14951\n",
            "14952\n",
            "14953\n",
            "14954\n",
            "14955\n",
            "14956\n",
            "14957\n",
            "14958\n",
            "14959\n",
            "14960\n",
            "14961\n",
            "14962\n",
            "14963\n",
            "14964\n",
            "14965\n",
            "14966\n",
            "14967\n",
            "14968\n",
            "14969\n",
            "14970\n",
            "14971\n",
            "14972\n",
            "14973\n",
            "14974\n",
            "14975\n",
            "14976\n",
            "14977\n",
            "14978\n",
            "14979\n",
            "14980\n",
            "14981\n",
            "14982\n",
            "14983\n",
            "14984\n",
            "14985\n",
            "14986\n",
            "14987\n",
            "14988\n",
            "14989\n",
            "14990\n",
            "14991\n",
            "14992\n",
            "14993\n",
            "14994\n",
            "14995\n",
            "14996\n",
            "14997\n",
            "14998\n",
            "14999\n",
            "15000\n",
            "15001\n",
            "15002\n",
            "15003\n",
            "15004\n",
            "15005\n",
            "15006\n",
            "15007\n",
            "15008\n",
            "15009\n",
            "15010\n",
            "15011\n",
            "15012\n",
            "15013\n",
            "15014\n",
            "15015\n",
            "15016\n",
            "15017\n",
            "15018\n",
            "15019\n",
            "15020\n",
            "15021\n",
            "15022\n",
            "15023\n",
            "15024\n",
            "15025\n",
            "15026\n",
            "15027\n",
            "15028\n",
            "15029\n",
            "15030\n",
            "15031\n",
            "15032\n",
            "15033\n",
            "15034\n",
            "15035\n",
            "15036\n",
            "15037\n",
            "15038\n",
            "15039\n",
            "15040\n",
            "15041\n",
            "15042\n",
            "15043\n",
            "15044\n",
            "15045\n",
            "15046\n",
            "15047\n",
            "15048\n",
            "15049\n",
            "15050\n",
            "15051\n",
            "15052\n",
            "15053\n",
            "15054\n",
            "15055\n",
            "15056\n",
            "15057\n",
            "15058\n",
            "15059\n",
            "15060\n",
            "15061\n",
            "15062\n",
            "15063\n",
            "15064\n",
            "15065\n",
            "15066\n",
            "15067\n",
            "15068\n",
            "15069\n",
            "15070\n",
            "15071\n",
            "15072\n",
            "15073\n",
            "15074\n",
            "15075\n",
            "15076\n",
            "15077\n",
            "15078\n",
            "15079\n",
            "15080\n",
            "15081\n",
            "15082\n",
            "15083\n",
            "15084\n",
            "15085\n",
            "15086\n",
            "15087\n",
            "15088\n",
            "15089\n",
            "15090\n",
            "15091\n",
            "15092\n",
            "15093\n",
            "15094\n",
            "15095\n",
            "15096\n",
            "15097\n",
            "15098\n",
            "15099\n",
            "15100\n",
            "15101\n",
            "15102\n",
            "15103\n",
            "15104\n",
            "15105\n",
            "15106\n",
            "15107\n",
            "15108\n",
            "15109\n",
            "15110\n",
            "15111\n",
            "15112\n",
            "15113\n",
            "15114\n",
            "15115\n",
            "15116\n",
            "15117\n",
            "15118\n",
            "15119\n",
            "15120\n",
            "15121\n",
            "15122\n",
            "15123\n",
            "15124\n",
            "15125\n",
            "15126\n",
            "15127\n",
            "15128\n",
            "15129\n",
            "15130\n",
            "15131\n",
            "15132\n",
            "15133\n",
            "15134\n",
            "15135\n",
            "15136\n",
            "15137\n",
            "15138\n",
            "15139\n",
            "15140\n",
            "15141\n",
            "15142\n",
            "15143\n",
            "15144\n",
            "15145\n",
            "15146\n",
            "15147\n",
            "15148\n",
            "15149\n",
            "15150\n",
            "15151\n",
            "15152\n",
            "15153\n",
            "15154\n",
            "15155\n",
            "15156\n",
            "15157\n",
            "15158\n",
            "15159\n",
            "15160\n",
            "15161\n",
            "15162\n",
            "15163\n",
            "15164\n",
            "15165\n",
            "15166\n",
            "15167\n",
            "15168\n",
            "15169\n",
            "15170\n",
            "15171\n",
            "15172\n",
            "15173\n",
            "15174\n",
            "15175\n",
            "15176\n",
            "15177\n",
            "15178\n",
            "15179\n",
            "15180\n",
            "15181\n",
            "15182\n",
            "15183\n",
            "15184\n",
            "15185\n",
            "15186\n",
            "15187\n",
            "15188\n",
            "15189\n",
            "15190\n",
            "15191\n",
            "15192\n",
            "15193\n",
            "15194\n",
            "15195\n",
            "15196\n",
            "15197\n",
            "15198\n",
            "15199\n",
            "15200\n",
            "15201\n",
            "15202\n",
            "15203\n",
            "15204\n",
            "15205\n",
            "15206\n",
            "15207\n",
            "15208\n",
            "15209\n",
            "15210\n",
            "15211\n",
            "15212\n",
            "15213\n",
            "15214\n",
            "15215\n",
            "15216\n",
            "15217\n",
            "15218\n",
            "15219\n",
            "15220\n",
            "15221\n",
            "15222\n",
            "15223\n",
            "15224\n",
            "15225\n",
            "15226\n",
            "15227\n",
            "15228\n",
            "15229\n",
            "15230\n",
            "15231\n",
            "15232\n",
            "15233\n",
            "15234\n",
            "15235\n",
            "15236\n",
            "15237\n",
            "15238\n",
            "15239\n",
            "15240\n",
            "15241\n",
            "15242\n",
            "15243\n",
            "15244\n",
            "15245\n",
            "15246\n",
            "15247\n",
            "15248\n",
            "15249\n",
            "15250\n",
            "15251\n",
            "15252\n",
            "15253\n",
            "15254\n",
            "15255\n",
            "15256\n",
            "15257\n",
            "15258\n",
            "15259\n",
            "15260\n",
            "15261\n",
            "15262\n",
            "15263\n",
            "15264\n",
            "15265\n",
            "15266\n",
            "15267\n",
            "15268\n",
            "15269\n",
            "15270\n",
            "15271\n",
            "15272\n",
            "15273\n",
            "15274\n",
            "15275\n",
            "15276\n",
            "15277\n",
            "15278\n",
            "15279\n",
            "15280\n",
            "15281\n",
            "15282\n",
            "15283\n",
            "15284\n",
            "15285\n",
            "15286\n",
            "15287\n",
            "15288\n",
            "15289\n",
            "15290\n",
            "15291\n",
            "15292\n",
            "15293\n",
            "15294\n",
            "15295\n",
            "15296\n",
            "15297\n",
            "15298\n",
            "15299\n",
            "15300\n",
            "15301\n",
            "15302\n",
            "15303\n",
            "15304\n",
            "15305\n",
            "15306\n",
            "15307\n",
            "15308\n",
            "15309\n",
            "15310\n",
            "15311\n",
            "15312\n",
            "15313\n",
            "15314\n",
            "15315\n",
            "15316\n",
            "15317\n",
            "15318\n",
            "15319\n",
            "15320\n",
            "15321\n",
            "15322\n",
            "15323\n",
            "15324\n",
            "15325\n",
            "15326\n",
            "15327\n",
            "15328\n",
            "15329\n",
            "15330\n",
            "15331\n",
            "15332\n",
            "15333\n",
            "15334\n",
            "15335\n",
            "15336\n",
            "15337\n",
            "15338\n",
            "15339\n",
            "15340\n",
            "15341\n",
            "15342\n",
            "15343\n",
            "15344\n",
            "15345\n",
            "15346\n",
            "15347\n",
            "15348\n",
            "15349\n",
            "15350\n",
            "15351\n",
            "15352\n",
            "15353\n",
            "15354\n",
            "15355\n",
            "15356\n",
            "15357\n",
            "15358\n",
            "15359\n",
            "15360\n",
            "15361\n",
            "15362\n",
            "15363\n",
            "15364\n",
            "15365\n",
            "15366\n",
            "15367\n",
            "15368\n",
            "15369\n",
            "15370\n",
            "15371\n",
            "15372\n",
            "15373\n",
            "15374\n",
            "15375\n",
            "15376\n",
            "15377\n",
            "15378\n",
            "15379\n",
            "15380\n",
            "15381\n",
            "15382\n",
            "15383\n",
            "15384\n",
            "15385\n",
            "15386\n",
            "15387\n",
            "15388\n",
            "15389\n",
            "15390\n",
            "15391\n",
            "15392\n",
            "15393\n",
            "15394\n",
            "15395\n",
            "15396\n",
            "15397\n",
            "15398\n",
            "15399\n",
            "15400\n",
            "15401\n",
            "15402\n",
            "15403\n",
            "15404\n",
            "15405\n",
            "15406\n",
            "15407\n",
            "15408\n",
            "15409\n",
            "15410\n",
            "15411\n",
            "15412\n",
            "15413\n",
            "15414\n",
            "15415\n",
            "15416\n",
            "15417\n",
            "15418\n",
            "15419\n",
            "15420\n",
            "15421\n",
            "15422\n",
            "15423\n",
            "15424\n",
            "15425\n",
            "15426\n",
            "15427\n",
            "15428\n",
            "15429\n",
            "15430\n",
            "15431\n",
            "15432\n",
            "15433\n",
            "15434\n",
            "15435\n",
            "15436\n",
            "15437\n",
            "15438\n",
            "15439\n",
            "15440\n",
            "15441\n",
            "15442\n",
            "15443\n",
            "15444\n",
            "15445\n",
            "15446\n",
            "15447\n",
            "15448\n",
            "15449\n",
            "15450\n",
            "15451\n",
            "15452\n",
            "15453\n",
            "15454\n",
            "15455\n",
            "15456\n",
            "15457\n",
            "15458\n",
            "15459\n",
            "15460\n",
            "15461\n",
            "15462\n",
            "15463\n",
            "15464\n",
            "15465\n",
            "15466\n",
            "15467\n",
            "15468\n",
            "15469\n",
            "15470\n",
            "15471\n",
            "15472\n",
            "15473\n",
            "15474\n",
            "15475\n",
            "15476\n",
            "15477\n",
            "15478\n",
            "15479\n",
            "15480\n",
            "15481\n",
            "15482\n",
            "15483\n",
            "15484\n",
            "15485\n",
            "15486\n",
            "15487\n",
            "15488\n",
            "15489\n",
            "15490\n",
            "15491\n",
            "15492\n",
            "15493\n",
            "15494\n",
            "15495\n",
            "15496\n",
            "15497\n",
            "15498\n",
            "15499\n",
            "15500\n",
            "15501\n",
            "15502\n",
            "15503\n",
            "15504\n",
            "15505\n",
            "15506\n",
            "15507\n",
            "15508\n",
            "15509\n",
            "15510\n",
            "15511\n",
            "15512\n",
            "15513\n",
            "15514\n",
            "15515\n",
            "15516\n",
            "15517\n",
            "15518\n",
            "15519\n",
            "15520\n",
            "15521\n",
            "15522\n",
            "15523\n",
            "15524\n",
            "15525\n",
            "15526\n",
            "15527\n",
            "15528\n",
            "15529\n",
            "15530\n",
            "15531\n",
            "15532\n",
            "15533\n",
            "15534\n",
            "15535\n",
            "15536\n",
            "15537\n",
            "15538\n",
            "15539\n",
            "15540\n",
            "15541\n",
            "15542\n",
            "15543\n",
            "15544\n",
            "15545\n",
            "15546\n",
            "15547\n",
            "15548\n",
            "15549\n",
            "15550\n",
            "15551\n",
            "15552\n",
            "15553\n",
            "15554\n",
            "15555\n",
            "15556\n",
            "15557\n",
            "15558\n",
            "15559\n",
            "15560\n",
            "15561\n",
            "15562\n",
            "15563\n",
            "15564\n",
            "15565\n",
            "15566\n",
            "15567\n",
            "15568\n",
            "15569\n",
            "15570\n",
            "15571\n",
            "15572\n",
            "15573\n",
            "15574\n",
            "15575\n",
            "15576\n",
            "15577\n",
            "15578\n",
            "15579\n",
            "15580\n",
            "15581\n",
            "15582\n",
            "15583\n",
            "15584\n",
            "15585\n",
            "15586\n",
            "15587\n",
            "15588\n",
            "15589\n",
            "15590\n",
            "15591\n",
            "15592\n",
            "15593\n",
            "15594\n",
            "15595\n",
            "15596\n",
            "15597\n",
            "15598\n",
            "15599\n",
            "15600\n",
            "15601\n",
            "15602\n",
            "15603\n",
            "15604\n",
            "15605\n",
            "15606\n",
            "15607\n",
            "15608\n",
            "15609\n",
            "15610\n",
            "15611\n",
            "15612\n",
            "15613\n",
            "15614\n",
            "15615\n",
            "15616\n",
            "15617\n",
            "15618\n",
            "15619\n",
            "15620\n",
            "15621\n",
            "15622\n",
            "15623\n",
            "15624\n",
            "15625\n",
            "15626\n",
            "15627\n",
            "15628\n",
            "15629\n",
            "15630\n",
            "15631\n",
            "15632\n",
            "15633\n",
            "15634\n",
            "15635\n",
            "15636\n",
            "15637\n",
            "15638\n",
            "15639\n",
            "15640\n",
            "15641\n",
            "15642\n",
            "15643\n",
            "15644\n",
            "15645\n",
            "15646\n",
            "15647\n",
            "15648\n",
            "15649\n",
            "15650\n",
            "15651\n",
            "15652\n",
            "15653\n",
            "15654\n",
            "15655\n",
            "15656\n",
            "15657\n",
            "15658\n",
            "15659\n",
            "15660\n",
            "15661\n",
            "15662\n",
            "15663\n",
            "15664\n",
            "15665\n",
            "15666\n",
            "15667\n",
            "15668\n",
            "15669\n",
            "15670\n",
            "15671\n",
            "15672\n",
            "15673\n",
            "15674\n",
            "15675\n",
            "15676\n",
            "15677\n",
            "15678\n",
            "15679\n",
            "15680\n",
            "15681\n",
            "15682\n",
            "15683\n",
            "15684\n",
            "15685\n",
            "15686\n",
            "15687\n",
            "15688\n",
            "15689\n",
            "15690\n",
            "15691\n",
            "15692\n",
            "15693\n",
            "15694\n",
            "15695\n",
            "15696\n",
            "15697\n",
            "15698\n",
            "15699\n",
            "15700\n",
            "15701\n",
            "15702\n",
            "15703\n",
            "15704\n",
            "15705\n",
            "15706\n",
            "15707\n",
            "15708\n",
            "15709\n",
            "15710\n",
            "15711\n",
            "15712\n",
            "15713\n",
            "15714\n",
            "15715\n",
            "15716\n",
            "15717\n",
            "15718\n",
            "15719\n",
            "15720\n",
            "15721\n",
            "15722\n",
            "15723\n",
            "15724\n",
            "15725\n",
            "15726\n",
            "15727\n",
            "15728\n",
            "15729\n",
            "15730\n",
            "15731\n",
            "15732\n",
            "15733\n",
            "15734\n",
            "15735\n",
            "15736\n",
            "15737\n",
            "15738\n",
            "15739\n",
            "15740\n",
            "15741\n",
            "15742\n",
            "15743\n",
            "15744\n",
            "15745\n",
            "15746\n",
            "15747\n",
            "15748\n",
            "15749\n",
            "15750\n",
            "15751\n",
            "15752\n",
            "15753\n",
            "15754\n",
            "15755\n",
            "15756\n",
            "15757\n",
            "15758\n",
            "15759\n",
            "15760\n",
            "15761\n",
            "15762\n",
            "15763\n",
            "15764\n",
            "15765\n",
            "15766\n",
            "15767\n",
            "15768\n",
            "15769\n",
            "15770\n",
            "15771\n",
            "15772\n",
            "15773\n",
            "15774\n",
            "15775\n",
            "15776\n",
            "15777\n",
            "15778\n",
            "15779\n",
            "15780\n",
            "15781\n",
            "15782\n",
            "15783\n",
            "15784\n",
            "15785\n",
            "15786\n",
            "15787\n",
            "15788\n",
            "15789\n",
            "15790\n",
            "15791\n",
            "15792\n",
            "15793\n",
            "15794\n",
            "15795\n",
            "15796\n",
            "15797\n",
            "15798\n",
            "15799\n",
            "15800\n",
            "15801\n",
            "15802\n",
            "15803\n",
            "15804\n",
            "15805\n",
            "15806\n",
            "15807\n",
            "15808\n",
            "15809\n",
            "15810\n",
            "15811\n",
            "15812\n",
            "15813\n",
            "15814\n",
            "15815\n",
            "15816\n",
            "15817\n",
            "15818\n",
            "15819\n",
            "15820\n",
            "15821\n",
            "15822\n",
            "15823\n",
            "15824\n",
            "15825\n",
            "15826\n",
            "15827\n",
            "15828\n",
            "15829\n",
            "15830\n",
            "15831\n",
            "15832\n",
            "15833\n",
            "15834\n",
            "15835\n",
            "15836\n",
            "15837\n",
            "15838\n",
            "15839\n",
            "15840\n",
            "15841\n",
            "15842\n",
            "15843\n",
            "15844\n",
            "15845\n",
            "15846\n",
            "15847\n",
            "15848\n",
            "15849\n",
            "15850\n",
            "15851\n",
            "15852\n",
            "15853\n",
            "15854\n",
            "15855\n",
            "15856\n",
            "15857\n",
            "15858\n",
            "15859\n",
            "15860\n",
            "15861\n",
            "15862\n",
            "15863\n",
            "15864\n",
            "15865\n",
            "15866\n",
            "15867\n",
            "15868\n",
            "15869\n",
            "15870\n",
            "15871\n",
            "15872\n",
            "15873\n",
            "15874\n",
            "15875\n",
            "15876\n",
            "15877\n",
            "15878\n",
            "15879\n",
            "15880\n",
            "15881\n",
            "15882\n",
            "15883\n",
            "15884\n",
            "15885\n",
            "15886\n",
            "15887\n",
            "15888\n",
            "15889\n",
            "15890\n",
            "15891\n",
            "15892\n",
            "15893\n",
            "15894\n",
            "15895\n",
            "15896\n",
            "15897\n",
            "15898\n",
            "15899\n",
            "15900\n",
            "15901\n",
            "15902\n",
            "15903\n",
            "15904\n",
            "15905\n",
            "15906\n",
            "15907\n",
            "15908\n",
            "15909\n",
            "15910\n",
            "15911\n",
            "15912\n",
            "15913\n",
            "15914\n",
            "15915\n",
            "15916\n",
            "15917\n",
            "15918\n",
            "15919\n",
            "15920\n",
            "15921\n",
            "15922\n",
            "15923\n",
            "15924\n",
            "15925\n",
            "15926\n",
            "15927\n",
            "15928\n",
            "15929\n",
            "15930\n",
            "15931\n",
            "15932\n",
            "15933\n",
            "15934\n",
            "15935\n",
            "15936\n",
            "15937\n",
            "15938\n",
            "15939\n",
            "15940\n",
            "15941\n",
            "15942\n",
            "15943\n",
            "15944\n",
            "15945\n",
            "15946\n",
            "15947\n",
            "15948\n",
            "15949\n",
            "15950\n",
            "15951\n",
            "15952\n",
            "15953\n",
            "15954\n",
            "15955\n",
            "15956\n",
            "15957\n",
            "15958\n",
            "15959\n",
            "15960\n",
            "15961\n",
            "15962\n",
            "15963\n",
            "15964\n",
            "15965\n",
            "15966\n",
            "15967\n",
            "15968\n",
            "15969\n",
            "15970\n",
            "15971\n",
            "15972\n",
            "15973\n",
            "15974\n",
            "15975\n",
            "15976\n",
            "15977\n",
            "15978\n",
            "15979\n",
            "15980\n",
            "15981\n",
            "15982\n",
            "15983\n",
            "15984\n",
            "15985\n",
            "15986\n",
            "15987\n",
            "15988\n",
            "15989\n",
            "15990\n",
            "15991\n",
            "15992\n",
            "15993\n",
            "15994\n",
            "15995\n",
            "15996\n",
            "15997\n",
            "15998\n",
            "15999\n",
            "16000\n",
            "16001\n",
            "16002\n",
            "16003\n",
            "16004\n",
            "16005\n",
            "16006\n",
            "16007\n",
            "16008\n",
            "16009\n",
            "16010\n",
            "16011\n",
            "16012\n",
            "16013\n",
            "16014\n",
            "16015\n",
            "16016\n",
            "16017\n",
            "16018\n",
            "16019\n",
            "16020\n",
            "16021\n",
            "16022\n",
            "16023\n",
            "16024\n",
            "16025\n",
            "16026\n",
            "16027\n",
            "16028\n",
            "16029\n",
            "16030\n",
            "16031\n",
            "16032\n",
            "16033\n",
            "16034\n",
            "16035\n",
            "16036\n",
            "16037\n",
            "16038\n",
            "16039\n",
            "16040\n",
            "16041\n",
            "16042\n",
            "16043\n",
            "16044\n",
            "16045\n",
            "16046\n",
            "16047\n",
            "16048\n",
            "16049\n",
            "16050\n",
            "16051\n",
            "16052\n",
            "16053\n",
            "16054\n",
            "16055\n",
            "16056\n",
            "16057\n",
            "16058\n",
            "16059\n",
            "16060\n",
            "16061\n",
            "16062\n",
            "16063\n",
            "16064\n",
            "16065\n",
            "16066\n",
            "16067\n",
            "16068\n",
            "16069\n",
            "16070\n",
            "16071\n",
            "16072\n",
            "16073\n",
            "16074\n",
            "16075\n",
            "16076\n",
            "16077\n",
            "16078\n",
            "16079\n",
            "16080\n",
            "16081\n",
            "16082\n",
            "16083\n",
            "16084\n",
            "16085\n",
            "16086\n",
            "16087\n",
            "16088\n",
            "16089\n",
            "16090\n",
            "16091\n",
            "16092\n",
            "16093\n",
            "16094\n",
            "16095\n",
            "16096\n",
            "16097\n",
            "16098\n",
            "16099\n",
            "16100\n",
            "16101\n",
            "16102\n",
            "16103\n",
            "16104\n",
            "16105\n",
            "16106\n",
            "16107\n",
            "16108\n",
            "16109\n",
            "16110\n",
            "16111\n",
            "16112\n",
            "16113\n",
            "16114\n",
            "16115\n",
            "16116\n",
            "16117\n",
            "16118\n",
            "16119\n",
            "16120\n",
            "16121\n",
            "16122\n",
            "16123\n",
            "16124\n",
            "16125\n",
            "16126\n",
            "16127\n",
            "16128\n",
            "16129\n",
            "16130\n",
            "16131\n",
            "16132\n",
            "16133\n",
            "16134\n",
            "16135\n",
            "16136\n",
            "16137\n",
            "16138\n",
            "16139\n",
            "16140\n",
            "16141\n",
            "16142\n",
            "16143\n",
            "16144\n",
            "16145\n",
            "16146\n",
            "16147\n",
            "16148\n",
            "16149\n",
            "16150\n",
            "16151\n",
            "16152\n",
            "16153\n",
            "16154\n",
            "16155\n",
            "16156\n",
            "16157\n",
            "16158\n",
            "16159\n",
            "16160\n",
            "16161\n",
            "16162\n",
            "16163\n",
            "16164\n",
            "16165\n",
            "16166\n",
            "16167\n",
            "16168\n",
            "16169\n",
            "16170\n",
            "16171\n",
            "16172\n",
            "16173\n",
            "16174\n",
            "16175\n",
            "16176\n",
            "16177\n",
            "16178\n",
            "16179\n",
            "16180\n",
            "16181\n",
            "16182\n",
            "16183\n",
            "16184\n",
            "16185\n",
            "16186\n",
            "16187\n",
            "16188\n",
            "16189\n",
            "16190\n",
            "16191\n",
            "16192\n",
            "16193\n",
            "16194\n",
            "16195\n",
            "16196\n",
            "16197\n",
            "16198\n",
            "16199\n",
            "16200\n",
            "16201\n",
            "16202\n",
            "16203\n",
            "16204\n",
            "16205\n",
            "16206\n",
            "16207\n",
            "16208\n",
            "16209\n",
            "16210\n",
            "16211\n",
            "16212\n",
            "16213\n",
            "16214\n",
            "16215\n",
            "16216\n",
            "16217\n",
            "16218\n",
            "16219\n",
            "16220\n",
            "16221\n",
            "16222\n",
            "16223\n",
            "16224\n",
            "16225\n",
            "16226\n",
            "16227\n",
            "16228\n",
            "16229\n",
            "16230\n",
            "16231\n",
            "16232\n",
            "16233\n",
            "16234\n",
            "16235\n",
            "16236\n",
            "16237\n",
            "16238\n",
            "16239\n",
            "16240\n",
            "16241\n",
            "16242\n",
            "16243\n",
            "16244\n",
            "16245\n",
            "16246\n",
            "16247\n",
            "16248\n",
            "16249\n",
            "16250\n",
            "16251\n",
            "16252\n",
            "16253\n",
            "16254\n",
            "16255\n",
            "16256\n",
            "16257\n",
            "16258\n",
            "16259\n",
            "16260\n",
            "16261\n",
            "16262\n",
            "16263\n",
            "16264\n",
            "16265\n",
            "16266\n",
            "16267\n",
            "16268\n",
            "16269\n",
            "16270\n",
            "16271\n",
            "16272\n",
            "16273\n",
            "16274\n",
            "16275\n",
            "16276\n",
            "16277\n",
            "16278\n",
            "16279\n",
            "16280\n",
            "16281\n",
            "16282\n",
            "16283\n",
            "16284\n",
            "16285\n",
            "16286\n",
            "16287\n",
            "16288\n",
            "16289\n",
            "16290\n",
            "16291\n",
            "16292\n",
            "16293\n",
            "16294\n",
            "16295\n",
            "16296\n",
            "16297\n",
            "16298\n",
            "16299\n",
            "16300\n",
            "16301\n",
            "16302\n",
            "16303\n",
            "16304\n",
            "16305\n",
            "16306\n",
            "16307\n",
            "16308\n",
            "16309\n",
            "16310\n",
            "16311\n",
            "16312\n",
            "16313\n",
            "16314\n",
            "16315\n",
            "16316\n",
            "16317\n",
            "16318\n",
            "16319\n",
            "16320\n",
            "16321\n",
            "16322\n",
            "16323\n",
            "16324\n",
            "16325\n",
            "16326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCsZaKtb_q6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec975325-0bc6-44b7-98e3-3f05f2e36a3a"
      },
      "source": [
        "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on val data is: 96.17188247679204 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HprWRhuXeD2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8639c8ab-5d4f-4eb6-cb29-e1971d960f3b"
      },
      "source": [
        "len(total_labels)==len(total_predictions)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB0kEgq2gv1F"
      },
      "source": [
        "total_labels=[x.tolist() for x in total_labels]\n",
        "total_predictions=[x.tolist() for x in total_predictions]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlywfbeWg3mv"
      },
      "source": [
        "total_labels=list(np.concatenate(total_labels).flat)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFtFvaOOhJir"
      },
      "source": [
        "total_predictions=list(np.concatenate(total_predictions).flat)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQm81XJyDH-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248de4b2-16c8-438c-ba74-50b2a54153fe"
      },
      "source": [
        "f1_score(total_labels, total_predictions)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6933455995093529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvqFpaBMu39"
      },
      "source": [
        "Final bert score using entire training data"
      ]
    }
  ]
}