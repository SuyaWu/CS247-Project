{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtep92FewPO-"
   },
   "source": [
    "The data itself is too big, so we just train a partial of the training data, even for Google Colab Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1621792020819,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "dhOCIdHiwNOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2419,
     "status": "ok",
     "timestamp": 1621792023410,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "v59BLuXOh4vw",
    "outputId": "763330d6-0f64-4944-bbf0-fdf648c6fa19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "#Run this if you don't have transformers\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2052,
     "status": "ok",
     "timestamp": 1621792025457,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "SUpkAFhseK5c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification,AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1621792025458,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "65PGkLGQei8D"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop=stopwords.words('english') \n",
    "# def remove_stop_words(x):\n",
    "#     for word in stop:\n",
    "#         token = \" \" + word + \" \"\n",
    "#         if (x.find(token) != -1):\n",
    "#             x = x.replace(token, \" \")\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1621792027653,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "qcdI_8WlelGz"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"/content/drive/Shareddrives/CS 247 project/data/train.csv\")\n",
    "#test_data=pd.read_csv(\"/content/drive/Shareddrives/CS 247 project/data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1621792027653,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "gPVdQDJtuH-K"
   },
   "outputs": [],
   "source": [
    "# Becuase it is too large so need to only use first 300000 data to act as the file to be splited\n",
    "train_data=train_data[:300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1621792027838,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "eZV-xpqjiDpX"
   },
   "outputs": [],
   "source": [
    "train_data['question_text']=train_data['question_text'].str.lower()\n",
    "sentences=train_data.question_text.values\n",
    "# sentences=list(map(remove_stop_words, sentences))\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = train_data.target.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 91368,
     "status": "ok",
     "timestamp": 1621792119205,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "wjpQAvH4f3ZT"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5389,
     "status": "ok",
     "timestamp": 1621792124582,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "ZeeFp_8bk6yR"
   },
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "for i in range(len(tokenized_texts)):\n",
    "  input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1621792126020,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "Y4s2nJx4j5l6"
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 256\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 47781,
     "status": "ok",
     "timestamp": 1621792173799,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "4Yp0g-4olHfk"
   },
   "outputs": [],
   "source": [
    "#Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1621792173800,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "JCMZYqWslYxV"
   },
   "outputs": [],
   "source": [
    "#do the train test split of 8:2\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2933,
     "status": "ok",
     "timestamp": 1621792176719,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "vn1e1UcUmno8"
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5493,
     "status": "ok",
     "timestamp": 1621792182210,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "ZfU0YZRpnL9J",
    "outputId": "ab3a5555-84d3-46fa-c860-3429ead9ee76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device ='cpu'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1621792182210,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "Rpqdt9znz_DO"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1621792182211,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "XMgz_UUvnMz-"
   },
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "num_total_steps = 1000\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1621792182211,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "wTjpPWycpbf8"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11934089,
     "status": "ok",
     "timestamp": 1621804116287,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "DgITTtCRnRXO",
    "outputId": "f6ab0d01-f6d7-4de3-972a-55bc4063e524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/15000], Loss: 0.9357\n",
      "Epoch [1/2], Step [51/15000], Loss: 0.1492\n",
      "Epoch [1/2], Step [101/15000], Loss: 0.2914\n",
      "Epoch [1/2], Step [151/15000], Loss: 0.2592\n",
      "Epoch [1/2], Step [201/15000], Loss: 0.0631\n",
      "Epoch [1/2], Step [251/15000], Loss: 0.2439\n",
      "Epoch [1/2], Step [301/15000], Loss: 0.0302\n",
      "Epoch [1/2], Step [351/15000], Loss: 0.0136\n",
      "Epoch [1/2], Step [401/15000], Loss: 0.0132\n",
      "Epoch [1/2], Step [451/15000], Loss: 0.1586\n",
      "Epoch [1/2], Step [501/15000], Loss: 0.3002\n",
      "Epoch [1/2], Step [551/15000], Loss: 0.0342\n",
      "Epoch [1/2], Step [601/15000], Loss: 0.0033\n",
      "Epoch [1/2], Step [651/15000], Loss: 0.0661\n",
      "Epoch [1/2], Step [701/15000], Loss: 0.0085\n",
      "Epoch [1/2], Step [751/15000], Loss: 0.0100\n",
      "Epoch [1/2], Step [801/15000], Loss: 0.1253\n",
      "Epoch [1/2], Step [851/15000], Loss: 0.0458\n",
      "Epoch [1/2], Step [901/15000], Loss: 0.0035\n",
      "Epoch [1/2], Step [951/15000], Loss: 0.3221\n",
      "Epoch [1/2], Step [1001/15000], Loss: 0.0364\n",
      "Epoch [1/2], Step [1051/15000], Loss: 0.3188\n",
      "Epoch [1/2], Step [1101/15000], Loss: 0.1294\n",
      "Epoch [1/2], Step [1151/15000], Loss: 0.1077\n",
      "Epoch [1/2], Step [1201/15000], Loss: 0.0702\n",
      "Epoch [1/2], Step [1251/15000], Loss: 0.1279\n",
      "Epoch [1/2], Step [1301/15000], Loss: 0.0641\n",
      "Epoch [1/2], Step [1351/15000], Loss: 0.0860\n",
      "Epoch [1/2], Step [1401/15000], Loss: 0.1776\n",
      "Epoch [1/2], Step [1451/15000], Loss: 0.3371\n",
      "Epoch [1/2], Step [1501/15000], Loss: 0.0434\n",
      "Epoch [1/2], Step [1551/15000], Loss: 0.1577\n",
      "Epoch [1/2], Step [1601/15000], Loss: 0.1249\n",
      "Epoch [1/2], Step [1651/15000], Loss: 0.0344\n",
      "Epoch [1/2], Step [1701/15000], Loss: 0.0788\n",
      "Epoch [1/2], Step [1751/15000], Loss: 0.0431\n",
      "Epoch [1/2], Step [1801/15000], Loss: 0.0153\n",
      "Epoch [1/2], Step [1851/15000], Loss: 0.0222\n",
      "Epoch [1/2], Step [1901/15000], Loss: 0.0082\n",
      "Epoch [1/2], Step [1951/15000], Loss: 0.1274\n",
      "Epoch [1/2], Step [2001/15000], Loss: 0.0132\n",
      "Epoch [1/2], Step [2051/15000], Loss: 0.1946\n",
      "Epoch [1/2], Step [2101/15000], Loss: 0.0093\n",
      "Epoch [1/2], Step [2151/15000], Loss: 0.1178\n",
      "Epoch [1/2], Step [2201/15000], Loss: 0.1852\n",
      "Epoch [1/2], Step [2251/15000], Loss: 0.0221\n",
      "Epoch [1/2], Step [2301/15000], Loss: 0.1012\n",
      "Epoch [1/2], Step [2351/15000], Loss: 0.1810\n",
      "Epoch [1/2], Step [2401/15000], Loss: 0.1762\n",
      "Epoch [1/2], Step [2451/15000], Loss: 0.4664\n",
      "Epoch [1/2], Step [2501/15000], Loss: 0.2556\n",
      "Epoch [1/2], Step [2551/15000], Loss: 0.3105\n",
      "Epoch [1/2], Step [2601/15000], Loss: 0.0234\n",
      "Epoch [1/2], Step [2651/15000], Loss: 0.1309\n",
      "Epoch [1/2], Step [2701/15000], Loss: 0.0629\n",
      "Epoch [1/2], Step [2751/15000], Loss: 0.0758\n",
      "Epoch [1/2], Step [2801/15000], Loss: 0.1851\n",
      "Epoch [1/2], Step [2851/15000], Loss: 0.0852\n",
      "Epoch [1/2], Step [2901/15000], Loss: 0.2113\n",
      "Epoch [1/2], Step [2951/15000], Loss: 0.5061\n",
      "Epoch [1/2], Step [3001/15000], Loss: 0.0096\n",
      "Epoch [1/2], Step [3051/15000], Loss: 0.0531\n",
      "Epoch [1/2], Step [3101/15000], Loss: 0.1419\n",
      "Epoch [1/2], Step [3151/15000], Loss: 0.0638\n",
      "Epoch [1/2], Step [3201/15000], Loss: 0.0579\n",
      "Epoch [1/2], Step [3251/15000], Loss: 0.0793\n",
      "Epoch [1/2], Step [3301/15000], Loss: 0.0486\n",
      "Epoch [1/2], Step [3351/15000], Loss: 0.0525\n",
      "Epoch [1/2], Step [3401/15000], Loss: 0.0286\n",
      "Epoch [1/2], Step [3451/15000], Loss: 0.1714\n",
      "Epoch [1/2], Step [3501/15000], Loss: 0.0129\n",
      "Epoch [1/2], Step [3551/15000], Loss: 0.0132\n",
      "Epoch [1/2], Step [3601/15000], Loss: 0.0120\n",
      "Epoch [1/2], Step [3651/15000], Loss: 0.1433\n",
      "Epoch [1/2], Step [3701/15000], Loss: 0.0196\n",
      "Epoch [1/2], Step [3751/15000], Loss: 0.1198\n",
      "Epoch [1/2], Step [3801/15000], Loss: 0.1485\n",
      "Epoch [1/2], Step [3851/15000], Loss: 0.0082\n",
      "Epoch [1/2], Step [3901/15000], Loss: 0.0337\n",
      "Epoch [1/2], Step [3951/15000], Loss: 0.0216\n",
      "Epoch [1/2], Step [4001/15000], Loss: 0.2637\n",
      "Epoch [1/2], Step [4051/15000], Loss: 0.0381\n",
      "Epoch [1/2], Step [4101/15000], Loss: 0.1485\n",
      "Epoch [1/2], Step [4151/15000], Loss: 0.0209\n",
      "Epoch [1/2], Step [4201/15000], Loss: 0.2599\n",
      "Epoch [1/2], Step [4251/15000], Loss: 0.0571\n",
      "Epoch [1/2], Step [4301/15000], Loss: 0.1437\n",
      "Epoch [1/2], Step [4351/15000], Loss: 0.0944\n",
      "Epoch [1/2], Step [4401/15000], Loss: 0.1025\n",
      "Epoch [1/2], Step [4451/15000], Loss: 0.0548\n",
      "Epoch [1/2], Step [4501/15000], Loss: 0.0946\n",
      "Epoch [1/2], Step [4551/15000], Loss: 0.1141\n",
      "Epoch [1/2], Step [4601/15000], Loss: 0.2029\n",
      "Epoch [1/2], Step [4651/15000], Loss: 0.0461\n",
      "Epoch [1/2], Step [4701/15000], Loss: 0.0760\n",
      "Epoch [1/2], Step [4751/15000], Loss: 0.0932\n",
      "Epoch [1/2], Step [4801/15000], Loss: 0.0761\n",
      "Epoch [1/2], Step [4851/15000], Loss: 0.3090\n",
      "Epoch [1/2], Step [4901/15000], Loss: 0.1223\n",
      "Epoch [1/2], Step [4951/15000], Loss: 0.1179\n",
      "Epoch [1/2], Step [5001/15000], Loss: 0.1055\n",
      "Epoch [1/2], Step [5051/15000], Loss: 0.0185\n",
      "Epoch [1/2], Step [5101/15000], Loss: 0.1706\n",
      "Epoch [1/2], Step [5151/15000], Loss: 0.0224\n",
      "Epoch [1/2], Step [5201/15000], Loss: 0.0327\n",
      "Epoch [1/2], Step [5251/15000], Loss: 0.2114\n",
      "Epoch [1/2], Step [5301/15000], Loss: 0.0509\n",
      "Epoch [1/2], Step [5351/15000], Loss: 0.0071\n",
      "Epoch [1/2], Step [5401/15000], Loss: 0.3178\n",
      "Epoch [1/2], Step [5451/15000], Loss: 0.1926\n",
      "Epoch [1/2], Step [5501/15000], Loss: 0.0121\n",
      "Epoch [1/2], Step [5551/15000], Loss: 0.0261\n",
      "Epoch [1/2], Step [5601/15000], Loss: 0.0398\n",
      "Epoch [1/2], Step [5651/15000], Loss: 0.1435\n",
      "Epoch [1/2], Step [5701/15000], Loss: 0.4101\n",
      "Epoch [1/2], Step [5751/15000], Loss: 0.0642\n",
      "Epoch [1/2], Step [5801/15000], Loss: 0.0435\n",
      "Epoch [1/2], Step [5851/15000], Loss: 0.0928\n",
      "Epoch [1/2], Step [5901/15000], Loss: 0.0943\n",
      "Epoch [1/2], Step [5951/15000], Loss: 0.0481\n",
      "Epoch [1/2], Step [6001/15000], Loss: 0.1578\n",
      "Epoch [1/2], Step [6051/15000], Loss: 0.0142\n",
      "Epoch [1/2], Step [6101/15000], Loss: 0.1448\n",
      "Epoch [1/2], Step [6151/15000], Loss: 0.2169\n",
      "Epoch [1/2], Step [6201/15000], Loss: 0.0331\n",
      "Epoch [1/2], Step [6251/15000], Loss: 0.0421\n",
      "Epoch [1/2], Step [6301/15000], Loss: 0.0501\n",
      "Epoch [1/2], Step [6351/15000], Loss: 0.0051\n",
      "Epoch [1/2], Step [6401/15000], Loss: 0.1195\n",
      "Epoch [1/2], Step [6451/15000], Loss: 0.1327\n",
      "Epoch [1/2], Step [6501/15000], Loss: 0.0504\n",
      "Epoch [1/2], Step [6551/15000], Loss: 0.0340\n",
      "Epoch [1/2], Step [6601/15000], Loss: 0.2018\n",
      "Epoch [1/2], Step [6651/15000], Loss: 0.4125\n",
      "Epoch [1/2], Step [6701/15000], Loss: 0.0633\n",
      "Epoch [1/2], Step [6751/15000], Loss: 0.0414\n",
      "Epoch [1/2], Step [6801/15000], Loss: 0.0285\n",
      "Epoch [1/2], Step [6851/15000], Loss: 0.2987\n",
      "Epoch [1/2], Step [6901/15000], Loss: 0.0445\n",
      "Epoch [1/2], Step [6951/15000], Loss: 0.0047\n",
      "Epoch [1/2], Step [7001/15000], Loss: 0.2528\n",
      "Epoch [1/2], Step [7051/15000], Loss: 0.1619\n",
      "Epoch [1/2], Step [7101/15000], Loss: 0.0190\n",
      "Epoch [1/2], Step [7151/15000], Loss: 0.4064\n",
      "Epoch [1/2], Step [7201/15000], Loss: 0.0068\n",
      "Epoch [1/2], Step [7251/15000], Loss: 0.0533\n",
      "Epoch [1/2], Step [7301/15000], Loss: 0.0251\n",
      "Epoch [1/2], Step [7351/15000], Loss: 0.0497\n",
      "Epoch [1/2], Step [7401/15000], Loss: 0.0190\n",
      "Epoch [1/2], Step [7451/15000], Loss: 0.0371\n",
      "Epoch [1/2], Step [7501/15000], Loss: 0.0709\n",
      "Epoch [1/2], Step [7551/15000], Loss: 0.0412\n",
      "Epoch [1/2], Step [7601/15000], Loss: 0.2574\n",
      "Epoch [1/2], Step [7651/15000], Loss: 0.0159\n",
      "Epoch [1/2], Step [7701/15000], Loss: 0.0177\n",
      "Epoch [1/2], Step [7751/15000], Loss: 0.1673\n",
      "Epoch [1/2], Step [7801/15000], Loss: 0.0509\n",
      "Epoch [1/2], Step [7851/15000], Loss: 0.1317\n",
      "Epoch [1/2], Step [7901/15000], Loss: 0.1967\n",
      "Epoch [1/2], Step [7951/15000], Loss: 0.0310\n",
      "Epoch [1/2], Step [8001/15000], Loss: 0.1188\n",
      "Epoch [1/2], Step [8051/15000], Loss: 0.3198\n",
      "Epoch [1/2], Step [8101/15000], Loss: 0.0639\n",
      "Epoch [1/2], Step [8151/15000], Loss: 0.1882\n",
      "Epoch [1/2], Step [8201/15000], Loss: 0.0922\n",
      "Epoch [1/2], Step [8251/15000], Loss: 0.0426\n",
      "Epoch [1/2], Step [8301/15000], Loss: 0.5323\n",
      "Epoch [1/2], Step [8351/15000], Loss: 0.1132\n",
      "Epoch [1/2], Step [8401/15000], Loss: 0.2125\n",
      "Epoch [1/2], Step [8451/15000], Loss: 0.1710\n",
      "Epoch [1/2], Step [8501/15000], Loss: 0.3730\n",
      "Epoch [1/2], Step [8551/15000], Loss: 0.5241\n",
      "Epoch [1/2], Step [8601/15000], Loss: 0.0177\n",
      "Epoch [1/2], Step [8651/15000], Loss: 0.0031\n",
      "Epoch [1/2], Step [8701/15000], Loss: 0.0079\n",
      "Epoch [1/2], Step [8751/15000], Loss: 0.2805\n",
      "Epoch [1/2], Step [8801/15000], Loss: 0.5884\n",
      "Epoch [1/2], Step [8851/15000], Loss: 0.1583\n",
      "Epoch [1/2], Step [8901/15000], Loss: 0.1140\n",
      "Epoch [1/2], Step [8951/15000], Loss: 0.0976\n",
      "Epoch [1/2], Step [9001/15000], Loss: 0.0941\n",
      "Epoch [1/2], Step [9051/15000], Loss: 0.0119\n",
      "Epoch [1/2], Step [9101/15000], Loss: 0.1048\n",
      "Epoch [1/2], Step [9151/15000], Loss: 0.0828\n",
      "Epoch [1/2], Step [9201/15000], Loss: 0.1431\n",
      "Epoch [1/2], Step [9251/15000], Loss: 0.1425\n",
      "Epoch [1/2], Step [9301/15000], Loss: 0.0153\n",
      "Epoch [1/2], Step [9351/15000], Loss: 0.2836\n",
      "Epoch [1/2], Step [9401/15000], Loss: 0.1771\n",
      "Epoch [1/2], Step [9451/15000], Loss: 0.0575\n",
      "Epoch [1/2], Step [9501/15000], Loss: 0.0775\n",
      "Epoch [1/2], Step [9551/15000], Loss: 0.0807\n",
      "Epoch [1/2], Step [9601/15000], Loss: 0.0259\n",
      "Epoch [1/2], Step [9651/15000], Loss: 0.0788\n",
      "Epoch [1/2], Step [9701/15000], Loss: 0.0112\n",
      "Epoch [1/2], Step [9751/15000], Loss: 0.0300\n",
      "Epoch [1/2], Step [9801/15000], Loss: 0.1335\n",
      "Epoch [1/2], Step [9851/15000], Loss: 0.0347\n",
      "Epoch [1/2], Step [9901/15000], Loss: 0.0579\n",
      "Epoch [1/2], Step [9951/15000], Loss: 0.2561\n",
      "Epoch [1/2], Step [10001/15000], Loss: 0.2682\n",
      "Epoch [1/2], Step [10051/15000], Loss: 0.0959\n",
      "Epoch [1/2], Step [10101/15000], Loss: 0.0285\n",
      "Epoch [1/2], Step [10151/15000], Loss: 0.7416\n",
      "Epoch [1/2], Step [10201/15000], Loss: 0.0204\n",
      "Epoch [1/2], Step [10251/15000], Loss: 0.2053\n",
      "Epoch [1/2], Step [10301/15000], Loss: 0.0359\n",
      "Epoch [1/2], Step [10351/15000], Loss: 0.0585\n",
      "Epoch [1/2], Step [10401/15000], Loss: 0.0404\n",
      "Epoch [1/2], Step [10451/15000], Loss: 0.0047\n",
      "Epoch [1/2], Step [10501/15000], Loss: 0.0588\n",
      "Epoch [1/2], Step [10551/15000], Loss: 0.0125\n",
      "Epoch [1/2], Step [10601/15000], Loss: 0.0123\n",
      "Epoch [1/2], Step [10651/15000], Loss: 0.1801\n",
      "Epoch [1/2], Step [10701/15000], Loss: 0.0823\n",
      "Epoch [1/2], Step [10751/15000], Loss: 0.0966\n",
      "Epoch [1/2], Step [10801/15000], Loss: 0.2572\n",
      "Epoch [1/2], Step [10851/15000], Loss: 0.1140\n",
      "Epoch [1/2], Step [10901/15000], Loss: 0.1703\n",
      "Epoch [1/2], Step [10951/15000], Loss: 0.0422\n",
      "Epoch [1/2], Step [11001/15000], Loss: 0.0632\n",
      "Epoch [1/2], Step [11051/15000], Loss: 0.0627\n",
      "Epoch [1/2], Step [11101/15000], Loss: 0.0460\n",
      "Epoch [1/2], Step [11151/15000], Loss: 0.2813\n",
      "Epoch [1/2], Step [11201/15000], Loss: 0.2242\n",
      "Epoch [1/2], Step [11251/15000], Loss: 0.0698\n",
      "Epoch [1/2], Step [11301/15000], Loss: 0.1716\n",
      "Epoch [1/2], Step [11351/15000], Loss: 0.0819\n",
      "Epoch [1/2], Step [11401/15000], Loss: 0.0465\n",
      "Epoch [1/2], Step [11451/15000], Loss: 0.3485\n",
      "Epoch [1/2], Step [11501/15000], Loss: 0.0064\n",
      "Epoch [1/2], Step [11551/15000], Loss: 0.0713\n",
      "Epoch [1/2], Step [11601/15000], Loss: 0.2788\n",
      "Epoch [1/2], Step [11651/15000], Loss: 0.0383\n",
      "Epoch [1/2], Step [11701/15000], Loss: 0.0100\n",
      "Epoch [1/2], Step [11751/15000], Loss: 0.5550\n",
      "Epoch [1/2], Step [11801/15000], Loss: 0.0755\n",
      "Epoch [1/2], Step [11851/15000], Loss: 0.0226\n",
      "Epoch [1/2], Step [11901/15000], Loss: 0.0103\n",
      "Epoch [1/2], Step [11951/15000], Loss: 0.2120\n",
      "Epoch [1/2], Step [12001/15000], Loss: 0.0150\n",
      "Epoch [1/2], Step [12051/15000], Loss: 0.0495\n",
      "Epoch [1/2], Step [12101/15000], Loss: 0.0070\n",
      "Epoch [1/2], Step [12151/15000], Loss: 0.0016\n",
      "Epoch [1/2], Step [12201/15000], Loss: 0.2746\n",
      "Epoch [1/2], Step [12251/15000], Loss: 0.0524\n",
      "Epoch [1/2], Step [12301/15000], Loss: 0.1319\n",
      "Epoch [1/2], Step [12351/15000], Loss: 0.0569\n",
      "Epoch [1/2], Step [12401/15000], Loss: 0.1390\n",
      "Epoch [1/2], Step [12451/15000], Loss: 0.1090\n",
      "Epoch [1/2], Step [12501/15000], Loss: 0.0741\n",
      "Epoch [1/2], Step [12551/15000], Loss: 0.0680\n",
      "Epoch [1/2], Step [12601/15000], Loss: 0.0176\n",
      "Epoch [1/2], Step [12651/15000], Loss: 0.1674\n",
      "Epoch [1/2], Step [12701/15000], Loss: 0.1140\n",
      "Epoch [1/2], Step [12751/15000], Loss: 0.0331\n",
      "Epoch [1/2], Step [12801/15000], Loss: 0.5638\n",
      "Epoch [1/2], Step [12851/15000], Loss: 0.0114\n",
      "Epoch [1/2], Step [12901/15000], Loss: 0.2401\n",
      "Epoch [1/2], Step [12951/15000], Loss: 0.1013\n",
      "Epoch [1/2], Step [13001/15000], Loss: 0.2097\n",
      "Epoch [1/2], Step [13051/15000], Loss: 0.0031\n",
      "Epoch [1/2], Step [13101/15000], Loss: 0.1543\n",
      "Epoch [1/2], Step [13151/15000], Loss: 0.0142\n",
      "Epoch [1/2], Step [13201/15000], Loss: 0.0275\n",
      "Epoch [1/2], Step [13251/15000], Loss: 0.0941\n",
      "Epoch [1/2], Step [13301/15000], Loss: 0.0271\n",
      "Epoch [1/2], Step [13351/15000], Loss: 0.0140\n",
      "Epoch [1/2], Step [13401/15000], Loss: 0.0264\n",
      "Epoch [1/2], Step [13451/15000], Loss: 0.0278\n",
      "Epoch [1/2], Step [13501/15000], Loss: 0.0885\n",
      "Epoch [1/2], Step [13551/15000], Loss: 0.0386\n",
      "Epoch [1/2], Step [13601/15000], Loss: 0.1879\n",
      "Epoch [1/2], Step [13651/15000], Loss: 0.0019\n",
      "Epoch [1/2], Step [13701/15000], Loss: 0.0410\n",
      "Epoch [1/2], Step [13751/15000], Loss: 0.0230\n",
      "Epoch [1/2], Step [13801/15000], Loss: 0.0381\n",
      "Epoch [1/2], Step [13851/15000], Loss: 0.0120\n",
      "Epoch [1/2], Step [13901/15000], Loss: 0.1290\n",
      "Epoch [1/2], Step [13951/15000], Loss: 0.0062\n",
      "Epoch [1/2], Step [14001/15000], Loss: 0.0144\n",
      "Epoch [1/2], Step [14051/15000], Loss: 0.0817\n",
      "Epoch [1/2], Step [14101/15000], Loss: 0.0202\n",
      "Epoch [1/2], Step [14151/15000], Loss: 0.4591\n",
      "Epoch [1/2], Step [14201/15000], Loss: 0.0995\n",
      "Epoch [1/2], Step [14251/15000], Loss: 0.1921\n",
      "Epoch [1/2], Step [14301/15000], Loss: 0.0684\n",
      "Epoch [1/2], Step [14351/15000], Loss: 0.0330\n",
      "Epoch [1/2], Step [14401/15000], Loss: 0.0843\n",
      "Epoch [1/2], Step [14451/15000], Loss: 0.0803\n",
      "Epoch [1/2], Step [14501/15000], Loss: 0.0176\n",
      "Epoch [1/2], Step [14551/15000], Loss: 0.0841\n",
      "Epoch [1/2], Step [14601/15000], Loss: 0.0833\n",
      "Epoch [1/2], Step [14651/15000], Loss: 0.0215\n",
      "Epoch [1/2], Step [14701/15000], Loss: 0.0867\n",
      "Epoch [1/2], Step [14751/15000], Loss: 0.0063\n",
      "Epoch [1/2], Step [14801/15000], Loss: 0.0296\n",
      "Epoch [1/2], Step [14851/15000], Loss: 0.0094\n",
      "Epoch [1/2], Step [14901/15000], Loss: 0.0931\n",
      "Epoch [1/2], Step [14951/15000], Loss: 0.1622\n",
      "Epoch [2/2], Step [1/15000], Loss: 0.1111\n",
      "Epoch [2/2], Step [51/15000], Loss: 0.1679\n",
      "Epoch [2/2], Step [101/15000], Loss: 0.0289\n",
      "Epoch [2/2], Step [151/15000], Loss: 0.0762\n",
      "Epoch [2/2], Step [201/15000], Loss: 0.0493\n",
      "Epoch [2/2], Step [251/15000], Loss: 0.1274\n",
      "Epoch [2/2], Step [301/15000], Loss: 0.2258\n",
      "Epoch [2/2], Step [351/15000], Loss: 0.0345\n",
      "Epoch [2/2], Step [401/15000], Loss: 0.0105\n",
      "Epoch [2/2], Step [451/15000], Loss: 0.1822\n",
      "Epoch [2/2], Step [501/15000], Loss: 0.0067\n",
      "Epoch [2/2], Step [551/15000], Loss: 0.0046\n",
      "Epoch [2/2], Step [601/15000], Loss: 0.0530\n",
      "Epoch [2/2], Step [651/15000], Loss: 0.0342\n",
      "Epoch [2/2], Step [701/15000], Loss: 0.0545\n",
      "Epoch [2/2], Step [751/15000], Loss: 0.0061\n",
      "Epoch [2/2], Step [801/15000], Loss: 0.0128\n",
      "Epoch [2/2], Step [851/15000], Loss: 0.0899\n",
      "Epoch [2/2], Step [901/15000], Loss: 0.0566\n",
      "Epoch [2/2], Step [951/15000], Loss: 0.0102\n",
      "Epoch [2/2], Step [1001/15000], Loss: 0.0584\n",
      "Epoch [2/2], Step [1051/15000], Loss: 0.0261\n",
      "Epoch [2/2], Step [1101/15000], Loss: 0.0940\n",
      "Epoch [2/2], Step [1151/15000], Loss: 0.0729\n",
      "Epoch [2/2], Step [1201/15000], Loss: 0.0309\n",
      "Epoch [2/2], Step [1251/15000], Loss: 0.1065\n",
      "Epoch [2/2], Step [1301/15000], Loss: 0.0654\n",
      "Epoch [2/2], Step [1351/15000], Loss: 0.0013\n",
      "Epoch [2/2], Step [1401/15000], Loss: 0.1123\n",
      "Epoch [2/2], Step [1451/15000], Loss: 0.0133\n",
      "Epoch [2/2], Step [1501/15000], Loss: 0.1140\n",
      "Epoch [2/2], Step [1551/15000], Loss: 0.0575\n",
      "Epoch [2/2], Step [1601/15000], Loss: 0.0012\n",
      "Epoch [2/2], Step [1651/15000], Loss: 0.0448\n",
      "Epoch [2/2], Step [1701/15000], Loss: 0.0085\n",
      "Epoch [2/2], Step [1751/15000], Loss: 0.0007\n",
      "Epoch [2/2], Step [1801/15000], Loss: 0.0650\n",
      "Epoch [2/2], Step [1851/15000], Loss: 0.0093\n",
      "Epoch [2/2], Step [1901/15000], Loss: 0.0069\n",
      "Epoch [2/2], Step [1951/15000], Loss: 0.0092\n",
      "Epoch [2/2], Step [2001/15000], Loss: 0.6295\n",
      "Epoch [2/2], Step [2051/15000], Loss: 0.1072\n",
      "Epoch [2/2], Step [2101/15000], Loss: 0.2839\n",
      "Epoch [2/2], Step [2151/15000], Loss: 0.0051\n",
      "Epoch [2/2], Step [2201/15000], Loss: 0.0473\n",
      "Epoch [2/2], Step [2251/15000], Loss: 0.0058\n",
      "Epoch [2/2], Step [2301/15000], Loss: 0.1149\n",
      "Epoch [2/2], Step [2351/15000], Loss: 0.1778\n",
      "Epoch [2/2], Step [2401/15000], Loss: 0.1144\n",
      "Epoch [2/2], Step [2451/15000], Loss: 0.0128\n",
      "Epoch [2/2], Step [2501/15000], Loss: 0.0295\n",
      "Epoch [2/2], Step [2551/15000], Loss: 0.2080\n",
      "Epoch [2/2], Step [2601/15000], Loss: 0.0884\n",
      "Epoch [2/2], Step [2651/15000], Loss: 0.0027\n",
      "Epoch [2/2], Step [2701/15000], Loss: 0.0012\n",
      "Epoch [2/2], Step [2751/15000], Loss: 0.2845\n",
      "Epoch [2/2], Step [2801/15000], Loss: 0.0579\n",
      "Epoch [2/2], Step [2851/15000], Loss: 0.0129\n",
      "Epoch [2/2], Step [2901/15000], Loss: 0.0369\n",
      "Epoch [2/2], Step [2951/15000], Loss: 0.0586\n",
      "Epoch [2/2], Step [3001/15000], Loss: 0.0088\n",
      "Epoch [2/2], Step [3051/15000], Loss: 0.0049\n",
      "Epoch [2/2], Step [3101/15000], Loss: 0.0348\n",
      "Epoch [2/2], Step [3151/15000], Loss: 0.0343\n",
      "Epoch [2/2], Step [3201/15000], Loss: 0.0021\n",
      "Epoch [2/2], Step [3251/15000], Loss: 0.0301\n",
      "Epoch [2/2], Step [3301/15000], Loss: 0.4559\n",
      "Epoch [2/2], Step [3351/15000], Loss: 0.0014\n",
      "Epoch [2/2], Step [3401/15000], Loss: 0.0250\n",
      "Epoch [2/2], Step [3451/15000], Loss: 0.0889\n",
      "Epoch [2/2], Step [3501/15000], Loss: 0.0274\n",
      "Epoch [2/2], Step [3551/15000], Loss: 0.6286\n",
      "Epoch [2/2], Step [3601/15000], Loss: 0.0368\n",
      "Epoch [2/2], Step [3651/15000], Loss: 0.1857\n",
      "Epoch [2/2], Step [3701/15000], Loss: 0.0942\n",
      "Epoch [2/2], Step [3751/15000], Loss: 0.0145\n",
      "Epoch [2/2], Step [3801/15000], Loss: 0.0313\n",
      "Epoch [2/2], Step [3851/15000], Loss: 0.1238\n",
      "Epoch [2/2], Step [3901/15000], Loss: 0.1110\n",
      "Epoch [2/2], Step [3951/15000], Loss: 0.0498\n",
      "Epoch [2/2], Step [4001/15000], Loss: 0.0808\n",
      "Epoch [2/2], Step [4051/15000], Loss: 0.0951\n",
      "Epoch [2/2], Step [4101/15000], Loss: 0.2975\n",
      "Epoch [2/2], Step [4151/15000], Loss: 0.1693\n",
      "Epoch [2/2], Step [4201/15000], Loss: 0.0032\n",
      "Epoch [2/2], Step [4251/15000], Loss: 0.2900\n",
      "Epoch [2/2], Step [4301/15000], Loss: 0.0085\n",
      "Epoch [2/2], Step [4351/15000], Loss: 0.0047\n",
      "Epoch [2/2], Step [4401/15000], Loss: 0.0799\n",
      "Epoch [2/2], Step [4451/15000], Loss: 0.1606\n",
      "Epoch [2/2], Step [4501/15000], Loss: 0.1542\n",
      "Epoch [2/2], Step [4551/15000], Loss: 0.0853\n",
      "Epoch [2/2], Step [4601/15000], Loss: 0.1122\n",
      "Epoch [2/2], Step [4651/15000], Loss: 0.0760\n",
      "Epoch [2/2], Step [4701/15000], Loss: 0.1398\n",
      "Epoch [2/2], Step [4751/15000], Loss: 0.0252\n",
      "Epoch [2/2], Step [4801/15000], Loss: 0.0097\n",
      "Epoch [2/2], Step [4851/15000], Loss: 0.0176\n",
      "Epoch [2/2], Step [4901/15000], Loss: 0.2066\n",
      "Epoch [2/2], Step [4951/15000], Loss: 0.0130\n",
      "Epoch [2/2], Step [5001/15000], Loss: 0.3704\n",
      "Epoch [2/2], Step [5051/15000], Loss: 0.0062\n",
      "Epoch [2/2], Step [5101/15000], Loss: 0.0711\n",
      "Epoch [2/2], Step [5151/15000], Loss: 0.0087\n",
      "Epoch [2/2], Step [5201/15000], Loss: 0.1859\n",
      "Epoch [2/2], Step [5251/15000], Loss: 0.1467\n",
      "Epoch [2/2], Step [5301/15000], Loss: 0.0452\n",
      "Epoch [2/2], Step [5351/15000], Loss: 0.0296\n",
      "Epoch [2/2], Step [5401/15000], Loss: 0.2405\n",
      "Epoch [2/2], Step [5451/15000], Loss: 0.0462\n",
      "Epoch [2/2], Step [5501/15000], Loss: 0.0813\n",
      "Epoch [2/2], Step [5551/15000], Loss: 0.1126\n",
      "Epoch [2/2], Step [5601/15000], Loss: 0.0415\n",
      "Epoch [2/2], Step [5651/15000], Loss: 0.0130\n",
      "Epoch [2/2], Step [5701/15000], Loss: 0.0218\n",
      "Epoch [2/2], Step [5751/15000], Loss: 0.0153\n",
      "Epoch [2/2], Step [5801/15000], Loss: 0.0429\n",
      "Epoch [2/2], Step [5851/15000], Loss: 0.0523\n",
      "Epoch [2/2], Step [5901/15000], Loss: 0.0485\n",
      "Epoch [2/2], Step [5951/15000], Loss: 0.2012\n",
      "Epoch [2/2], Step [6001/15000], Loss: 0.2509\n",
      "Epoch [2/2], Step [6051/15000], Loss: 0.3456\n",
      "Epoch [2/2], Step [6101/15000], Loss: 0.1133\n",
      "Epoch [2/2], Step [6151/15000], Loss: 0.1150\n",
      "Epoch [2/2], Step [6201/15000], Loss: 0.0048\n",
      "Epoch [2/2], Step [6251/15000], Loss: 0.0009\n",
      "Epoch [2/2], Step [6301/15000], Loss: 0.0950\n",
      "Epoch [2/2], Step [6351/15000], Loss: 0.0098\n",
      "Epoch [2/2], Step [6401/15000], Loss: 0.2434\n",
      "Epoch [2/2], Step [6451/15000], Loss: 0.0511\n",
      "Epoch [2/2], Step [6501/15000], Loss: 0.0089\n",
      "Epoch [2/2], Step [6551/15000], Loss: 0.0367\n",
      "Epoch [2/2], Step [6601/15000], Loss: 0.1118\n",
      "Epoch [2/2], Step [6651/15000], Loss: 0.5777\n",
      "Epoch [2/2], Step [6701/15000], Loss: 0.1080\n",
      "Epoch [2/2], Step [6751/15000], Loss: 0.1006\n",
      "Epoch [2/2], Step [6801/15000], Loss: 0.0870\n",
      "Epoch [2/2], Step [6851/15000], Loss: 0.0509\n",
      "Epoch [2/2], Step [6901/15000], Loss: 0.0719\n",
      "Epoch [2/2], Step [6951/15000], Loss: 0.1052\n",
      "Epoch [2/2], Step [7001/15000], Loss: 0.0068\n",
      "Epoch [2/2], Step [7051/15000], Loss: 0.1987\n",
      "Epoch [2/2], Step [7101/15000], Loss: 0.0350\n",
      "Epoch [2/2], Step [7151/15000], Loss: 0.1343\n",
      "Epoch [2/2], Step [7201/15000], Loss: 0.0131\n",
      "Epoch [2/2], Step [7251/15000], Loss: 0.0311\n",
      "Epoch [2/2], Step [7301/15000], Loss: 0.0122\n",
      "Epoch [2/2], Step [7351/15000], Loss: 0.0034\n",
      "Epoch [2/2], Step [7401/15000], Loss: 0.0011\n",
      "Epoch [2/2], Step [7451/15000], Loss: 0.0317\n",
      "Epoch [2/2], Step [7501/15000], Loss: 0.0145\n",
      "Epoch [2/2], Step [7551/15000], Loss: 0.0138\n",
      "Epoch [2/2], Step [7601/15000], Loss: 0.1775\n",
      "Epoch [2/2], Step [7651/15000], Loss: 0.0367\n",
      "Epoch [2/2], Step [7701/15000], Loss: 0.0750\n",
      "Epoch [2/2], Step [7751/15000], Loss: 0.1118\n",
      "Epoch [2/2], Step [7801/15000], Loss: 0.0213\n",
      "Epoch [2/2], Step [7851/15000], Loss: 0.0197\n",
      "Epoch [2/2], Step [7901/15000], Loss: 0.1896\n",
      "Epoch [2/2], Step [7951/15000], Loss: 0.0949\n",
      "Epoch [2/2], Step [8001/15000], Loss: 0.1141\n",
      "Epoch [2/2], Step [8051/15000], Loss: 0.0059\n",
      "Epoch [2/2], Step [8101/15000], Loss: 0.0110\n",
      "Epoch [2/2], Step [8151/15000], Loss: 0.0847\n",
      "Epoch [2/2], Step [8201/15000], Loss: 0.1693\n",
      "Epoch [2/2], Step [8251/15000], Loss: 0.0389\n",
      "Epoch [2/2], Step [8301/15000], Loss: 0.2119\n",
      "Epoch [2/2], Step [8351/15000], Loss: 0.0089\n",
      "Epoch [2/2], Step [8401/15000], Loss: 0.0215\n",
      "Epoch [2/2], Step [8451/15000], Loss: 0.0254\n",
      "Epoch [2/2], Step [8501/15000], Loss: 0.0354\n",
      "Epoch [2/2], Step [8551/15000], Loss: 0.0072\n",
      "Epoch [2/2], Step [8601/15000], Loss: 0.0734\n",
      "Epoch [2/2], Step [8651/15000], Loss: 0.0412\n",
      "Epoch [2/2], Step [8701/15000], Loss: 0.0101\n",
      "Epoch [2/2], Step [8751/15000], Loss: 0.2362\n",
      "Epoch [2/2], Step [8801/15000], Loss: 0.0070\n",
      "Epoch [2/2], Step [8851/15000], Loss: 0.0259\n",
      "Epoch [2/2], Step [8901/15000], Loss: 0.1764\n",
      "Epoch [2/2], Step [8951/15000], Loss: 0.0624\n",
      "Epoch [2/2], Step [9001/15000], Loss: 0.1093\n",
      "Epoch [2/2], Step [9051/15000], Loss: 0.0173\n",
      "Epoch [2/2], Step [9101/15000], Loss: 0.0030\n",
      "Epoch [2/2], Step [9151/15000], Loss: 0.0690\n",
      "Epoch [2/2], Step [9201/15000], Loss: 0.1855\n",
      "Epoch [2/2], Step [9251/15000], Loss: 0.0015\n",
      "Epoch [2/2], Step [9301/15000], Loss: 0.0517\n",
      "Epoch [2/2], Step [9351/15000], Loss: 0.2267\n",
      "Epoch [2/2], Step [9401/15000], Loss: 0.0032\n",
      "Epoch [2/2], Step [9451/15000], Loss: 0.0441\n",
      "Epoch [2/2], Step [9501/15000], Loss: 0.1639\n",
      "Epoch [2/2], Step [9551/15000], Loss: 0.0187\n",
      "Epoch [2/2], Step [9601/15000], Loss: 0.0142\n",
      "Epoch [2/2], Step [9651/15000], Loss: 0.1500\n",
      "Epoch [2/2], Step [9701/15000], Loss: 0.1493\n",
      "Epoch [2/2], Step [9751/15000], Loss: 0.0791\n",
      "Epoch [2/2], Step [9801/15000], Loss: 0.0553\n",
      "Epoch [2/2], Step [9851/15000], Loss: 0.1647\n",
      "Epoch [2/2], Step [9901/15000], Loss: 0.1255\n",
      "Epoch [2/2], Step [9951/15000], Loss: 0.0073\n",
      "Epoch [2/2], Step [10001/15000], Loss: 0.0204\n",
      "Epoch [2/2], Step [10051/15000], Loss: 0.1206\n",
      "Epoch [2/2], Step [10101/15000], Loss: 0.0217\n",
      "Epoch [2/2], Step [10151/15000], Loss: 0.0567\n",
      "Epoch [2/2], Step [10201/15000], Loss: 0.0023\n",
      "Epoch [2/2], Step [10251/15000], Loss: 0.0749\n",
      "Epoch [2/2], Step [10301/15000], Loss: 0.3808\n",
      "Epoch [2/2], Step [10351/15000], Loss: 0.1968\n",
      "Epoch [2/2], Step [10401/15000], Loss: 0.0449\n",
      "Epoch [2/2], Step [10451/15000], Loss: 0.0379\n",
      "Epoch [2/2], Step [10501/15000], Loss: 0.4445\n",
      "Epoch [2/2], Step [10551/15000], Loss: 0.0038\n",
      "Epoch [2/2], Step [10601/15000], Loss: 0.0708\n",
      "Epoch [2/2], Step [10651/15000], Loss: 0.0506\n",
      "Epoch [2/2], Step [10701/15000], Loss: 0.1696\n",
      "Epoch [2/2], Step [10751/15000], Loss: 0.1381\n",
      "Epoch [2/2], Step [10801/15000], Loss: 0.0020\n",
      "Epoch [2/2], Step [10851/15000], Loss: 0.0172\n",
      "Epoch [2/2], Step [10901/15000], Loss: 0.0355\n",
      "Epoch [2/2], Step [10951/15000], Loss: 0.0632\n",
      "Epoch [2/2], Step [11001/15000], Loss: 0.0458\n",
      "Epoch [2/2], Step [11051/15000], Loss: 0.0044\n",
      "Epoch [2/2], Step [11101/15000], Loss: 0.2870\n",
      "Epoch [2/2], Step [11151/15000], Loss: 0.0514\n",
      "Epoch [2/2], Step [11201/15000], Loss: 0.0264\n",
      "Epoch [2/2], Step [11251/15000], Loss: 0.0431\n",
      "Epoch [2/2], Step [11301/15000], Loss: 0.0309\n",
      "Epoch [2/2], Step [11351/15000], Loss: 0.0916\n",
      "Epoch [2/2], Step [11401/15000], Loss: 0.2054\n",
      "Epoch [2/2], Step [11451/15000], Loss: 0.0090\n",
      "Epoch [2/2], Step [11501/15000], Loss: 0.0396\n",
      "Epoch [2/2], Step [11551/15000], Loss: 0.0047\n",
      "Epoch [2/2], Step [11601/15000], Loss: 0.0760\n",
      "Epoch [2/2], Step [11651/15000], Loss: 0.1786\n",
      "Epoch [2/2], Step [11701/15000], Loss: 0.0022\n",
      "Epoch [2/2], Step [11751/15000], Loss: 0.1587\n",
      "Epoch [2/2], Step [11801/15000], Loss: 0.0464\n",
      "Epoch [2/2], Step [11851/15000], Loss: 0.0331\n",
      "Epoch [2/2], Step [11901/15000], Loss: 0.0577\n",
      "Epoch [2/2], Step [11951/15000], Loss: 0.0359\n",
      "Epoch [2/2], Step [12001/15000], Loss: 0.0111\n",
      "Epoch [2/2], Step [12051/15000], Loss: 0.1894\n",
      "Epoch [2/2], Step [12101/15000], Loss: 0.1106\n",
      "Epoch [2/2], Step [12151/15000], Loss: 0.0400\n",
      "Epoch [2/2], Step [12201/15000], Loss: 0.0477\n",
      "Epoch [2/2], Step [12251/15000], Loss: 0.1808\n",
      "Epoch [2/2], Step [12301/15000], Loss: 0.0623\n",
      "Epoch [2/2], Step [12351/15000], Loss: 0.0530\n",
      "Epoch [2/2], Step [12401/15000], Loss: 0.0533\n",
      "Epoch [2/2], Step [12451/15000], Loss: 0.0185\n",
      "Epoch [2/2], Step [12501/15000], Loss: 0.0258\n",
      "Epoch [2/2], Step [12551/15000], Loss: 0.0311\n",
      "Epoch [2/2], Step [12601/15000], Loss: 0.0507\n",
      "Epoch [2/2], Step [12651/15000], Loss: 0.1221\n",
      "Epoch [2/2], Step [12701/15000], Loss: 0.0475\n",
      "Epoch [2/2], Step [12751/15000], Loss: 0.1337\n",
      "Epoch [2/2], Step [12801/15000], Loss: 0.4098\n",
      "Epoch [2/2], Step [12851/15000], Loss: 0.0185\n",
      "Epoch [2/2], Step [12901/15000], Loss: 0.0175\n",
      "Epoch [2/2], Step [12951/15000], Loss: 0.0561\n",
      "Epoch [2/2], Step [13001/15000], Loss: 0.0779\n",
      "Epoch [2/2], Step [13051/15000], Loss: 0.0061\n",
      "Epoch [2/2], Step [13101/15000], Loss: 0.0076\n",
      "Epoch [2/2], Step [13151/15000], Loss: 0.0121\n",
      "Epoch [2/2], Step [13201/15000], Loss: 0.0275\n",
      "Epoch [2/2], Step [13251/15000], Loss: 0.0044\n",
      "Epoch [2/2], Step [13301/15000], Loss: 0.0788\n",
      "Epoch [2/2], Step [13351/15000], Loss: 0.0135\n",
      "Epoch [2/2], Step [13401/15000], Loss: 0.0435\n",
      "Epoch [2/2], Step [13451/15000], Loss: 0.0101\n",
      "Epoch [2/2], Step [13501/15000], Loss: 0.0191\n",
      "Epoch [2/2], Step [13551/15000], Loss: 0.0495\n",
      "Epoch [2/2], Step [13601/15000], Loss: 0.0152\n",
      "Epoch [2/2], Step [13651/15000], Loss: 0.1357\n",
      "Epoch [2/2], Step [13701/15000], Loss: 0.1588\n",
      "Epoch [2/2], Step [13751/15000], Loss: 0.0424\n",
      "Epoch [2/2], Step [13801/15000], Loss: 0.0341\n",
      "Epoch [2/2], Step [13851/15000], Loss: 0.1228\n",
      "Epoch [2/2], Step [13901/15000], Loss: 0.0026\n",
      "Epoch [2/2], Step [13951/15000], Loss: 0.0659\n",
      "Epoch [2/2], Step [14001/15000], Loss: 0.0780\n",
      "Epoch [2/2], Step [14051/15000], Loss: 0.0724\n",
      "Epoch [2/2], Step [14101/15000], Loss: 0.0020\n",
      "Epoch [2/2], Step [14151/15000], Loss: 0.1094\n",
      "Epoch [2/2], Step [14201/15000], Loss: 0.0488\n",
      "Epoch [2/2], Step [14251/15000], Loss: 0.0236\n",
      "Epoch [2/2], Step [14301/15000], Loss: 0.0718\n",
      "Epoch [2/2], Step [14351/15000], Loss: 0.0784\n",
      "Epoch [2/2], Step [14401/15000], Loss: 0.0712\n",
      "Epoch [2/2], Step [14451/15000], Loss: 0.1456\n",
      "Epoch [2/2], Step [14501/15000], Loss: 0.2614\n",
      "Epoch [2/2], Step [14551/15000], Loss: 0.0338\n",
      "Epoch [2/2], Step [14601/15000], Loss: 0.0087\n",
      "Epoch [2/2], Step [14651/15000], Loss: 0.1158\n",
      "Epoch [2/2], Step [14701/15000], Loss: 0.0126\n",
      "Epoch [2/2], Step [14751/15000], Loss: 0.0578\n",
      "Epoch [2/2], Step [14801/15000], Loss: 0.0791\n",
      "Epoch [2/2], Step [14851/15000], Loss: 0.0476\n",
      "Epoch [2/2], Step [14901/15000], Loss: 0.1739\n",
      "Epoch [2/2], Step [14951/15000], Loss: 0.0451\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_dataloader)\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  \n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Forward pass\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "      loss = outputs[0]\n",
    "      train_loss_set.append(loss.item())    \n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      # Update parameters and take a step using the computed gradient\n",
    "      optimizer.step()\n",
    "      #scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      if (i) % 50 == 0:\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1621804116453,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "dJ-yUQn5R5yO",
    "outputId": "64bc3007-e744-42d9-e052-9e0ec66ff014"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_set)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1621804116455,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "LmDu7IkYDDDB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 508784,
     "status": "ok",
     "timestamp": 1621804625226,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "Ztqfa6mE_f9G"
   },
   "outputs": [],
   "source": [
    "total_labels=[]\n",
    "total_predictions=[]\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Forward pass\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      # print (outputs)\n",
    "\n",
    "      prediction = torch.argmax(outputs[0],dim=1)\n",
    "      total_labels.append(b_labels)\n",
    "      total_predictions.append(prediction)\n",
    "      total += b_labels.size(0)\n",
    "      correct+=(prediction==b_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1621804625226,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "RCsZaKtb_q6z",
    "outputId": "5b8804a2-0828-4ad6-d06e-cade534cb66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on val data is: 94.70333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1621804625227,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "DNzwmKW4D0hy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1621804625228,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "HprWRhuXeD2P",
    "outputId": "1bb5285a-4d6e-4def-bd7f-321429bdec28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_labels)==len(total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1621804625228,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "WB0kEgq2gv1F"
   },
   "outputs": [],
   "source": [
    "total_labels=[x.tolist() for x in total_labels]\n",
    "total_predictions=[x.tolist() for x in total_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1621804625229,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "dlywfbeWg3mv"
   },
   "outputs": [],
   "source": [
    "total_labels=list(np.concatenate(total_labels).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1621804625229,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "QFtFvaOOhJir"
   },
   "outputs": [],
   "source": [
    "total_predictions=list(np.concatenate(total_predictions).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1621804625419,
     "user": {
      "displayName": "Zhaoyi Huang",
      "photoUrl": "",
      "userId": "07120073316656133052"
     },
     "user_tz": 420
    },
    "id": "hQm81XJyDH-W",
    "outputId": "3e52a266-778e-4881-8125-5827b65fc1c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6535862219315457"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(total_labels, total_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "941-vSsYH74f"
   },
   "source": [
    "On a 300000 lines data, we have the result of 0.653. This is the result on the validation dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMB8GNnh+KKmtEzoRyUXEgH",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1nMP0TkN72GAtzxhazNNbkqvTiSt3CymN",
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
