{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "reHLNiQ70_jZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "38abFt4bibY4"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/splited_train.csv')\n",
    "test_data = pd.read_csv('./data/splited_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "As7F7E6fjqUC"
   },
   "outputs": [],
   "source": [
    "train_input = list(train_data['question_text'])\n",
    "train_label = list(train_data['target'])\n",
    "\n",
    "test_input = list(test_data['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7OD7O3ojudW",
    "outputId": "40e290e2-b888-4798-b5d6-15f07d5004fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#remove all the stop words for the \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english') \n",
    "\n",
    "def remove_stop_words(x):\n",
    "    for word in stop:\n",
    "        token = \" \" + word + \" \"\n",
    "        if (x.find(token) != -1):\n",
    "            x = x.replace(token, \" \")\n",
    "    return x\n",
    "\n",
    "# train_input_rsw = list(map(remove_stop_words, train_input))\n",
    "# test_input_rsw = list(map(remove_stop_words, test_input))\n",
    "train_input_rsw=train_input\n",
    "test_input_rsw=test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aWmBKPD1llau"
   },
   "outputs": [],
   "source": [
    "max_features=95000\n",
    "embed_size = 300 \n",
    "max_length = 70 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wn5jUZJInZPx"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(\"./data/embeddings/glove.840B.300d/glove.840B.300d.txt\",'r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        word,coefs=get_coefs(*line.split(\" \"))\n",
    "        #coefs = np.asarray(coefs, dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bp7o_4rvnhMh",
    "outputId": "67885730-8927-4133-ec5c-8a8bc7f6f8f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words=max_features)\n",
    "\n",
    "tokenizer.fit_on_texts(train_input_rsw)\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "n_words=min(max_features,len(word_index))\n",
    "\n",
    "# embedding_matrix = np.zeros((n_words+1, embed_size))\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: \n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: \n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wusi8R48nB_o",
    "outputId": "361fc6f4-8f0f-4a13-8cfe-0cff1ac1f592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1044897, 70)\n",
      "(261225, 70)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(train_input_rsw)\n",
    "\n",
    "train_input_padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "print(train_input_padded.shape)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_input_rsw)\n",
    "test_input_padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "print(test_input_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QUclEQCBolmj"
   },
   "outputs": [],
   "source": [
    "train_text, cv_text, train_target, cv_target = train_test_split(train_input_padded, train_label, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CJyYFMrnzWDr"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TTS6lJ_Zo2Ba"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,Bidirectional,LSTM,Dropout,Conv1D,MaxPooling1D,Dense\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpC5AcyLo4Ku",
    "outputId": "e8c0f54e-3ff8-4848-b2da-34b68e787dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 70)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 70, 300)           28500000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 70, 256)           439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 128)           164352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 70, 128)           0         \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 128)               198       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 29,112,167\n",
      "Trainable params: 612,167\n",
      "Non-trainable params: 28,500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_length,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x=Dropout(0.4)(x)\n",
    "x = Attention(max_length)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jIBvkjWGqzQH"
   },
   "outputs": [],
   "source": [
    "# del embeddings_index\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEdAmpA6q1aF",
    "outputId": "2c27d619-ee8c-4287-9d6f-59d7cc5afe4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "919/919 [==============================] - 97s 78ms/step - loss: 0.1480 - acc: 0.9478 - val_loss: 0.1166 - val_acc: 0.9536\n",
      "Epoch 2/5\n",
      "919/919 [==============================] - 70s 76ms/step - loss: 0.1074 - acc: 0.9575 - val_loss: 0.1045 - val_acc: 0.9587\n",
      "Epoch 3/5\n",
      "919/919 [==============================] - 70s 76ms/step - loss: 0.1006 - acc: 0.9599 - val_loss: 0.1015 - val_acc: 0.9594\n",
      "Epoch 4/5\n",
      "919/919 [==============================] - 70s 76ms/step - loss: 0.0958 - acc: 0.9618 - val_loss: 0.1007 - val_acc: 0.9598\n",
      "Epoch 5/5\n",
      "919/919 [==============================] - 70s 76ms/step - loss: 0.0906 - acc: 0.9637 - val_loss: 0.1015 - val_acc: 0.9604\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(np.array(train_text), np.array(train_target), epochs =5, validation_data=(np.array(cv_text),np.array(cv_target)), batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXVFNv80xJ9A",
    "outputId": "54a65d65-9182-4e00-c42f-f7337e71af23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.6198985219501434\n",
      "F1 score at threshold 0.11 is 0.6255674494199405\n",
      "F1 score at threshold 0.12 is 0.6299669816691336\n",
      "F1 score at threshold 0.13 is 0.6353757225433526\n",
      "F1 score at threshold 0.14 is 0.6390629575402635\n",
      "F1 score at threshold 0.15 is 0.6430818543360836\n",
      "F1 score at threshold 0.16 is 0.6470694319206493\n",
      "F1 score at threshold 0.17 is 0.6517155219696508\n",
      "F1 score at threshold 0.18 is 0.6542010087341615\n",
      "F1 score at threshold 0.19 is 0.6566774434220344\n",
      "F1 score at threshold 0.2 is 0.6595076613916101\n",
      "F1 score at threshold 0.21 is 0.6622701331642359\n",
      "F1 score at threshold 0.22 is 0.663715678727226\n",
      "F1 score at threshold 0.23 is 0.6651129669191429\n",
      "F1 score at threshold 0.24 is 0.6664052287581699\n",
      "F1 score at threshold 0.25 is 0.668733100309965\n",
      "F1 score at threshold 0.26 is 0.6694582917912928\n",
      "F1 score at threshold 0.27 is 0.6711382386477966\n",
      "F1 score at threshold 0.28 is 0.6730248917748918\n",
      "F1 score at threshold 0.29 is 0.6742708831363977\n",
      "F1 score at threshold 0.3 is 0.6761747278489734\n",
      "F1 score at threshold 0.31 is 0.6775135586149353\n",
      "F1 score at threshold 0.32 is 0.6776002242781048\n",
      "F1 score at threshold 0.33 is 0.6768404691253356\n",
      "F1 score at threshold 0.34 is 0.6763009895351321\n",
      "F1 score at threshold 0.35 is 0.6766119199598365\n",
      "F1 score at threshold 0.36 is 0.6761622442339671\n",
      "F1 score at threshold 0.37 is 0.6757604493398498\n",
      "F1 score at threshold 0.38 is 0.67544504928645\n",
      "F1 score at threshold 0.39 is 0.6756556526892872\n",
      "F1 score at threshold 0.4 is 0.6738854454484355\n",
      "F1 score at threshold 0.41 is 0.6712865320879783\n",
      "F1 score at threshold 0.42 is 0.6717256032781909\n",
      "F1 score at threshold 0.43 is 0.6715160796324655\n",
      "F1 score at threshold 0.44 is 0.670166769610871\n",
      "F1 score at threshold 0.45 is 0.6678635547576303\n",
      "F1 score at threshold 0.46 is 0.6675590551181102\n",
      "F1 score at threshold 0.47 is 0.6666666666666666\n",
      "F1 score at threshold 0.48 is 0.6660787300569229\n",
      "F1 score at threshold 0.49 is 0.6644588045234249\n",
      "F1 score at threshold 0.5 is 0.6632461926867009\n",
      "Best value [0.32, 0.6776002242781048]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "cv_predictions = model.predict(cv_text, batch_size=512)\n",
    "\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    result = f1_score(cv_target, (cv_predictions>thresh).astype(int))\n",
    "    thresholds.append([thresh, result])\n",
    "    print(\"F1 score at threshold {} is {}\".format(thresh, result))\n",
    "\n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value {}\".format(thresholds[0]))\n",
    "best_thresh = thresholds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LHDWU0GxVVu",
    "outputId": "881aed94-bd71-45f8-c048-3d6edd8f2fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6750872810876785\n"
     ]
    }
   ],
   "source": [
    "#run on test set with best threshold\n",
    "total_predictions=model.predict(test_input_padded, batch_size=512)\n",
    "test_labels=list(test_data['target'])\n",
    "predictions1 = (total_predictions>best_thresh).astype(int)\n",
    "res=f1_score(test_labels, predictions1[:,0])\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BiLSTM_Keras_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
